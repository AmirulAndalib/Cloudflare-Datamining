<div class="mb2 gray5">5 min read</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/6TFLxyM1NGOT9t2uesY1Cp/c96e34adc5f3aca99adf8acdc29d95cd/the-story-of-one-latency-spike.png" alt="">
<div class="post-content lh-copy gray1">
	<p>A customer reported an unusual problem with our CloudFlare CDN: our servers were responding to some HTTP requests slowly. Extremely slowly. 30 seconds slowly. This happened very rarely and wasn't easily reproducible. To make things worse all our usual monitoring hadn't caught the problem. At the application layer everything was fine: our NGINX servers were not reporting any long running requests.</p>
	<p>Time to send in The Wolf.</p>
	<p>He solves problems.</p>
	<div class="flex anchor relative">
		<h3 id="following-the-evidence">Following the evidence</h3>
		<a href="https://blog.cloudflare.com/#following-the-evidence" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>First, we attempted to reproduce what the customer reportedâ€”long HTTP responses. Here is a chart of of test HTTP requests time measured against our CDN:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/LYBBggheuHIbAUYBe31YN/29f5e5ce721d47974a48fddf1fb84de2/http-ms.png" alt="" class="kg-image" width="2048" height="768" loading="lazy">

	</figure>
	<p>We ran thousands of HTTP queries against one server over a couple of hours. Almost all the requests finished in milliseconds, but, as you can clearly see, 5 requests out of thousands took as long as 1000ms to finish. When debugging network problems the delays of 1s, 30s are very characteristic. They may indicate packet loss since the SYN packets are usually retransmitted at times 1s, 3s, 7s, 15, 31s.</p>
	<div class="flex anchor relative">
		<h3 id="blame-the-network">Blame the network</h3>
		<a href="https://blog.cloudflare.com/#blame-the-network" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>At first we thought the spikes in HTTP load times might indicate some sort of network problem. To be sure we ran ICMP pings against two IPs over many hours.</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/2Saulc094Nt6DPjZsPIrSB/dc7bf086678829b1bb4fc0f6fdab0e5a/design.png" alt="" class="kg-image" width="1228" height="332" loading="lazy">

	</figure>
	<p>The first "ping" went from an external test machine to the router and showed a flat latency of about 20ms (with the exception of two peaks at about 120ms due to the slowness on the VPS being used to test from):</p>
	<pre class="language-.txt"><code class="language-.txt">--- ping statistics ---
114890 packets transmitted, 114845 received, 0% packet loss
rtt min/avg/max/mdev = 10.432/11.025/122.768/4.114 ms</code></pre>

	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1vnwnZD3FgIURuAAT7oHOs/824227ae5061cd460943c2414e5b9b21/ping-router-1.png" alt="" class="kg-image" width="2048" height="768" loading="lazy">

	</figure>
	<p>This ~20 ms is a decent <a href="https://www.cloudflare.com/learning/cdn/glossary/round-trip-time-rtt">round trip time (RTT)</a> for that network and confirms the network connection was in fact stable.</p>
	<p>The second "ping" session was launched from our external test machine against one of our Linux servers behind the router:</p>
	<pre class="language-.txt"><code class="language-.txt">--- ping statistics ---
114931 packets transmitted, 114805 received, 0% packet loss
rtt min/avg/max/mdev = 10.434/11.607/1868.110/22.703 ms</code></pre>

	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/6TA72RC9L9TkfsoicaPSnN/261ddc76031242b46ef5c7f83cdac7a7/ping-server-1.png" alt="" class="kg-image" width="2048" height="768" loading="lazy">

	</figure>
	<p>The "ping" output shows the max RTT being 1.8s. The gigantic latency spikes are also clearly visible on the graph.</p>
	<p>The first experiment showed that the network between the external testing server and a router is not malfunctioning. But the second test, against a server just behind this router, revealed awful spikes. This indicates the problem is somewhere between the router and the server inside our datacenter.</p>
	<div class="flex anchor relative">
		<h3 id="tcpdump-to-the-rescue">tcpdump to the rescue</h3>
		<a href="https://blog.cloudflare.com/#tcpdump-to-the-rescue" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>To verify the problem we ran a <code>tcpdump</code> on the affected server, trying to pinpoint the particular ICMP packets affected by a spike:</p>
	<pre class="language-.txt"><code class="language-.txt">$ tcpdump -ttt -n -i eth2 icmp
00:00.000000 IP x.x.x.a &gt; x.x.x.b: ICMP echo request, id 19283
00:01.296841 IP x.x.x.b &gt; x.x.x.a: ICMP echo reply, id 19283</code></pre>
	<p>As you see from this <code>tcpdump</code> output, one particular ICMP packet was indeed received from the network at time 0, but for some reason the operating system waited 1.3s before answering it. On Linux network packets are handled promptly when the interrupt occurs; this delayed ICMP response indicates some serious kernel trouble.</p>
	<div class="flex anchor relative">
		<h3 id="welcome-the-system-tap">Welcome the System Tap</h3>
		<a href="https://blog.cloudflare.com/#welcome-the-system-tap" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>To understand what's going on we had to look at the internals of operating system packet processing. Nowadays there are a plethora of debugging tools for Linux and, for no particular reason, we chose <a href="https://en.wikipedia.org/wiki/SystemTap">System Tap</a> (<code>stap</code>). With a help of <a href="http://brendangregg.com/FlameGraphs/cpuflamegraphs.html">a flame graph</a> we identified a function of interest: <code>net_rx_action</code>.</p>
	<pre class="language-bash"><code class="language-bash">$ ./flame-kernel-global.sh
$ ./stackcollapse-stap.pl out.stap-stacks | ./flamegraph.pl &gt; stap-kernel.svg</code></pre>
	<p>The flamegraph itself:</p>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1Qp3eJtMZPXfqb1q5wM0st/2ea2ee934e297b8dee5135fa0b8525a5/flamegraph.png" alt="" class="kg-image" width="1196" height="633" loading="lazy">

	</figure>
	<p>The <code>net_rx_action</code> function is responsible for handling packets in Soft IRQ mode. It will handle up to <code>netdev_budget</code> packets in one go:</p>
	<pre class="language-bash"><code class="language-bash">$ sysctl net.core.netdev_budget
net.core.netdev_budget = 300</code></pre>
	<p>Here is a run of <a href="https://github.com/majek/dump/blob/master/system-tap/histogram-kernel.stp">our <code>stap</code> script showing the latency</a> distribution for this function:</p>
	<pre class="language-.sh"><code class="language-.sh">$ stap -v histogram-kernel.stp 'kernel.function("net_rx_action")' 30
Duration min:0ms avg:0ms max:23ms count:3685271
Duration (ms):
value |-------------------------------------------------- count
    0 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  3685011
    1 |                                                   215
    2 |                                                    30
    4 |                                                     9
    8 |                                                     5
   16 |                                                     1
   32 |                                                     0</code></pre>
	<p>During a 30s run, we hit the <code>net_rx_action</code> function 3.6 million times. Out of these runs most finished in under 1ms, but there were some outliers. Most importantly one run took an astonishing 23ms.</p>
	<p>Having a 23ms stall in low level packet handling is disastrous. It's totally possible to run out of buffer space and start dropping packets if a couple of such events get accumulated. No wonder the ICMP packets weren't handled in time!</p>
	<div class="flex anchor relative">
		<h3 id="deeper-into-the-rabbit-hole">Deeper into the rabbit hole</h3>
		<a href="https://blog.cloudflare.com/#deeper-into-the-rabbit-hole" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>We repeated the procedure a couple more times. That is:</p>
	<ul>
		<li>
			<p>We made a flame graph <a href="https://github.com/majek/dump/blob/master/system-tap/flame-kernel.stp">source</a>.</p>
		</li>
		<li>
			<p>By trial and error figured out which descendant of<code>net_rx_action</code> caused the latency spike <a href="https://github.com/majek/dump/blob/master/system-tap/histogram-kernel.stp">source</a>.</p>
		</li>
	</ul>
	<p>This procedure was pretty effective, and after a couple of runs we identified the culprit: the <a href="http://lxr.free-electrons.com/source/net/ipv4/tcp_input.c?v=4.2#L4602"><code>tcp_collapse</code></a> function. Here's a summary of the latency measurements:</p>
	<pre class="language-.txt"><code class="language-.txt">$ stap -v histogram-kernel.stp 'kernel.function("tcp_collapse")' 300
Duration min:0ms avg:3ms max:21ms count:1552</code></pre>
	<p>Over 300 seconds there were just about 1,500 executions of the <code>tcp_collapse</code> function. Out of these executions half finished in under 3ms, but the max time was 21ms.</p>
	<div class="flex anchor relative">
		<h3 id="lets-collapse-the-tcp">Let's collapse the TCP</h3>
		<a href="https://blog.cloudflare.com/#lets-collapse-the-tcp" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>The <code>tcp_collapse</code> function is interesting. It turns out to be deeply intermixed with how the BSD sockets API works. To fully understand it let's start with a pub question:</p>
	<blockquote>
		<p><b>If you set a "receive buffer size" on a TCP socket, what does it actually mean?</b></p>
		<p>Go on, read <a href="http://man7.org/linux/man-pages/man7/tcp.7.html">the man page</a>, dust off your <a href="https://books.google.co.uk/books?id=X-l9NX3iemAC">Stevens</a>. I'll wait.</p>
	</blockquote>
	<p>The naive answer would go something along the lines of: the TCP receive buffer setting indicates the maximum number of bytes a <code>read()</code> syscall could retrieve without blocking.</p>
	<p>While this is the intention, this is not exactly how it works. In fact, <a href="http://lxr.free-electrons.com/source/include/net/sock.h?v=4.2#L228">the receive buffer size value</a> on a socket is a hint to the operating system of how much total memory it could use to handle the received data. Most importantly, this includes not only the payload bytes that could be delivered to the application, but also the metadata around it.</p>
	<p>Under normal circumstances, a TCP socket structure contains a doubly-linked list of packetsâ€”the <a href="http://people.cs.clemson.edu/~westall/853/notes/skbuff.pdf"><code>sk_buff</code> structures</a>. Each packet contains not only the data, but also the <code>sk_buff</code> metadata (sk_buff is said <a href="https://lwn.net/Articles/466494">to take 240 bytes</a>). The metadata size does count against the receive buffer size counter. In a pessimistic caseâ€”when the packets are very shortâ€”<i>it is possible the receive buffer memory is almost entirely used by the metadata</i>.</p>
	<p>Using a large chunk of receive buffer space for the metadata is not really what the programmer wants. To counter that, when the socket is under memory pressure complex logic is run with the intention of freeing some space. One of the operations is <code>tcp_collapse</code> and it will merge adjacent TCP packets into one larger <code>sk_buff</code>. This behavior is pretty much a garbage collection (GC)â€”and as everyone knows, when the garbage collection kicks in, the latency must spike.</p>
	<div class="flex anchor relative">
		<h3 id="tuning-the-rmem">Tuning the rmem</h3>
		<a href="https://blog.cloudflare.com/#tuning-the-rmem" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>There are two ways to control the TCP socket receive buffer on Linux:</p>
	<ul>
		<li>
			<p>You can set <code>setsockopt(SO_RCVBUF)</code> explicitly.</p>
		</li>
		<li>
			<p>Or you can leave it to the operating system and allow it to auto-tune it, using the <code>tcp_rmem</code> sysctl as a hint.</p>
		</li>
	</ul>
	<p>At CloudFlare we use the latter approach and the receive buffer sizes are controlled by a sysctl:</p>
	<pre class="language-bash"><code class="language-bash">$ sysctl net.ipv4.tcp_rmem
net.ipv4.tcp_rmem = 4096 5242880 33554432</code></pre>
	<p>This setting tells Linux to autotune socket receive buffers, and allocate between 4KiB and 32MiB, with a default start buffer of 5MiB.</p>
	<p>Since the receive buffer sizes are fairly large, garbage collection could take a long time. To test this we reduced the max <code>rmem</code> size to 2MiB and repeated the latency measurements:</p>
	<pre class="language-.txt"><code class="language-.txt">$ sysctl net.ipv4.tcp_rmem
net.ipv4.tcp_rmem = 4096 1048576 2097152

$ stap -v histogram-kernel.stp 'kernel.function("tcp_collapse")' 300
Duration min:0ms avg:0ms max:3ms count:592</code></pre>
	<p>Now, these numbers are so much better. With the changed settings the <code>tcp_collapse</code> never took more than 3ms!</p>
	<p>We verified that the <code>net_rx_action</code> latency also improved:</p>
	<pre class="language-.txt"><code class="language-.txt">$ stap -v histogram-kernel.stp 'kernel.function("net_rx_action")'
Duration min:0ms avg:0ms max:3ms count:3567235
Duration (ms):
value |-------------------------------------------------- count
    0 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  3567196
    1 |                                                    36
    2 |                                                     3
    4 |                                                     0</code></pre>
	<p>With the <code>rmem</code> changes the max latency of observed <code>net_rx_action</code> times dropped from 23ms to just 3ms.</p>
	<div class="flex anchor relative">
		<h3 id="summary">Summary</h3>
		<a href="https://blog.cloudflare.com/#summary" aria-hidden="true" class="relative sm:absolute sm:-left-5">
			<svg width="16" height="16" viewBox="0 0 24 24">
				<path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path>
			</svg>
		</a>
	</div>
	<p>Setting the <code>rmem</code> sysctl to only 2MiB is not recommended as it could affect the performance of high throughput, high latency connections. On the other hand reducing <code>rmem</code> definitely helps to alleviate the observed latency issue. We settled with 4MiB max <code>rmem</code> value which offers a compromise of reasonable GC times and shouldn't affect the throughput on the TCP layer.</p>
	<p>But most importantly, we showed how to use System Tap to debug latency issues. Use our scripts to measure the <code>net_rx_action</code> latency on your system!</p>
	<p>Our <a href="https://github.com/majek/dump/blob/master/system-tap">simple Stap scripts</a> are available on GitHub.</p>
	<p>The fixes mentioned in this article have already been rolled out. All our customers should feel just a tiny bit faster :)</p>
	<p>If it sounds interesting to work on this type of debugging... <a href="https://www.cloudflare.com/join-our-team">we're hiring</a> in London, Singapore and San Francisco!</p>
</div>