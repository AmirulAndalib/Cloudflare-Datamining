{
	"locale": "en-us",
	"post": {
		"id": "646371ae4ca802000af838bd",
		"uuid": "92e41d2c-acd1-4edb-9fdf-624b9a2b1072",
		"title": "Cloudflare R2 and MosaicML enable training LLMs on any compute, anywhere in the world, with zero switching costs",
		"slug": "cloudflare-r2-mosaicml-train-llms-anywhere-faster-cheaper",
		"html": "<!--kg-card-begin: markdown--><p><small>This post is also available in <a href=\"http://blog.cloudflare.com/zh-cn/cloudflare-r2-mosaicml-train-llms-anywhere-faster-cheaper-zh-cn/\">简体中文</a>, <a href=\"http://blog.cloudflare.com/ja-jp/cloudflare-r2-mosaicml-train-llms-anywhere-faster-cheaper-ja-jp/\">日本語</a>, <a href=\"http://blog.cloudflare.com/de-de/cloudflare-r2-mosaicml-train-llms-anywhere-faster-cheaper-de-de/\">Deutsch</a>, <a href=\"http://blog.cloudflare.com/fr-fr/cloudflare-r2-mosaicml-train-llms-anywhere-faster-cheaper-fr-fr/\">Français</a> and <a href=\"http://blog.cloudflare.com/es-es/cloudflare-r2-mosaicml-train-llms-anywhere-faster-cheaper-es-es/\">Español</a>.</small></p>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2023/05/111.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"512\" height=\"288\"></figure><p>Building the large language models (LLMs) and diffusion models that power <a href=\"https://www.cloudflare.com/learning/ai/what-is-generative-ai/\">generative AI</a> requires massive infrastructure. The most obvious component is compute – hundreds to thousands of GPUs – but an equally critical (and often overlooked) component is the <strong>data storage infrastructure. </strong>Training datasets can be terabytes to petabytes in size, and this data needs to be read in parallel by thousands of processes. In addition, model checkpoints need to be saved frequently throughout a training run, and for LLMs these checkpoints can each be hundreds of gigabytes!</p><p>To manage storage costs and scalability, many machine learning teams have been moving to <a href=\"https://www.cloudflare.com/learning/cloud/what-is-object-storage/\">object storage</a> to host their datasets and checkpoints. Unfortunately, most object store providers use egress fees to “lock in” users to their platform. This makes it very difficult to leverage GPU capacity across multiple cloud providers, or take advantage of lower / dynamic pricing elsewhere, since the data and model checkpoints are too expensive to move. At a time when cloud GPUs are scarce, and new hardware options are entering the market, it’s more important than ever to stay flexible.</p><p>In addition to high egress fees, there is a technical barrier to object-store-centric machine learning training. Reading and writing data between object storage and compute clusters requires high throughput, efficient use of network bandwidth, determinism, and elasticity (the ability to train on different #s of GPUs). Building training software to handle all of this correctly and reliably is hard!</p><p>Today, we’re excited to show how MosaicML’s tools and Cloudflare R2 can be used together to address these challenges. First, with MosaicML’s open source <a href=\"https://github.com/mosaicml/streaming\">StreamingDataset</a> and <a href=\"https://github.com/mosaicml/composer\">Composer</a> libraries, you can easily stream in training data and read/write model checkpoints back to R2. All you need is an Internet connection. Second, thanks to R2’s zero-egress pricing, you can start/stop/move/resize jobs in response to GPU availability and prices across compute providers, without paying any data transfer fees. The MosaicML training platform makes it dead simple to orchestrate such training jobs across multiple clouds.</p><p>Together, Cloudflare and MosaicML give you the freedom to train LLMs on <em>any</em> compute, <em>anywhere</em> in the world, with zero switching costs. That means faster, cheaper training runs, and no vendor lock in :)</p><blockquote><em>“With the MosaicML training platform, customers can efficiently use R2 as the durable storage backend for training LLMs on any compute provider with zero egress fees. AI companies are facing outrageous cloud costs, and they are on the hunt for the tools that can provide them with the speed and flexibility to train their best model at the best price.” </em>– <strong>Naveen Rao, CEO and co-founder, MosaicML</strong></blockquote><h3 id=\"reading-data-from-r2-using-streamingdataset\">Reading data from R2 using StreamingDataset</h3><p>To read data from R2 efficiently and deterministically, you can use the MosaicML <a href=\"https://github.com/mosaicml/streaming\">StreamingDataset</a> library. First, write your training data (images, text, video, anything!) into <code>.mds</code> shard files using the provided Python API:</p><pre><code class=\"language-python\">import numpy as np\nfrom PIL import Image\nfrom streaming import MDSWriter\n\n# Local or remote directory in which to store the compressed output files\ndata_dir = 'path-to-dataset'\n\n# A dictionary mapping input fields to their data types\ncolumns = {\n    'image': 'jpeg',\n    'class': 'int'\n}\n\n# Shard compression, if any\ncompression = 'zstd'\n\n# Save the samples as shards using MDSWriter\nwith MDSWriter(out=data_dir, columns=columns, compression=compression) as out:\n    for i in range(10000):\n        sample = {\n            'image': Image.fromarray(np.random.randint(0, 256, (32, 32, 3), np.uint8)),\n            'class': np.random.randint(10),\n        }\n        out.write(sample)\n</code></pre><p>After your dataset has been converted, you can upload it to R2. Below we demonstrate this with the <code>awscli</code> command line tool, but you can also use `wrangler `or any S3-compatible tool of your choice. StreamingDataset will also support direct cloud writing to R2 soon!</p><pre><code>$ aws s3 cp --recursive path-to-dataset s3://my-bucket/folder --endpoint-url $S3_ENDPOINT_URL\n</code></pre><p>Finally, you can read the data into any device that has read access to your R2 bucket. You can fetch individual samples, loop over the dataset, and feed it into a standard PyTorch dataloader.</p><pre><code class=\"language-python\">from torch.utils.data import DataLoader\nfrom streaming import StreamingDataset\n\n# Make sure that R2 credentials and $S3_ENDPOINT_URL are set in your environment    \n# e.g. export S3_ENDPOINT_URL=\"https://[uid].r2.cloudflarestorage.com\"\n\n# Remote path where full dataset is persistently stored\nremote = 's3://my-bucket/folder'\n\n# Local working dir where dataset is cached during operation\nlocal = '/tmp/path-to-dataset'\n\n# Create streaming dataset\ndataset = StreamingDataset(local=local, remote=remote, shuffle=True)\n\n# Let's see what is in sample #1337...\nsample = dataset[1337]\nimg = sample['image']\ncls = sample['class']\n\n# Create PyTorch DataLoader\ndataloader = DataLoader(dataset)\n</code></pre><p>StreamingDataset comes out of the box with high performance, elastic determinism, fast resumption, and multi-worker support. It also uses smart shuffling and distribution to ensure that download bandwidth is minimized. Across a variety of workloads such as LLMs and diffusion models, we find that there is no impact on training throughput (no dataloader bottleneck) when training from object stores like R2. For more information, check out the StreamingDataset <a href=\"https://www.mosaicml.com/blog/mosaicml-streamingdataset\">announcement blog</a>!</p><h3 id=\"readingwriting-model-checkpoints-to-r2-using-composer\">Reading/writing model checkpoints to R2 using Composer</h3><p>Streaming data into your training loop solves half of the problem, but how do you load/save your model checkpoints? Luckily, if you use a training library like <a href=\"https://github.com/mosaicml/composer\">Composer</a>, it’s as easy as pointing at an R2 path!</p><pre><code class=\"language-Python\">from composer import Trainer\n...\n\n# Make sure that R2 credentials and $S3_ENDPOINT_URL are set in your environment\n# e.g. export S3_ENDPOINT_URL=\"https://[uid].r2.cloudflarestorage.com\"\n\ntrainer = Trainer(\n        run_name='mpt-7b',\n        model=model,\n        train_dataloader=train_loader,\n        ...\n        save_folder=s3://my-bucket/mpt-7b/checkpoints,\n        save_interval='1000ba',\n        # load_path=s3://my-bucket/mpt-7b-prev/checkpoints/ep0-ba100-rank0.pt,\n    )\n</code></pre><p>Composer uses asynchronous uploads to minimize wait time as checkpoints are being saved during training. It also works out of the box with multi-GPU and multi-node training, and <strong>does not require a shared file system.</strong> This means you can skip setting up an expensive EFS/NFS system for your compute cluster, saving thousands of dollars or more per month on public clouds. All you need is an Internet connection and appropriate credentials – your checkpoints arrive safely in your R2 bucket giving you scalable and secure storage for your private models.</p><h3 id=\"using-mosaicml-and-r2-to-train-anywhere-efficiently\">Using MosaicML and R2 to train anywhere efficiently</h3><p>Using the above tools together with Cloudflare R2 enables users to run training workloads on any compute provider, with total freedom and zero switching costs.</p><p>As a demonstration, in Figure X we use the MosaicML training platform to launch an LLM training job starting on Oracle Cloud Infrastructure, with data streaming in and checkpoints uploaded back to R2. Part way through, we pause the job and seamlessly resume on a different set of GPUs on Amazon Web Services. Composer loads the model weights from the last saved checkpoint in R2, and the streaming dataloader instantly resumes to the correct batch. Training continues deterministically. Finally, we move again to Google Cloud to finish the run.</p><p>As we train our LLM across three cloud providers, the only costs we pay are for GPU compute and data storage. No egress fees or lock in thanks to Cloudflare R2!</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://blog.cloudflare.com/content/images/2023/05/image2-19.png\" class=\"kg-image\" alt=\"Using the MosaicML training platform with Cloudflare R2 to run an LLM training job across three different cloud providers, with zero egress fees.\" loading=\"lazy\" width=\"1999\" height=\"1304\"><figcaption><em>Using the MosaicML training platform with Cloudflare R2 to run an LLM training job across three different cloud providers, with zero egress fees.</em></figcaption></figure><pre><code>$ mcli get clusters\nNAME            PROVIDER      GPU_TYPE   GPUS             INSTANCE                   NODES\nmml-1            MosaicML   │  a100_80gb  8             │  mosaic.a100-80sxm.1        1    \n                            │  none       0             │  cpu                        1    \ngcp-1            GCP        │  t4         -             │  n1-standard-48-t4-4        -    \n                            │  a100_40gb  -             │  a2-highgpu-8g              -    \n                            │  none       0             │  cpu                        1    \naws-1            AWS        │  a100_40gb  ≤8,16,...,32  │  p4d.24xlarge               ≤4   \n                            │  none       0             │  cpu                        1    \noci-1            OCI        │  a100_40gb  8,16,...,64   │  oci.bm.gpu.b4.8            ≤8  \n                            │  none       0             │  cpu                        1    \n\n$ mcli create secret s3 --name r2-creds --config-file path/to/config --credentials-file path/to/credentials\n✔  Created s3 secret: r2-creds      \n\n$ mcli create secret env S3_ENDPOINT_URL=\"https://[uid].r2.cloudflarestorage.com\"\n✔  Created environment secret: s3-endpoint-url      \n               \n$ mcli run -f mpt-125m-r2.yaml --follow\n✔  Run mpt-125m-r2-X2k3Uq started                                                                                    \ni  Following run logs. Press Ctrl+C to quit.                                                                            \n                                                                                                                        \nCloning into 'llm-foundry'...\n</code></pre><p><em>Using the MCLI command line tool to manage compute clusters, secrets, and submit runs.</em></p><pre><code>### mpt-125m-r2.yaml ###\n# set up secrets once with `mcli create secret ...`\n# and they will be present in the environment in any subsequent run\n\nintegrations:\n- integration_type: git_repo\n  git_repo: mosaicml/llm-foundry\n  git_branch: main\n  pip_install: -e .[gpu]\n\nimage: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04\n\ncommand: |\n  cd llm-foundry/scripts\n  composer train/train.py train/yamls/mpt/125m.yaml \\\n    data_remote=s3://bucket/path-to-data \\\n    max_duration=100ba \\\n    save_folder=s3://checkpoints/mpt-125m \\\n    save_interval=20ba\n\nrun_name: mpt-125m-r2\n\ngpu_num: 8\ngpu_type: a100_40gb\ncluster: oci-1  # can be any compute cluster!\n</code></pre><p><em>An MCLI job template. Specify a run name, a Docker image, a set of commands, and a compute cluster to run on.</em></p><h3 id=\"get-started-today\">Get started today!</h3><p>The MosaicML platform is an invaluable tool to take your training to the next level, and in this post, we explored how Cloudflare R2 empowers you to train models on your own data, with any compute provider – or all of them. By eliminating egress fees, R2’s storage is an exceptionally cost-effective complement to MosaicML training, providing maximum autonomy and control. With this combination, you can switch between cloud service providers to fit your organization’s needs over time.</p><p>To learn more about using MosaicML to train custom state-of-the-art AI on your own data visit <a href=\"https://www.mosaicml.com/\">here</a> or <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSepW7QB3Xkv6T7GJRwrR9DmGAEjm5G2lBxJC7PUe3JXcBZYbw/viewform\">get in touch</a>.</p><h3 id=\"watch-on-cloudflare-tv\">Watch on Cloudflare TV</h3><!--kg-card-begin: html--><div style=\"position: relative; padding-top: 56.25%;\"><iframe src=\"https://customer-rhnwzxvb3mg4wz3v.cloudflarestream.com/e87f8536439db9b5eea7dfd33ad2f11e/iframe?preload=true&poster=https%3A%2F%2Fcustomer-rhnwzxvb3mg4wz3v.cloudflarestream.com%2Fe87f8536439db9b5eea7dfd33ad2f11e%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D1s%26height%3D600\" style=\"border: none; position: absolute; top: 0; left: 0; height: 100%; width: 100%;\" allow=\"accelerometer; gyroscope; autoplay; encrypted-media; picture-in-picture;\" allowfullscreen=\"true\"></iframe></div><!--kg-card-end: html-->",
		"comment_id": "646371ae4ca802000af838bd",
		"feature_image": "http://blog.cloudflare.com/content/images/2023/05/111-1.png",
		"featured": false,
		"visibility": "public",
		"created_at": "2023-05-16T13:06:06.000+01:00",
		"updated_at": "2024-01-05T16:56:34.000+00:00",
		"published_at": "2023-05-16T14:00:54.000+01:00",
		"custom_excerpt": "Together, Cloudflare and MosaicML give customers the freedom to train LLMs on any compute, anywhere in the world, with zero switching costs. That means faster, cheaper training runs, and no vendor lock in.",
		"codeinjection_head": null,
		"codeinjection_foot": null,
		"custom_template": null,
		"canonical_url": null,
		"authors": [
			{
				"id": "64637dbf4ca802000af8395d",
				"name": "Abhinav Venigalla (Guest Author)",
				"slug": "abhinav",
				"profile_image": "http://blog.cloudflare.com/content/images/2023/05/abhi.jpg",
				"cover_image": null,
				"bio": "Abhi is the NLP Architect at MosaicML and works on reducing the time, complexity, and cost of training LLMs.",
				"website": null,
				"location": null,
				"facebook": null,
				"twitter": "@abhi_venigalla",
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/abhinav/"
			},
			{
				"id": "645a7570a04edf000aa2f20a",
				"name": "Phillip Jones",
				"slug": "phillip",
				"profile_image": "http://blog.cloudflare.com/content/images/2023/05/IMG_0238.jpg",
				"cover_image": null,
				"bio": null,
				"website": null,
				"location": null,
				"facebook": null,
				"twitter": "@akaphill",
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/phillip/"
			},
			{
				"id": "606cb0d74b1e0f03f340ef53",
				"name": "Abhi Das",
				"slug": "abhi",
				"profile_image": "http://blog.cloudflare.com/content/images/2021/04/1595307628960.jpeg",
				"cover_image": null,
				"bio": null,
				"website": null,
				"location": null,
				"facebook": null,
				"twitter": "@abhidasone",
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/abhi/"
			}
		],
		"tags": [
			{
				"id": "646372d14ca802000af838d3",
				"name": "#BLOG-1855",
				"slug": "hash-blog-1855",
				"description": null,
				"feature_image": null,
				"visibility": "internal",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/404/"
			},
			{
				"id": "607030964b1e0f03f340f2d8",
				"name": "Developer Week",
				"slug": "developer-week",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/developer-week/"
			},
			{
				"id": "5d16450341acde0011a95204",
				"name": "Developers",
				"slug": "developers",
				"description": null,
				"feature_image": "http://blog.cloudflare.com/content/images/2020/10/Developers-1.png",
				"visibility": "public",
				"meta_title": "Cloudflare Blog: Developers",
				"meta_description": "Collection of Cloudflare blog posts tagged 'Developers'.",
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/developers/"
			},
			{
				"id": "5d16450341acde0011a95263",
				"name": "Partners",
				"slug": "partners",
				"description": null,
				"feature_image": "https://blog-cloudflare-com-assets.storage.googleapis.com/2020/06/image-2-1.png",
				"visibility": "public",
				"meta_title": "Partners Blog",
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/partners/"
			},
			{
				"id": "61ddc4d00bfc69000a525a5b",
				"name": "R2 Storage",
				"slug": "cloudflare-r2",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/cloudflare-r2/"
			},
			{
				"id": "60e2dc511e77b701bcb3d134",
				"name": "Egress",
				"slug": "egress",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/egress/"
			}
		],
		"primary_author": {
			"id": "64637dbf4ca802000af8395d",
			"name": "Abhinav Venigalla (Guest Author)",
			"slug": "abhinav",
			"profile_image": "http://blog.cloudflare.com/content/images/2023/05/abhi.jpg",
			"cover_image": null,
			"bio": "Abhi is the NLP Architect at MosaicML and works on reducing the time, complexity, and cost of training LLMs.",
			"website": null,
			"location": null,
			"facebook": null,
			"twitter": "@abhi_venigalla",
			"meta_title": null,
			"meta_description": null,
			"url": "http://blog.cloudflare.com/author/abhinav/"
		},
		"primary_tag": null,
		"url": "http://blog.cloudflare.com/cloudflare-r2-mosaicml-train-llms-anywhere-faster-cheaper/",
		"excerpt": "Together, Cloudflare and MosaicML give customers the freedom to train LLMs on any compute, anywhere in the world, with zero switching costs. That means faster, cheaper training runs, and no vendor lock in.",
		"reading_time": 6,
		"access": true,
		"comments": false,
		"og_image": "http://blog.cloudflare.com/content/images/2023/05/Cloudflare-R2-and-MosaicML-enable-training-LLMs-on-any-compute--anywhere-in-the-world--with-zero-switching-costs-OG-1.png",
		"og_title": null,
		"og_description": null,
		"twitter_image": "http://blog.cloudflare.com/content/images/2023/05/Cloudflare-R2-and-MosaicML-enable-training-LLMs-on-any-compute--anywhere-in-the-world--with-zero-switching-costs-OG.png",
		"twitter_title": null,
		"twitter_description": null,
		"meta_title": null,
		"meta_description": "Together, Cloudflare and MosaicML give customers the freedom to train LLMs on any compute, anywhere in the world, with zero switching costs. That means faster, cheaper training runs, and no vendor lock in.",
		"email_subject": null,
		"frontmatter": null,
		"feature_image_alt": null,
		"feature_image_caption": null
	}
}