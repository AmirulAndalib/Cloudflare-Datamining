{
	"post": {
		"id": "648c531e3bb168000add45fa",
		"uuid": "b762fd44-d511-467a-accd-a3a15556f51f",
		"title": "How Cloudflare runs machine learning inference in microseconds",
		"slug": "how-cloudflare-runs-ml-inference-in-microseconds",
		"html": "<figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2023/06/image1-6.png\" class=\"kg-image\" alt=\"How Cloudflare runs machine learning inference in microseconds\" loading=\"lazy\" width=\"1201\" height=\"675\"></figure><p>Cloudflare executes an array of security checks on servers spread across our global network. These checks are designed to block attacks and prevent malicious or unwanted traffic from reaching our customers’ servers. But every check carries a cost - some amount of computation, and therefore some amount of time must be spent evaluating every request we process. As we deploy new protections, the amount of time spent executing security checks increases.</p><p>Latency is a key metric on which <a href=\"https://www.cloudflare.com/learning/cdn/what-is-a-cdn/\">CDNs</a> are evaluated. Just as we optimize network latency by provisioning servers in close proximity to end users, we also optimize processing latency - which is the time spent processing a request before serving a response from cache or passing the request forward to the customers’ servers. Due to the scale of our network and the diversity of use-cases we serve, our edge software is subject to demanding specifications, both in terms of throughput and latency.</p><p>Cloudflare's <a href=\"https://www.cloudflare.com/learning/bots/what-is-bot-management/\">bot management</a> module is one suite of security checks which executes during the hot path of request processing. This module calculates a variety of bot signals and integrates directly with our front line servers, allowing us to customize behavior based on those signals. This module evaluates every request for heuristics and behaviors indicative of bot traffic, and scores every request with several <a href=\"https://www.cloudflare.com/learning/ai/what-is-machine-learning/\">machine learning models</a>.</p><p>To reduce processing latency, we've undertaken a project to rewrite our bot management technology, porting it from Lua to Rust, and applying a number of performance optimizations. This post focuses on optimizations applied to the machine-learning detections within the bot management module, which account for approximately 15% of the latency added by bot detection. By switching away from a garbage collected language, removing memory allocations, and optimizing our parsers, we reduce the P50 latency of the bot management module by 79μs - a 20% reduction.</p><h2 id=\"engineering-for-zero-allocations\">Engineering for zero allocations</h2><p>Writing software without memory allocation poses several challenges. Indeed, high-level programming languages often trade memory management for productivity, abstracting away the details of memory management. But, in those details, are a number of algorithms to find contiguous regions of free memory, handle fragmentation, and call into the kernel to request new memory pages. Garbage collected languages incur additional costs throughout program execution to track when memory can be freed, plus pauses in program execution while the garbage collector executes. But, when performance is a critical requirement, languages should be evaluated for their ability to meet performance constraints.</p><h3 id=\"stack-allocation\">Stack allocation</h3><p>One of the simplest ways to reduce memory allocations is to work with fixed-size buffers. Fixed-sized buffers can be placed on the stack, which eliminates the need to invoke heap allocation logic; the compiler simply reserves space in the current stack frame to hold local variables. Alternatively, the buffers can be heap-allocated outside the hot path (for example, at application startup), incurring a one-time cost.</p><p>Arrays can be stack allocated:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">let mut buf = [0u8; BUFFER_SIZE];\n</code></pre>\n<!--kg-card-end: markdown--><p>Vectors can be created outside the hot path:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">let mut buf = Vec::with_capacity(BUFFER_SIZE);\n</code></pre>\n<!--kg-card-end: markdown--><p>To demonstrate the performance difference, let's compare two implementations of a case-insensitive string equality check. The first will allocate a buffer for each invocation to store the lower-case version of the string. The second will use a buffer that has already been allocated for this purpose.</p><p>Allocate a new buffer for each iteration:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">fn case_insensitive_equality_buffer_with_capacity(s: &amp;str, pat: &amp;str) -&gt; bool {\n\tlet mut buf = String::with_capacity(s.len());\n\tbuf.extend(s.chars().map(|c| c.to_ascii_lowercase()));\n\tbuf == pat\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Re-use a buffer for each iteration, avoiding allocations:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">fn case_insensitive_equality_buffer_with_capacity(s: &amp;str, pat: &amp;str, buf: &amp;mut String) -&gt; bool {\n\tbuf.clear();\n\tbuf.extend(s.chars().map(|c| c.to_ascii_lowercase()));\n\tbuf == pat\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Benchmarking the two code snippets, the first executes in ~40ns per iteration, the second in ~25ns. Changing only the memory allocation behavior, the second implementation is ~38% faster.</p><h3 id=\"choice-of-algorithms\">Choice of algorithms</h3><p>Another strategy to reduce the number of memory allocations is to choose algorithms that operate on the data in-place and store any necessary state on the stack.</p><p>Returning to our string comparison function from above, let's rewrite it operate completely on the stack, and without copying data into a separate buffer:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">fn case_insensitive_equality_buffer_iter(s: &amp;str, pat: &amp;str) -&gt; bool {\n\ts.chars().map(|c| c.to_ascii_lowercase()).eq(pat.chars())\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>In addition to being the shortest, this function is also the fastest. This function benchmarks at ~13ns/iter, which is just slightly slower than the 11ns used to execute <code>eq_ignore_ascii_case</code> from the standard library. And the standard library implementation similarly avoids buffer allocation through use of iterators.</p><h3 id=\"testing-allocations\">Testing allocations</h3><p>Automated testing of memory allocation on the critical path prevents accidental use of functions or libraries which allocate memory. <code>dhat</code> is a crate in the Rust ecosystem that supports such testing. By setting a new global allocator, <code>dhat</code> is able to count the number of allocations, as well as the number of bytes allocated on a given code path.</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">/// Assert that the hot path logic performs zero allocations.\n#[test]\nfn zero_allocations() {\n\tlet _profiler = dhat::Profiler::builder().testing().build();\n\n\t// Execute hot-path logic here.\n\n\t// Assert that no allocations occurred.\n\tdhat::assert_eq!(stats.total_blocks, 0);\n\tdhat::assert_eq!(stats.total_bytes, 0);\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>It is important to note, <code>dhat</code> does have the limitation that it only detects allocations in Rust code. External libraries can still allocate memory without using the Rust allocator. FFI calls, such as those made to C, are one such place where memory allocations may slip past <code>dhat</code>'s measurements.</p><h2 id=\"zero-allocation-decision-trees\">Zero allocation decision trees</h2><p>CatBoost is an open-source machine learning library used within the bot management module. The core logic of CatBoost is implemented in C++, and the library exposes bindings to a number of other languages - such as C and Rust. The Lua-based implementation of the bot management module relied on FFI calls to the C API to execute our models.</p><p>By removing memory allocations and implementing buffer re-use, we optimize the execution duration of the sample model included in the CatBoost repository by 10%. Our production models see gains up to 15%.</p><h3 id=\"optimize-for-single-document-evaluation\">Optimize for single-document evaluation</h3><p>By optimizing CatBoost to evaluate a single set of features at a time, we reduce memory allocations and reduce latency. The CatBoost API has several functions which are optimized for evaluating multiple documents at a time, but this API does not benefit our application where requests are evaluated in the order they are received, and delaying processing to coalesce batches is undesirable. To support evaluation of a variable number of documents, the CatBoost implementation allocates vectors and iterates over the input documents, writing them into the vectors.</p><!--kg-card-begin: markdown--><pre><code class=\"language-cpp\">TVector&lt;TConstArrayRef&lt;float&gt;&gt; floatFeaturesVec(docCount);\nTVector&lt;TConstArrayRef&lt;int&gt;&gt; catFeaturesVec(docCount);\nfor (size_t i = 0; i &lt; docCount; ++i) {\n    if (floatFeaturesSize &gt; 0) {\n        floatFeaturesVec[i] = TConstArrayRef&lt;float&gt;(floatFeatures[i], floatFeaturesSize);\n    }\n    if (catFeaturesSize &gt; 0) {\n        catFeaturesVec[i] = TConstArrayRef&lt;int&gt;(catFeatures[i], catFeaturesSize);\n    }\n}\nFULL_MODEL_PTR(modelHandle)-&gt;Calc(floatFeaturesVec, catFeaturesVec, TArrayRef&lt;double&gt;(result, resultSize));\n</code></pre>\n<!--kg-card-end: markdown--><p>To evaluate a single document, however, CatBoost only needs access to a reference to contiguous memory holding feature values. The above code can be replaced with the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-cpp\">TConstArrayRef&lt;float&gt; floatFeaturesArray = TConstArrayRef&lt;float&gt;(floatFeatures, floatFeaturesSize);\nTConstArrayRef&lt;int&gt; catFeaturesArray = TConstArrayRef&lt;int&gt;(catFeatures, catFeaturesSize);\nFULL_MODEL_PTR(modelHandle)-&gt;Calc(floatFeaturesArray, catFeaturesArray, TArrayRef&lt;double&gt;(result, resultSize));\n</code></pre>\n<!--kg-card-end: markdown--><p>Similar to the C++ implementation, the CatBoost Rust bindings also allocate vectors to support multi-document evaluation. For example, the bindings iterate over a vector of vectors, mapping it to a newly allocated vector of pointers:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">let mut float_features_ptr = float_features\n   .iter()\n   .map(|x| x.as_ptr())\n   .collect::&lt;Vec&lt;_&gt;&gt;();\n</code></pre>\n<!--kg-card-end: markdown--><p>But in the single-document case, we don't need the outer vector at all. We can simply pass the inner pointer value directly:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">let float_features_ptr = float_features.as_ptr();\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"reusing-buffers\">Reusing buffers</h3><p>The previous API in the Rust bindings accepted owned <code>Vec</code>s as input. By taking ownership of a heap-allocated data structure, the function also takes responsibility for freeing the memory at the conclusion of its execution. This is undesirable as it forecloses the possibility of buffer reuse. Additionally, categorical features are passed as owned <code>String</code>s, which prevents us from passing references to bytes in the original request. Instead, we must allocate a temporary <code>String</code> on the heap and copy bytes into it.</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">pub fn calc_model_prediction(\n\t&amp;self,\n\tfloat_features: Vec&lt;Vec&lt;f32&gt;&gt;,\n\tcat_features: Vec&lt;Vec&lt;String&gt;&gt;,\n) -&gt; CatBoostResult&lt;Vec&lt;f64&gt;&gt; { ... }\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's rewrite this function to take <code>&amp;[f32]</code> and <code>&amp;[&amp;str]</code>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">pub fn calc_model_prediction_single(\n\t&amp;self,\n\tfloat_features: &amp;[f32],\n\tcat_features: &amp;[&amp;str],\n) -&gt; CatBoostResult&lt;f64&gt; { ... }\n</code></pre>\n<!--kg-card-end: markdown--><p>But, we also execute several models per request, and those models may use the same categorical features. Instead of calculating the hash for each separate model we execute, let's compute the hashes first and then pass them to each model that requires them:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">pub fn calc_model_prediction_single_with_hashed_cat_features(\n\t&amp;self,\n\tfloat_features: &amp;[f32],\n\thashed_cat_features: &amp;[i32],\n) -&gt; CatBoostResult&lt;f64&gt; { ... }\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"summary\">Summary</h2><p>By optimizing away unnecessary memory allocations in the bot management module, we reduced P50 latency from 388us to 309us (20%), and reduced P99 latency from 940us to 813us (14%). And, in many cases, the optimized code is shorter and easier to read than the unoptimized implementation.</p><p>These optimizations were targeted at model execution in the bot management module. To learn more about how we are porting bot management from Lua to Rust, check out <a href=\"http://blog.cloudflare.com/scalable-machine-learning-at-cloudflare/\">this blog post</a>.</p>",
		"comment_id": "648c531e3bb168000add45fa",
		"feature_image": "http://blog.cloudflare.com/content/images/2023/06/image1-5.png",
		"featured": false,
		"visibility": "public",
		"created_at": "2023-06-16T13:18:38.000+01:00",
		"updated_at": "2023-12-13T16:10:42.000+00:00",
		"published_at": "2023-06-19T14:00:13.000+01:00",
		"custom_excerpt": "How we optimized bot management’s machine learning model execution",
		"codeinjection_head": null,
		"codeinjection_foot": null,
		"custom_template": null,
		"canonical_url": null,
		"authors": [
			{
				"id": "648784d03bb168000add455f",
				"name": "Austin Hartzheim",
				"slug": "austinhartzheim",
				"profile_image": "http://blog.cloudflare.com/content/images/2023/06/ah.jpg",
				"cover_image": null,
				"bio": null,
				"website": "https://austinhartzheim.me/",
				"location": null,
				"facebook": null,
				"twitter": null,
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/austinhartzheim/"
			}
		],
		"tags": [
			{
				"id": "648c76c53bb168000add463f",
				"name": "#BLOG-1890",
				"slug": "hash-blog-1890",
				"description": null,
				"feature_image": null,
				"visibility": "internal",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/404/"
			},
			{
				"id": "5d16450341acde0011a952fa",
				"name": "Speed Week",
				"slug": "speed-week",
				"description": "Blog posts and announcements celebrating Speed Week at Cloudflare.",
				"feature_image": "http://blog.cloudflare.com/content/images/2023/06/Speed-Week-2023---Dark.png",
				"visibility": "public",
				"meta_title": null,
				"meta_description": "Blog posts and announcements celebrating Speed Week at Cloudflare.",
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/speed-week/"
			},
			{
				"id": "5d87c015fd9e450505d9658f",
				"name": "Bots",
				"slug": "bots",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/bots/"
			},
			{
				"id": "5d16450341acde0011a9523c",
				"name": "Rust",
				"slug": "rust",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/rust/"
			},
			{
				"id": "5eb280586fa3db01bc6a31b2",
				"name": "Bot Management",
				"slug": "bot-management",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/bot-management/"
			}
		],
		"primary_author": {
			"id": "648784d03bb168000add455f",
			"name": "Austin Hartzheim",
			"slug": "austinhartzheim",
			"profile_image": "http://blog.cloudflare.com/content/images/2023/06/ah.jpg",
			"cover_image": null,
			"bio": null,
			"website": "https://austinhartzheim.me/",
			"location": null,
			"facebook": null,
			"twitter": null,
			"meta_title": null,
			"meta_description": null,
			"url": "http://blog.cloudflare.com/author/austinhartzheim/"
		},
		"primary_tag": null,
		"url": "http://blog.cloudflare.com/how-cloudflare-runs-ml-inference-in-microseconds/",
		"excerpt": "How we optimized bot management’s machine learning model execution",
		"reading_time": 6,
		"access": true,
		"comments": false,
		"og_image": "http://blog.cloudflare.com/content/images/2023/06/How-Cloudflare-runs-machine-learning-inference-in-microseconds-OG-1.png",
		"og_title": null,
		"og_description": null,
		"twitter_image": "http://blog.cloudflare.com/content/images/2023/06/How-Cloudflare-runs-machine-learning-inference-in-microseconds-OG.png",
		"twitter_title": null,
		"twitter_description": null,
		"meta_title": null,
		"meta_description": "How we optimized bot management’s machine learning model execution.",
		"email_subject": null,
		"frontmatter": null,
		"feature_image_alt": "How Cloudflare runs machine learning inference in microseconds",
		"feature_image_caption": null
	},
	"locale": "en-us"
}