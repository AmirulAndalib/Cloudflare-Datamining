{
	"post": {
		"id": "5e720f639a5a0d03f5ec4a36",
		"uuid": "28d79d38-5b1a-49f4-a56c-9c2074c07438",
		"title": "Keepalives considered harmful",
		"slug": "keepalives-considered-harmful",
		"html": "<p>This may sound like a weird title, but hear me out. You’d think keepalives would always be helpful, but turns out reality isn’t always what you expect it to be. It really helps if you read <a href=\"http://blog.cloudflare.com/the-sad-state-of-linux-socket-balancing/\">Why does one NGINX worker take all the load?</a> first. This post is an adaptation of a rather old post on Cloudflare’s internal blog, so not all details are exactly as they are in production today but the lessons are still valid.<br></p><p>This is a story about how we were seeing some complaints about sporadic latency spikes, made some unconventional changes, and were able to slash the 99.9th latency percentile by 4x!</p><h3 id=\"request-flow-on-cloudflare-edge\">Request flow on Cloudflare edge</h3><p>I'm going to focus only on two parts of our edge stack: FL and SSL.</p><ul><li>FL accepts plain HTTP connections and does the main request logic, including our WAF</li><li>SSL terminates SSL and passes connections to FL over local Unix socket:</li></ul><p>Here’s a diagram:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/03/Screen-Shot-2020-03-19-at-12.22.41.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>These days we route all traffic through SSL for simplicity, but in the grand scheme of things it’s not going to matter much.</p><p>Each of these processes is not itself a single process, but rather a main process and a collection of workers that do actual processing. Another diagram for you to make this more visual:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/03/Screen-Shot-2020-03-19-at-12.22.55.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><h3 id=\"keepalives\">Keepalives</h3><p>Requests come over connections that are reused for performance reasons, which is sometimes referred to as “keepalive”. It's generally expensive for a client to open a new TCP connection and our servers keep some memory pools associated with connections that need to be recycled. In fact, one of the range of mitigations we use for attack traffic is disabling keepalives for abusive clients, forcing them to reopen connections, which slows them down considerably.</p><p>To illustrate the usefulness of keepalives, here's me requesting <a href=\"http://example.com/\">http://example.com/</a> from curl over the same connection:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">$ curl -s -w &quot;Time to connect: %{time_connect} Time to first byte: %{time_starttransfer}\\n&quot; -o /dev/null http://example.com/ -o /dev/null http://example.com/\n\nTime to connect: 0.012108 Time to first byte: 0.018724\nTime to connect: 0.000019 Time to first byte: 0.007391</code></pre>\n<!--kg-card-end: markdown--><p>The first request took 18.7ms and out of them 12.1ms were used to establish a new connection, which is not even a TLS one. When I sent another request over the same connection, I didn't need to pay extra and it took just 7.3ms to service my request. That's a big win, which gets even bigger if you need to establish a brand new connection (especially if it’s not <a href=\"https://www.cloudflare.com/learning-resources/tls-1-3/\">TLSv1.3</a> or <a href=\"http://blog.cloudflare.com/http3-the-past-present-and-future/\">QUIC</a>). DNS was also cached in the example above, but it may be another negative factor for domains with low TTL.</p><p>Keepalives tend to be used extensively, because they seem beneficial. Generally keepalives are enabled by default for this exact reason.</p><h3 id=\"taking-all-the-load\">Taking all the load</h3><p>Due to how Linux works (see the first link in this blog post), most of the load goes to a few workers out of the pool. When that worker is not able to accept() a pending connection because it's busy processing requests, that connection spills to another worker. The process cascades until some worker is ready to pick up.</p><p>This leaves us with a ladder of load spread between workers:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">               CPU%                                       Runtime\nnobody    4254 51.2  0.8 5655600 1093848 ?     R    Aug23 3938:34 nginx: worker process\nnobody    4257 47.9  0.8 5615848 1071612 ?     S    Aug23 3682:05 nginx: worker process\nnobody    4253 43.8  0.8 5594124 1069424 ?     R    Aug23 3368:27 nginx: worker process\nnobody    4255 39.4  0.8 5573888 1070272 ?     S    Aug23 3030:01 nginx: worker process\nnobody    4256 36.2  0.7 5556700 1052560 ?     R    Aug23 2784:23 nginx: worker process\nnobody    4251 33.1  0.8 5563276 1063700 ?     S    Aug23 2545:07 nginx: worker process\nnobody    4252 29.2  0.8 5561232 1058748 ?     S    Aug23 2245:59 nginx: worker process\nnobody    4248 26.7  0.8 5554652 1057288 ?     S    Aug23 2056:19 nginx: worker process\nnobody    4249 24.5  0.7 5537276 1043568 ?     S    Aug23 1883:18 nginx: worker process\nnobody    4245 22.5  0.7 5552340 1048592 ?     S    Aug23 1736:37 nginx: worker process\nnobody    4250 20.7  0.7 5533728 1038676 ?     R    Aug23 1598:16 nginx: worker process\nnobody    4247 19.6  0.7 5547548 1044480 ?     S    Aug23 1507:27 nginx: worker process\nnobody    4246 18.4  0.7 5538104 1043452 ?     S    Aug23 1421:23 nginx: worker process\nnobody    4244 17.5  0.7 5530480 1035264 ?     S    Aug23 1345:39 nginx: worker process\nnobody    4243 16.6  0.7 5529232 1024268 ?     S    Aug23 1281:55 nginx: worker process\nnobody    4242 16.6  0.7 5537956 1038408 ?     R    Aug23 1278:40 nginx: worker process\n</code></pre>\n<!--kg-card-end: markdown--><p>The third column is instant CPU%, the time after the date is total on-CPU time. If you look at the the same processes and count their open sockets, you'll see this (using the same order of processes as above):</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">4254 2357\n4257 1833\n4253 2180\n4255 1609\n4256 1565\n4251 1519\n4252 1175\n4248 1065\n4249 1056\n4245 1201\n4250 886\n4247 908\n4246 968\n4244 1180\n4243 867\n4242 884\n</code></pre>\n<!--kg-card-end: markdown--><p>More load corresponds to more open sockets. More open sockets generate more load to serve these connections. It’s a vicious circle.</p><h3 id=\"the-twist\">The twist</h3><p>Now that we have all these workers holding onto this connections, requests over these connections are also in a way pinned to workers:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/03/Screen-Shot-2020-03-19-at-12.23.16.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>In FL we're doing some things that are <em>somewhat</em> compute intensive, which means that some workers can be busy for a <em>somewhat</em> prolonged period of time, while other workers will be sitting idle, increasing latency for requests. Ideally we want to always hand over a request to a worker that's idle, because that maximizes our chances of not being blocked, even if it takes a few event loop iterations to serve a request (meaning that we may still block down the road).</p><p>One part of dealing with the issue is <a href=\"http://blog.cloudflare.com/the-problem-with-event-loops/\">offloading some of the compute intensive parts</a> of the request processing into a thread pool, but that was something that hadn’t happened at that point. We had to do something else in the meantime.</p><p>Clients have no way of knowing that their worker is busy and they should probably ask another worker to serve the connection. For clients over a real network this doesn't even make sense, since by the time their request comes to NGINX up to tens of milliseconds later, the situation will probably be different.</p><p>But our clients are not over a long haul network! They are local and they are SSL connecting over a Unix socket. It's not exactly that expensive to reopen a new connection for each request and what it gives is the ability to pass a fully formed request that's buffered in SSL into FL:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/03/Screen-Shot-2020-03-19-at-12.23.28.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Two key points here:</p><ul><li>The request is always picked up by an idle worker</li><li>The request is readily available for potentially compute intensive processing</li></ul><p>The former is the most important part.</p><h3 id=\"validating-the-hypothesis\">Validating the hypothesis</h3><p>To validate this assumption, I wrote the following program:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">package main\n \nimport (\n    &quot;flag&quot;\n    &quot;io/ioutil&quot;\n    &quot;log&quot;\n    &quot;net/http&quot;\n    &quot;os&quot;\n    &quot;time&quot;\n)\n \nfunc main() {\n    u := flag.String(&quot;url&quot;, &quot;&quot;, &quot;url to request&quot;)\n    p := flag.Duration(&quot;pause&quot;, 0, &quot;pause between requests&quot;)\n    t := flag.Duration(&quot;threshold&quot;, 0, &quot;threshold for reporting&quot;)\n    c := flag.Int(&quot;count&quot;, 10000, &quot;number of requests to send&quot;)\n    n := flag.Int(&quot;close&quot;, 1, &quot;close connection after every that many requests&quot;)\n \n    flag.Parse()\n \n    if *u == &quot;&quot; {\n        flag.PrintDefaults()\n        os.Exit(1)\n    }\n \n    client := http.Client{}\n \n    for i := 0; i &lt; *c; i++ {\n        started := time.Now()\n \n        request, err := http.NewRequest(&quot;GET&quot;, *u, nil)\n        if err != nil {\n            log.Fatalf(&quot;Error constructing request: %s&quot;, err)\n        }\n \n        if i%*n == 0 {\n            request.Header.Set(&quot;Connection&quot;, &quot;Close&quot;)\n        }\n \n        response, err := client.Do(request)\n        if err != nil {\n            log.Fatalf(&quot;Error performing request: %s&quot;, err)\n        }\n \n        _, err = ioutil.ReadAll(response.Body)\n        if err != nil {\n            log.Fatalf(&quot;Error reading request body: %s&quot;, err)\n        }\n \n        response.Body.Close()\n \n        elapsed := time.Since(started)\n        if elapsed &gt; *t {\n            log.Printf(&quot;Request %d took %dms&quot;, i, int(elapsed.Seconds()*1000))\n        }\n \n        time.Sleep(*p)\n    }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>The program connects to a requested URL and recycles the connection after X requests have completed. We also pause for a short time between requests.</p><p>If we close a connection after every request:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">$ go run /tmp/main.go -url http://test.domain/cdn-cgi/trace -count 10000 -pause 5ms -threshold 20ms -close 1\n2018/08/24 23:42:34 Request 453 took 32ms\n2018/08/24 23:42:38 Request 1044 took 24ms\n2018/08/24 23:43:00 Request 4106 took 83ms\n2018/08/24 23:43:12 Request 5778 took 27ms\n2018/08/24 23:43:16 Request 6292 took 27ms\n2018/08/24 23:43:20 Request 6856 took 21ms\n2018/08/24 23:43:32 Request 8578 took 45ms\n2018/08/24 23:43:42 Request 9938 took 22ms\n</code></pre>\n<!--kg-card-end: markdown--><p>We request an endpoint that's served in FL by Lua, so seeing any slow requests is unfortunate. There's an element of luck in this game and our program sees no cooperation from eyeballs or SSL, so it's somewhat expected.</p><p>Now, if we start closing the connection only after every other request, the situation immediately gets a lot worse:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">$ go run /tmp/main.go -url http://teste1.cfperf.net/cdn-cgi/trace -count 10000 -pause 5ms -threshold 20ms -close 2\n2018/08/24 23:43:51 Request 162 took 22ms\n2018/08/24 23:43:51 Request 220 took 21ms\n2018/08/24 23:43:53 Request 452 took 23ms\n2018/08/24 23:43:54 Request 540 took 41ms\n2018/08/24 23:43:54 Request 614 took 23ms\n2018/08/24 23:43:56 Request 900 took 40ms\n2018/08/24 23:44:02 Request 1705 took 21ms\n2018/08/24 23:44:03 Request 1850 took 27ms\n2018/08/24 23:44:03 Request 1878 took 36ms\n2018/08/24 23:44:08 Request 2470 took 21ms\n2018/08/24 23:44:11 Request 2926 took 22ms\n2018/08/24 23:44:14 Request 3350 took 37ms\n2018/08/24 23:44:14 Request 3404 took 21ms\n2018/08/24 23:44:16 Request 3598 took 32ms\n2018/08/24 23:44:16 Request 3606 took 22ms\n2018/08/24 23:44:19 Request 4026 took 33ms\n2018/08/24 23:44:20 Request 4250 took 74ms\n2018/08/24 23:44:22 Request 4483 took 20ms\n2018/08/24 23:44:23 Request 4572 took 21ms\n2018/08/24 23:44:23 Request 4644 took 23ms\n2018/08/24 23:44:24 Request 4758 took 63ms\n2018/08/24 23:44:25 Request 4808 took 39ms\n2018/08/24 23:44:30 Request 5496 took 28ms\n2018/08/24 23:44:31 Request 5736 took 88ms\n2018/08/24 23:44:32 Request 5845 took 43ms\n2018/08/24 23:44:33 Request 5988 took 52ms\n2018/08/24 23:44:34 Request 6042 took 26ms\n2018/08/24 23:44:34 Request 6049 took 23ms\n2018/08/24 23:44:40 Request 6872 took 86ms\n2018/08/24 23:44:40 Request 6940 took 23ms\n2018/08/24 23:44:40 Request 6964 took 23ms\n2018/08/24 23:44:44 Request 7532 took 32ms\n2018/08/24 23:44:49 Request 8224 took 22ms\n2018/08/24 23:44:49 Request 8234 took 29ms\n2018/08/24 23:44:51 Request 8536 took 24ms\n2018/08/24 23:44:55 Request 9028 took 22ms\n2018/08/24 23:44:55 Request 9050 took 23ms\n2018/08/24 23:44:55 Request 9092 took 26ms\n2018/08/24 23:44:57 Request 9330 took 25ms\n2018/08/24 23:45:01 Request 9962 took 48ms\n</code></pre>\n<!--kg-card-end: markdown--><p>If we close after every 5 requests, the number of slow responses almost doubles. This is counter-intuitive, keepalives are supposed to help with latency, not make it worse!</p><h3 id=\"trying-this-in-the-wild\">Trying this in the wild</h3><p>To see how this works out in the real world, we disabled keepalives between SSL and FL in one location, forcing SSL to send every request over a separate connection (remember: cheap local Unix socket). Here’s how our probes toward that location reacted to this:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/03/image5-6.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Here’s cumulative wait time between SSL and FL:<br></p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/03/image3-3.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>And finally 99.9th percentile of the same measurement:<br></p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/03/image1-8.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>This is a huge win.</p><p>Another telling graph is comparing our average “edge processing time” (which includes WAF) in a test location to a global value:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/03/image2-5.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>We reduced unnecessary wait time due to an imbalance without increasing the CPU load, which directly translates into improved user experience and lower resource consumption for us.</p><h3 id=\"the-downsides\">The downsides</h3><p>There has to be a downside from this, right? The problem we introduced is that CPU imbalance between individual CPU cores went up:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/03/image4-3.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Overall CPU usage did not change, just the distribution of it. We already know of a way of dealing with this: SO_REUSEPORT. Either that or <a href=\"https://lkml.org/lkml/2015/2/17/518\">EPOLLROUNDROBIN</a>, which doesn’t have some of the drawbacks of SO_REUSEPORT (which does not work for a Unix socket, for example), but requires a patched kernel. If we combine both disabled keepalives and EPOLLROUNDROBIN changes, we can see CPUs allocated to FL converge in their utilization nicely:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/03/image6-3.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>We’ve tried different combinations and having both EPOLLROUNDROBIN with disabled keepalives worked best. Having just one of them is not as beneficial to lower latency.</p><h3 id=\"conclusions\">Conclusions</h3><p>We disabled keepalives between SSL and FL running on the same box and this greatly improved our tail latency caused by requests landing on non-optimal FL workers. This was an unexpected fix, but it worked and we are able to explain it.</p><p>This doesn’t mean that you should go and disable keepalives everywhere. Generally keepalives are great and should stay enabled, but in our case the latency of local connection establishment is much lower than the delay we can get from landing on a busy worker.</p><p>In reality this means that we can run our machines hotter and not see latency rise as much as it did before. Imagine moving the CPU cap from 50% to 80% with no effect on latency. The numbers are arbitrary, but the idea holds. Running hotter allows for fewer machines able to serve the same amount of traffic, reducing our overall footprint. ?<br></p>",
		"comment_id": "5e720f639a5a0d03f5ec4a36",
		"feature_image": "http://blog.cloudflare.com/content/images/2020/03/featured.png",
		"featured": false,
		"visibility": "public",
		"created_at": "2020-03-18T12:09:07.000+00:00",
		"updated_at": "2020-08-21T00:32:55.000+01:00",
		"published_at": "2020-03-19T12:57:35.000+00:00",
		"custom_excerpt": "You’d think keepalives would always be helpful, but turns out reality isn’t always what you expect it to be. It really helps if you read Why does one NGINX worker take all the load? first.",
		"codeinjection_head": null,
		"codeinjection_foot": null,
		"custom_template": null,
		"canonical_url": null,
		"tags": [
			{
				"id": "5d16450341acde0011a95167",
				"name": "NGINX",
				"slug": "nginx",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/nginx/"
			},
			{
				"id": "5d16450341acde0011a951ff",
				"name": "Linux",
				"slug": "linux",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/linux/"
			},
			{
				"id": "5e7369969a5a0d03f5ec4b7e",
				"name": "Performance",
				"slug": "performance",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/performance/"
			}
		],
		"authors": [
			{
				"id": "5d1644b141acde0011a94f77",
				"name": "Ivan Babrou",
				"slug": "ivan",
				"profile_image": "http://blog.cloudflare.com/content/images/2022/09/Ivan-Babrou.png",
				"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-84.png",
				"bio": null,
				"website": null,
				"location": null,
				"facebook": null,
				"twitter": null,
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/ivan/"
			}
		],
		"primary_author": {
			"id": "5d1644b141acde0011a94f77",
			"name": "Ivan Babrou",
			"slug": "ivan",
			"profile_image": "http://blog.cloudflare.com/content/images/2022/09/Ivan-Babrou.png",
			"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-84.png",
			"bio": null,
			"website": null,
			"location": null,
			"facebook": null,
			"twitter": null,
			"meta_title": null,
			"meta_description": null,
			"url": "http://blog.cloudflare.com/author/ivan/"
		},
		"primary_tag": {
			"id": "5d16450341acde0011a95167",
			"name": "NGINX",
			"slug": "nginx",
			"description": null,
			"feature_image": null,
			"visibility": "public",
			"meta_title": null,
			"meta_description": null,
			"og_image": null,
			"og_title": null,
			"og_description": null,
			"twitter_image": null,
			"twitter_title": null,
			"twitter_description": null,
			"codeinjection_head": null,
			"codeinjection_foot": null,
			"canonical_url": null,
			"accent_color": null,
			"url": "http://blog.cloudflare.com/tag/nginx/"
		},
		"url": "http://blog.cloudflare.com/keepalives-considered-harmful/",
		"excerpt": "You’d think keepalives would always be helpful, but turns out reality isn’t always what you expect it to be. It really helps if you read Why does one NGINX worker take all the load? first.",
		"reading_time": 10,
		"access": true,
		"comments": false,
		"og_image": null,
		"og_title": null,
		"og_description": null,
		"twitter_image": null,
		"twitter_title": null,
		"twitter_description": null,
		"meta_title": null,
		"meta_description": "You’d think keepalives would always be helpful, but turns out reality isn’t always what you expect it to be. It really helps if you read Why does one NGINX worker take all the load? first.",
		"email_subject": null,
		"frontmatter": null,
		"feature_image_alt": null,
		"feature_image_caption": null
	},
	"locale": "en-us"
}