<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image3-13.png" class="kg-image" alt="Build applications of any size on Cloudflare with the Queues open beta" loading="lazy"></figure>
	<p>Message queues are a fundamental building block of cloud applications—and today the <a href="https://developers.cloudflare.com/queues/?ref=blog.cloudflare.com" target="_blank">Cloudflare Queues</a> open beta brings queues to every developer building for Region: Earth. Cloudflare Queues follows <a href="https://www.cloudflare.com/products/workers/?ref=blog.cloudflare.com" target="_blank">Cloudflare Workers</a> and <a href="https://www.cloudflare.com/products/r2/?ref=blog.cloudflare.com" target="_blank">Cloudflare R2</a> in a long line of innovative services built for the Workers Developer Platform, enabling developers to build more complex applications without configuring networks, choosing regions, or estimating capacity. Best of all, like many other Cloudflare services, there are no egregious egress charges!</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image8.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>If you’ve ever purchased something online and seen a message like “you will receive confirmation of your order shortly,” you’ve interacted with a queue. When you completed your order, your shopping cart and information were stored and the order was placed into a queue. At some later point, the order fulfillment service picks and packs your items and hands it off to the shipping service—again, via a queue. Your order may sit for only a minute, or much longer if an item is out of stock or a warehouse is busy, and queues enable all of this functionality.</p>
	<p>Message queues are great at decoupling components of applications, like the checkout and order fulfillment services for an ecommerce site. Decoupled services are easier to reason about, deploy, and implement, allowing you to ship features that delight your customers without worrying about synchronizing complex deployments.</p>
	<p>Queues also allow you to batch and buffer calls to downstream services and APIs. This post shows you how to enroll in the open beta, walks you through a practical example of using Queues to build a log sink, and tells you how we built Queues using other Cloudflare services. You’ll also learn a bit about the roadmap for the open beta.</p>
	<h2 id="getting-started">Getting started</h2>
	<h3 id="enrolling-in-the-open-beta">Enrolling in the open beta</h3>
	<p>Open the <a href="https://dash.cloudflare.com/?ref=blog.cloudflare.com" target="_blank">Cloudflare dashboard</a> and navigate to the <em>Workers</em> section. Select <em>Queues</em> from the <em>Workers</em> navigation menu and choose <em>Enable Queues Beta</em>.</p>
	<p>Review your order and choose <em>Proceed to Payment Details</em>.</p>
	<p><strong>Note: If you are not already subscribed to a </strong><a href="https://www.cloudflare.com/plans/developer-platform/?ref=blog.cloudflare.com#overview" target="_blank"><strong>Workers Paid Plan</strong></a><strong>, one will be added to your order automatically.</strong></p>
	<p>Enter your payment details and choose <em>Complete Purchase</em>. That’s it - you’re enrolled in the open beta! Choose <em>Return to Queues</em> on the confirmation page to return to the Cloudflare Queues home page.</p>
	<h3 id="creating-your-first-queue">Creating your first queue</h3>
	<p>After enabling the open beta, open the Queues home page and choose <em>Create Queue</em>. Name your queue `my-first-queue` and choose <em>Create queue</em>. That’s all there is to it!</p>
	<p>The dash displays a confirmation message along with a list of all the queues in your account.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image1-18.png" class="kg-image" alt="" loading="lazy"></figure>
	<p><strong>Note: As of the writing of this blog post each account is limited to ten queues. We intend to raise this limit as we build towards general availability.</strong></p>
	<h3 id="managing-your-queues-with-wrangler">Managing your queues with Wrangler</h3>
	<p>You can also manage your queues from the command line using <a href="https://github.com/cloudflare/wrangler2?ref=blog.cloudflare.com" target="_blank">Wrangler</a>, the CLI for Cloudflare Workers. In this section, you build a simple but complete application implementing a log aggregator or sink to learn how to integrate Workers, Queues, and R2.</p>
	<p><strong>Setting up resources</strong><br>To create this application, you need access to a Cloudflare Workers account with a subscription plan, access to the Queues open beta, and an R2 plan.</p>
	<p><a href="https://developers.cloudflare.com/workers/get-started/guide/?ref=blog.cloudflare.com#1-install-wrangler-workers-cli" target="_blank">Install</a> and <a href="https://developers.cloudflare.com/workers/get-started/guide/?ref=blog.cloudflare.com#2-authenticate-wrangler" target="_blank">authenticate</a> Wrangler then run <code>wrangler queues create log-sink</code> from the command line to create a queue for your application.</p>
	<p>Run <code>wrangler queues list</code> and note that Wrangler displays your new queue.</p>
	<p><strong>Note: The following screenshots use the </strong><a href="https://stedolan.github.io/jq/?ref=blog.cloudflare.com" target="_blank"><strong>jq</strong></a><strong> utility to format the JSON output of wrangler commands. You </strong><em><strong>do not</strong></em><strong> need to install jq to complete this application.</strong></p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/8-create-and-list-a-queue.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>Finally, run <code>wrangler r2 bucket create log-sink</code> to create an R2 bucket to store your aggregated logs. After the bucket is created, run <code>wrangler r2 bucket list</code> to see your new bucket.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/9-create-and-list-a-bucket.png" class="kg-image" alt="" loading="lazy"></figure>
	<p><strong>Creating your Worker</strong><br>Next, create a Workers application with two handlers: a <code>fetch()</code> handler to receive individual incoming log lines and a <code>queue()</code> handler to aggregate a batch of logs and write the batch to R2.</p>
	<p>In an empty directory, run <code>wrangler init</code> to create a new Cloudflare Workers application. When prompted:</p>
	<ul>
		<li>Choose “y” to create a new <em>package.json</em></li>
		<li>Choose “y” to use TypeScript</li>
		<li>Choose “Fetch handler” to create a new Worker at <em>src/index.ts</em></li>
	</ul>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/10-wrangler-init.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>Open <em>wrangler.toml</em> and replace the contents with the following:</p>
	<h4 id="wrangler-toml"><strong><em>wrangler.toml</em></strong></h4><!--kg-card-begin: markdown-->
	<pre><code class="language-toml">name = "queues-open-beta"
main = "src/index.ts"
compatibility_date = "2022-11-03"
 
 
[[queues.producers]]
 queue = "log-sink"
 binding = "BUFFER"
 
[[queues.consumers]]
 queue = "log-sink"
 max_batch_size = 100
 max_batch_timeout = 30
 
[[r2_buckets]]
 bucket_name = "log-sink"
 binding = "LOG_BUCKET"
</code></pre>
	<!--kg-card-end: markdown-->
	<p>The <code>[[queues.producers]]</code> section creates a <em>producer</em> binding for the Worker at <em>src/index.ts</em> called <code>BUFFER</code> that refers to the <em>log-sink</em> queue. This Worker can place messages onto the <em>log-sink</em> queue by calling <code>await env.BUFFER.send(log);</code></p>
	<p>The <code>[[queues.consumers]]</code> section creates a <em>consumer</em> binding for the <em>log-sink</em> queue for your Worker. Once the <em>log-sink</em> queue has a batch ready to be processed (or <em>consumed</em>), the Workers runtime will look for the <code>queue()</code> event handler in <em>src/index.ts</em> and invoke it, passing the batch as an argument. The <em>queue()</em> function signature looks as follows:</p>
	<p><code>async queue(batch: MessageBatch&lt;Error&gt;, env: Environment): Promise&lt;void&gt; {</code></p>
	<p>The final binding in your <em>wrangler.toml</em> creates a binding for the <em>log-sink</em> R2 bucket that makes the bucket available to your Worker via <em>env.LOG_BUCKET</em>.</p>
	<h4 id="src-index-ts"><strong><em>src/index.ts</em></strong></h4>
	<p>Open <em>src/index.ts</em> and replace the contents with the following code:</p><!--kg-card-begin: markdown-->
	<pre><code class="language-typescript">export interface Env {
 BUFFER: Queue;
 LOG_BUCKET: R2Bucket;
}
 
export default {
 async fetch(request: Request, env: Environment): Promise&lt;Response&gt; {
   let log = await request.json();
   await env.BUFFER.send(log);
   return new Response("Success!");
 },
 async queue(batch: MessageBatch&lt;Error&gt;, env: Environment): Promise&lt;void&gt; {
   const logBatch = JSON.stringify(batch.messages);
   await env.LOG_BUCKET.put(`logs/${Date.now()}.log.json`, logBatch);
 },
};

</code></pre>
	<!--kg-card-end: markdown-->
	<p>The <code>export interface Env</code> section exposes the two bindings you defined in <em>wrangler.toml</em>: a queue named <em>BUFFER</em> and an R2 bucket named <em>LOG_BUCKET</em>.</p>
	<p>The <code>fetch()</code> handler transforms the request body into JSON, adds the body to the <em>BUFFER</em> queue, then returns an HTTP 200 response with the message <em>Success!</em></p>
	<p>The `queue()` handler receives a batch of messages that each contain log entries, iterates through concatenating each log into a string buffer, then writes that buffer to the <em>LOG_BUCKET</em> R2 bucket using the current timestamp as the filename.</p>
	<p><strong>Publishing and running your application</strong><br>To publish your log sink application, run <code>wrangler publish</code>. Wrangler packages your application and its dependencies and deploys it to Cloudflare’s global network.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/11-wrangler-publish.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>Note that the output of <code>wrangler publish</code> includes the <em>BUFFER</em> queue binding, indicating that this Worker is a <em>producer</em> and can place messages onto the queue. The final line of output also indicates that this Worker is a <em>consumer</em> for the <em>log-sink</em> queue and can read and remove messages from the queue.</p>
	<p>Use your favorite API client, like <a href="https://curl.se/?ref=blog.cloudflare.com" target="_blank">curl</a>, <a href="https://httpie.io/?ref=blog.cloudflare.com" target="_blank">httpie</a>, or <a href="https://www.postman.com/?ref=blog.cloudflare.com" target="_blank">Postman</a>, to send JSON log entries to the published URL for your Worker via HTTP POST requests. Navigate to your <em>log-sink</em> R2 bucket in the <a href="https://dash.cloudflare.com/?ref=blog.cloudflare.com" target="_blank">Cloudflare dashboard</a> and note that the <em>logs</em> prefix is now populated with aggregated logs from your request.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/image4-9.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>Download and open one of the logfiles to view the JSON array inside. That’s it - with fewer than 45 lines of code and config, you’ve built a log aggregator to ingest and store data in R2!</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/11/13-aggregated-log-content.png" class="kg-image" alt="" loading="lazy"></figure>
	<h2 id="buffering-r2-writes-with-queues-in-the-real-world">Buffering R2 writes with Queues in the real world</h2>
	<p>In the previous example, you create a simple Workers application that buffers data into batches before writing the batches to R2. This reduces the number of calls to the downstream service, reducing load on the service and saving you money.</p>
	<p><a href="https://uuid.rocks/?ref=blog.cloudflare.com" target="_blank">UUID.rocks</a>, the fastest UUIDv4-as-a-service, wanted to confirm whether their API truly generates unique IDs on every request. With 80,000 requests per day, it wasn’t trivial to find out. They decided to write every generated UUID to R2 to compare IDs across the entire population. However, writing directly to R2 at the rate UUIDs are generated is inefficient and expensive.</p>
	<p>To reduce writes and costs, UUID.rocks introduced Cloudflare Queues into their UUID generation workflow. Each time a UUID is requested, a Worker places the value of the UUID into a queue. Once enough messages have been received, the buffered batch of JSON objects is written to R2. This avoids invoking an R2 write on every API call, saving costs and making the data easier to process later.</p>
	<p>The <a href="https://github.com/jahands/uuid-queue?ref=blog.cloudflare.com" target="_blank">uuid-queue application</a> consists of a single Worker with three event handlers:</p>
	<ol>
		<li>A fetch handler that receives a JSON object representing the generated UUID and writes it to a Cloudflare Queue.</li>
		<li>A queue handler that writes batches of JSON objects to R2 in CSV format.</li>
		<li>A scheduled handler that combines batches from the previous hour into a single file for future processing.</li>
	</ol>
	<p>To view the source or deploy this application into your own account, <a href="https://github.com/jahands/uuid-queue?ref=blog.cloudflare.com" target="_blank">visit the repository on GitHub</a>.</p>
	<h2 id="how-we-built-cloudflare-queues">How we built Cloudflare Queues</h2>
	<p>Like many of the Cloudflare services you use and love, we built Queues by composing other Cloudflare services like Workers and Durable Objects. This enabled us to rapidly solve two difficult challenges: securely invoking your Worker from our own service and maintaining a strongly consistent state at scale. Several recent Cloudflare innovations helped us overcome these challenges.</p>
	<h3 id="securely-invoking-your-worker">Securely invoking your Worker</h3>
	<p>In the<em> Before Times</em> (early 2022), invoking one Worker from another Worker meant a fresh HTTP call from inside your script. This was a brittle experience, requiring you to know your downstream endpoint at deployment time. Nested invocations ran as HTTP calls, passing all the way through the Cloudflare network a second time and adding latency to your request. It also meant security was on you - if you wanted to control how that second Worker was invoked, you had to create and implement your own authentication and authorization scheme.</p>
	<p><strong>Worker to Worker requests</strong><br>During Platform Week in May 2022, <a href="https://blog.cloudflare.com/service-bindings-ga/">Service Worker Bindings</a> entered general availability. With Service Worker Bindings, your Worker code has a binding to another Worker in your account that you invoke directly, avoiding the network penalty of a nested HTTP call. This removes the performance and security barriers discussed previously, but it still requires that you hard-code your nested Worker at compile time. You can think of this setup as “static dispatch,” where your Worker has a static reference to another Worker where it can dispatch events.</p>
	<p><strong>Dynamic dispatch</strong><br>As Service Worker Bindings entered general availability, we also <a href="https://blog.cloudflare.com/workers-for-platforms-ga/">launched a closed beta</a> of Workers for Platforms, our tool suite to help make any product programmable. With Workers for Platforms, software as a service (SaaS) and platform providers can allow users to upload their own scripts and run them safely via Cloudflare Workers. User scripts are not known at compile time, but are <em>dynamically dispatched</em> at runtime.</p>
	<p><a href="https://blog.cloudflare.com/workers-for-platforms-ga/">Workers for Platforms entered</a> general availability during <a href="https://www.cloudflare.com/ga-week/?ref=blog.cloudflare.com" target="_blank">GA week</a> in September 2022, and is available for all customers to build with today.</p>
	<p>With dynamic dispatch generally available, we now have the ability to discover and invoke Workers at runtime without the performance penalty of HTTP traffic over the network. We use dynamic dispatch to invoke your queue’s consumer Worker whenever a message or batch of messages is ready to be processed.</p>
	<h3 id="consistent-stateful-data-with-durable-objects">Consistent stateful data with Durable Objects</h3>
	<p>Another challenge we faced was storing messages durably without sacrificing performance. We took the design goal of ensuring that all messages were persisted to disk in multiple locations before we confirmed receipt of the message to the user. Again, we turned to an existing Cloudflare product—<a href="https://developers.cloudflare.com/workers/learning/using-durable-objects/?ref=blog.cloudflare.com" target="_blank">Durable Objects</a>—which <a href="https://blog.cloudflare.com/durable-objects-ga/">entered general availability nearly one year ago today</a>.</p>
	<p>Durable Objects are named instances of JavaScript classes that are guaranteed to be unique across Cloudflare’s entire network. Durable Objects process messages in-order and on a single-thread, allowing for coordination across messages and provide a strongly consistent storage API for key-value pairs. Offloading the hard problem of storing data durably in a distributed environment to Distributed Objects allowed us to reduce the time to build Queues and prepare it for open beta.</p>
	<h2 id="open-beta-roadmap">Open beta roadmap</h2>
	<p>Our open beta process empowers you to influence feature prioritization and delivery. We’ve set ambitious goals for ourselves on the path to general availability, most notably supporting unlimited throughput while maintaining 100% durability. We also have many other great features planned, like first-in first-out (FIFO) message processing and API compatibility layers to ease migrations, but we need your feedback to build what you need most, first.</p>
	<h2 id="conclusion">Conclusion</h2>
	<p>Cloudflare Queues is a global message queue for the Workers developer. Building with Queues makes your applications more performant, resilient, and cost-effective—but we’re not done yet. <a href="https://dash.cloudflare.com/?ref=blog.cloudflare.com" target="_blank">Join the Open Beta today</a> and <a href="https://discord.gg/aTsevRH3pG?ref=blog.cloudflare.com" target="_blank">share your feedback</a> to help shape the Queues roadmap as we deliver application integration services for the next generation cloud.</p>
</div>