<div class="mb2 gray5">8 min read</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/09/unnamed-2.png" class="kg-image" alt="The best place on Region: Earth for inference" loading="lazy" width="1600" height="900"></figure>
	<p>Heute ist die Workers-Plattform von Cloudflare der Ort, an dem über eine Million Entwickler und Entwicklerinnen anspruchsvolle Full-Stack-Anwendungen erstellen, wie sie früher nicht möglich gewesen wären.</p>
	<p>Natürlich war es nicht von Anfang an so. Alles begann an einem Tag wie diesem – mit einer Ankündigung der <a href="https://blog.cloudflare.com/introducing-cloudflare-workers">Birthday Week</a>. Das Produkt enthielt vielleicht nicht alle Funktionen, die es heute gibt, aber wenn Sie Workers bei seiner Markteinführung ausprobieren konnten, hatten Sie das Gefühl: „Das ist etwas anderes und es wird die Dinge verändern“. Plötzlich entstanden vollständig skalierbare, globale Anwendungen innerhalb von <em>Sekunden</em> und nicht erst nach Stunden, Tagen, Wochen oder gar Monaten. Es war der Beginn einer ganz neuen Art der Anwendungsentwicklung.</p>
	<p>Wenn Sie in den letzten Monaten mit generativer KI experimentiert haben, ging es Ihnen vielleicht ähnlich. Als wir Leute aus dem Freundes- und Kollegenkreis befragten, berichteten alle von verschiedenen „Aha“-Momenten, aber die allgemeine Meinung in der Branche ist derzeit einstimmig: Das ist etwas anderes und es wird die Dinge verändern.</p>
	<p>Heute freuen wir uns, eine Reihe von Ankündigungen zu machen, von denen wir glauben, dass sie die Zukunft der Datenverarbeitung ähnlich stark beeinflussen werden, wie dies für die Einführung von Workers gilt. Also ohne weitere Umschweife:</p>
	<ul>
		<li><strong>Workers AI</strong> (früher bekannt als Constellation), das <strong>auf NVIDIA-GPUs im globalen Netzwerk von Cloudflare läuft</strong> und das Serverless-Modell auf KI überträgt: Sie zahlen nur für das, was Sie nutzen, verlieren weniger Zeit für die Infrastruktur und gewinnen mehr Zeit für Ihre Anwendung.</li>
		<li><strong>Vectorize, unsere Vektordatenbank</strong>, macht es einfach, schnell und kostengünstig, Vektoren zu indizieren und zu speichern. Damit können Sie Anwendungsfälle unterstützen, die nicht nur Zugriff auf laufende Modelle, sondern auch auf benutzerdefinierte Daten benötigen.</li>
		<li><strong>AI Gateway</strong> gibt Unternehmen die Möglichkeit, ihre KI-Implementierungen <strong>zwischenzuspeichern, eine Durchsatzbegrenzung festzulegen und zu überwachen</strong>, unabhängig davon, wo sie ausgeführt werden.</li>
	</ul>
	<p>Aber das ist noch nicht alles.</p>
	<p>Die Umsetzung großer Projekte ist ein Mannschaftssport, und wir wollen nicht alles im Alleingang erledigen. Wie bei so vielen Projekten stehen wir auch hier auf den Schultern von Riesen. Wir freuen uns über die Zusammenarbeit mit einigen der größten Akteure in diesem Bereich: <strong>NVIDIA, Microsoft, Hugging Face und Meta</strong>.</p>
	<p>Unsere heutigen Ankündigungen sind nur der Anfang der Entwicklung von Cloudflare auf dem Gebiet der künstlichen Intelligenz, so wie es Workers vor sechs Jahren getan hat. Wir möchten Sie einladen, sich mit jeder unserer heutigen Produktpräsentationen zu beschäftigen (Sie werden nicht enttäuscht sein!), aber wir möchten bei dieser Gelegenheit auch einen Schritt zurücktreten, Ihnen unsere breitere Vision für KI vorstellen und die heutigen Ankündigungen einordnen.</p>
	<h3 id="inferenz-die-zukunft-der-ki-workloads">Inferenz: Die Zukunft der KI-Workloads</h3>
	<p>KI beinhaltet zwei Hauptprozesse: Training und Inferenz.</p>
	<p>Das Training eines generativen KI-Modells ist ein langwieriger (manchmal monatelanger) rechenintensiver Prozess, aus dem schließlich ein Modell hervorgeht. Training-Workloads lassen sich daher am besten in traditionellen, zentralisierten Cloud-Standorten ausführen. In letzter Zeit haben Unternehmen Schwierigkeiten, einen dauerhaften Zugang zu GUPs zu erhalten. Das führte zu einem Umstieg auf die Multi-Cloud. In diesem Zusammenhang haben wir darüber gesprochen, wie R2 einen unverzichtbaren Dienst bereitstellen kann, der <a href="https://www.cloudflare.com/learning/cloud/what-are-data-egress-fees" target="_blank">Gebühren für ausgehenden Traffic</a> für den Zugriff auf die Trainingsdaten von jeder Compute Cloud aus eliminiert. Aber heute soll es um etwas anderes gehen.</p>
	<p>Während das Training viele Ressourcen im Vorfeld erfordert, ist die weitaus häufigere KI-bezogene Rechenaufgabe die Inferenz. Wenn Sie ChatGPT kürzlich eine Frage gestellt, ein Bild generiert oder einen Text übersetzt haben, dann haben Sie eine Inferenzaufgabe durchgeführt. Da die Inferenz bei jedem einzelnen Aufruf erforderlich ist (und nicht nur einmal), gehen wir davon aus, dass die Inferenz der dominierende KI-Workload sein wird.</p>
	<p>Wenn sich das Training am besten für eine zentralisierte Cloud anbietet, was ist dann der beste Ort für die Inferenz?</p>
	<h3 id="das-netzwerk-%E2%80%93-%E2%80%9Egenau-richtig%E2%80%9C-f%C3%BCr-inferenz">Das Netzwerk – „genau richtig“ für Inferenz</h3>
	<p>Das bestimmende Merkmal der Inferenz ist, dass am anderen Ende meist Nutzende warten. Das heißt, es handelt sich um eine Aufgabe, bei der die Latenz eine Rolle spielt.</p>
	<p>Man könnte meinen, der beste Ort für eine latenzempfindliche Aufgabe sei auf dem Gerät. Und das mag in einigen Fällen zutreffen, aber es gibt dabei ein paar Probleme. Erstens ist die Hardware auf Geräten nicht annähernd so leistungsfähig. Die Lebensdauer der Batterie.</p>
	<p>Auf der anderen Seite gibt es zentralisiertes Cloud Computing. Im Gegensatz zu den Geräten hat die Hardware, die an zentralisierten Cloud-Standorten läuft, jede Menge Power. Das Problem ist natürlich, dass sie Hunderte von Millisekunden von den Nutzenden entfernt ist. Und manchmal befindet sie sich sogar jenseits der Landesgrenzen, was eine eigene Reihe von Herausforderungen mit sich bringt.</p>
	<p>Die Geräte sind also noch nicht leistungsfähig genug, und die zentrale Cloud ist zu weit entfernt. Das macht das Netzwerk zur habitablen Zone der Inferenz. Nicht zu weit entfernt, mit ausreichender Rechenleistung – also genau richtig.</p>
	<h3 id="die-erste-inferenz-cloud-die-auf-region-earth-l%C3%A4uft">Die erste Inferenz-Cloud, die auf Region Earth läuft</h3>
	<p>Eine Lektion, die wir beim Aufbau unserer Entwicklungsplattform gelernt haben, ist, dass die Ausführung von Anwendungen im Netzwerkmaßstab nicht nur zur Optimierung der Performance und Skalierung beiträgt (was natürlich ein netter Vorteil ist!), sondern, was noch wichtiger ist, die richtige Abstraktionsebene für Entwicklungsteams schafft, um schnelle Fortschritte zu erzielen.</p>
	<h3 id="workers-ai-f%C3%BCr-serverlose-inferenz">Workers AI für serverlose Inferenz</h3>
	<p>Mit unserer Ankündigung von <a href="https://blog.cloudflare.com/workers-ai">Workers AI</a> bringen wir die erste wirklich serverlose GPU-Cloud zu ihrer perfekten Partnerin – der Region Earth. Kein Fachwissen über maschinelles Lernen, keine Suche nach GPUs. Wählen Sie einfach eines der von uns bereitgestellten Modelle und los geht’s.</p>
	<p>Wir haben uns bei der Entwicklung von Workers AI viele Gedanken gemacht, um die Bereitstellung eines Modells so reibungslos wie möglich zu gestalten.</p>
	<p>Und wenn Sie im Jahr 2023 ein Modell bereitstellen, ist vermutlich eines davon ein <a href="https://www.cloudflare.com/learning/ai/what-is-large-language-model" target="_blank">LLM</a>.</p>
	<h3 id="vectorize-f%C3%BCr-das-speichern-von-vektoren">Vectorize für ... das Speichern von Vektoren!</h3>
	<p>Um einen Ende-zu-Ende-KI-gesteuerten Chatbot zu entwickeln, müssen Sie den Nutzenden eine Benutzeroberfläche präsentieren. Sie müssen den Informationskorpus, den Sie ihnen übermitteln wollen, parsen (z. B. Ihren Produktkatalog). Sie müssen das Modell verwenden, um es in Einbettungen umzuwandeln – und Sie müssen alles irgendwo speichern. Bis heute haben wir die Produkte für die ersten beiden Aufgaben angeboten, aber für die letzte Aufgabe – das Speichern von Einbettungen – benötigen Sie eine spezielle Lösung: eine Vektordatenbank.</p>
	<p>Genau wie bei der Ankündigung von Workers haben wir kurz darauf Workers KV vorgestellt – denn ohne Zugriff auf Status können Sie nur wenig mit Compute machen. Dasselbe gilt für KI: Um sinnvolle KI-Anwendungsfälle zu entwickeln, müssen Sie der KI Zugriff auf den Status geben. Hier kommt eine <a href="https://www.cloudflare.com/learning/ai/what-is-vector-database" target="_blank">Vektordatenbank</a> ins Spiel, und deshalb freuen wir uns, heute auch Vectorize, unsere eigene Vektordatenbank, vorstellen zu können.</p>
	<h3 id="ai-gateway-f%C3%BCr-caching-durchsatzbegrenzung-und-einblick-in-ihre-ki-implementierungen">AI Gateway für Caching, Durchsatzbegrenzung und Einblick in Ihre KI-Implementierungen</h3>
	<p>Wenn wir bei Cloudflare etwas verbessern wollen, besteht der erste Schritt immer aus einer Messung. Denn wie soll man etwas verbessern, wenn man es nicht messen kann? Als wir von Kunden über Probleme bei der Begrenzung der KI-Einsatzkosten hörten, überlegten wir, wie wir vorgehen würden – erst messen, dann verbessern.</p>
	<p>Mit unserem AI Gateway gelingt Ihnen beides!</p>
	<p>Echtzeit-Beobachtungsfunktionen ermöglichen ein proaktives Management, das die Überwachung, Fehlersuche und Feinjustierung von KI-Implementierungen erleichtert. Die Nutzung des Gateways zum Cachen, zur Durchsatzbegrenzung und zur Überwachung von KI-Implementierungen ist für die Optimierung der Performance und die effektive Verwaltung der Kosten unerlässlich. Durch das Cachen häufig genutzter KI-Antworten wird die Latenz reduziert und die Systemzuverlässigkeit erhöht, während die Durchsatzbegrenzung für eine effiziente Ressourcenzuweisung sorgt und die Herausforderungen der steigenden KI-Kosten bekämpft.</p>
	<h3 id="zusammenarbeit-mit-meta-um-llama-2-in-unser-globales-netzwerk-zu-bringen">Zusammenarbeit mit Meta, um Llama 2 in unser globales Netzwerk zu bringen</h3>
	<p>Wer auf ein LLM zugreifen wollte, konnte dies bis vor kurzem nur durch ein Aufrufen von proprietären Modellen. Das Training von LLMs ist eine erhebliche Investition – in Zeit, Rechenleistung und finanzielle Ressourcen – und daher für die meisten Entwickler und Entwicklerinnen nicht zugänglich. Mit der Veröffentlichung von Llama 2, einem quelloffenen LLM, hat Meta einen spannenden Wandel eingeleitet, der der Entwicklungs-Community ermöglicht, ihre eigenen LLMs zu betreiben und einzusetzen. Bis auf ein kleines Detail natürlich – Sie brauchen dazu immer noch Zugang zu einer GPU.</p>
	<p>Indem wir Llama 2 als Teil des Workers AI-Katalogs zur Verfügung stellen, freuen wir uns darauf, jedem Entwickler und jeder Entwicklerin den Zugang zu einem LLM zu ermöglichen – ohne dass eine Konfiguration erforderlich ist.</p>
	<p>Ein laufendes Modell ist natürlich nur eine Komponente einer KI-Anwendung.</p>
	<h3 id="nutzung-der-onnx-laufzeitumgebung-um-einen-nahtlosen-wechsel-zwischen-cloud-edge-und-ger%C3%A4t-zu-erm%C3%B6glichen">Nutzung der ONNX-Laufzeitumgebung, um einen nahtlosen Wechsel zwischen Cloud, Edge und Gerät zu ermöglichen</h3>
	<p>Auch wenn die Edge der optimale Ort für die Lösung vieler dieser Probleme sein mag, gehen wir davon aus, dass Anwendungen auch weiterhin an anderen Orten entlang des Spektrums von Gerät, Edge und zentralisierter Cloud bereitgestellt werden.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/09/image1-22.png" class="kg-image" alt="" loading="lazy" width="1801" height="752"></figure>
	<p>Denken Sie beispielsweise an selbstfahrende Autos: Wenn es bei Entscheidungen auf jede Millisekunde ankommt, müssen diese Entscheidungen auf dem Gerät getroffen werden. Wenn Sie dagegen Modelle mit Hunderten von Milliarden von Parametern ausführen wollen, ist die zentralisierte Cloud besser für Ihren Workload geeignet.</p>
	<p>Die Frage ist also: Wie können Sie reibungslos zwischen diesen Orten navigieren?</p>
	<p>Seit unserer ersten Version von Constellation (jetzt Workers AI) waren wir von einer Technologie besonders begeistert: der ONNX Runtime. Die ONNX Runtime schafft eine standardisierte Umgebung für die Ausführung von Modellen, wodurch es möglich ist, verschiedene Modelle an unterschiedlichen Orten auszuführen.</p>
	<p>Wir haben bereits darüber gesprochen, dass die Edge ein großartiger Ort für die Ausführung von Inferenzen selbst ist. Sie eignet sich aber auch hervorragend als Routing-Ebene, um Workloads reibungslos über alle drei Standorte zu leiten, je nach Anwendungsfall und Optimierungszielen – sei es Latenz, Präzision, Kosten, Compliance oder Datenschutz.</p>
	<h3 id="partnerschaft-mit-hugging-face-zur-bereitstellung-optimierter-modelle-auf-knopfdruck">Partnerschaft mit Hugging Face zur Bereitstellung optimierter Modelle auf Knopfdruck</h3>
	<p>Nichts kann Entwicklungsteams schneller voranbringen, als sie dort abzuholen, wo sie sind. Deshalb gehen wir eine <a href="https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable">Partnerschaft mit Hugging Face</a> ein, um serverlose Inferenz auf verfügbare Modelle zu bringen, genau dort, wo Entwickler und Entwicklerinnen sie erkunden.</p>
	<h3 id="partnerschaft-mit-databricks-zur-erstellung-von-ki-modellen">Partnerschaft mit Databricks zur Erstellung von KI-Modellen</h3>
	<p>Gemeinsam mit Databricks werden wir die Leistungsfähigkeit von MLflow für Data Scientists und die Entwicklungs-Community nutzbar machen. MLflow ist eine Open-Source-Plattform für die Verwaltung des gesamten Lebenszyklus des maschinellen Lernens; und diese Partnerschaft wird es Nutzenden erleichtern, ML-Modelle in großem Umfang bereitzustellen und zu verwalten. Dank dieser Partnerschaft können Entwicklungsteams, die auf Cloudflare Workers AI arbeiten, MLFlow-kompatible Modelle für eine einfache Bereitstellung im globalen Netzwerk von Cloudflare nutzen. Sie können MLflow verwenden, um ein Modell effizient zu bündeln, zu implementieren, bereitzustellen und direkt in der serverlosen Entwicklungsplattform von Cloudflare zu überwachen.</p>
	<p>KI, die Ihren CIOs, CFOs oder Rechtsabteilungen keine schlaflosen Nächte bereitet</p>
	<p>Die Entwicklung im Bereich der KI schreitet schnell voran, und es ist wichtig, den Entwicklern und Entwicklerinnen die nötigen Tools für ihre Arbeit an die Hand zu geben. Aber schnelle Fortschritte lassen sich nur schwer erzielen, wenn wichtige Überlegungen den Teams Sorgen bereiten: Was ist mit Compliance, Kosten, Datenschutz?</p>
	<h3 id="compliance-freundliche-ki">Compliance-freundliche KI</h3>
	<p>Auch wenn die meisten von uns die Tatsache lieber verdrängen würden, erlassen Regierungen immer mehr Vorschriften für KI und Datenresidenz. Wenn Regierungen verlangen, dass Daten lokal verarbeitet oder die Daten ihrer Bürger und Bürgerinnen im Land gespeichert werden, müssen Unternehmen auch darüber nachdenken, wo Inferenz Workloads ausgeführt werden. Was die Latenz angeht, so bietet die Netzwerk-Edge die Möglichkeit, so weit wie möglich zu gehen. Wenn es um die Compliance geht, liegt die Stärke eines Netzwerks, das sich über 300 Städte erstreckt, und eines Angebots wie unserer Datenlokalisierungs-Suite darin, dass es die nötige Granularität ermöglicht, um KI-Implementierungen lokal zu begrenzen.</p>
	<h3 id="budgetfreundliche-ki">Budgetfreundliche KI</h3>
	<p>In Gesprächen mit vielen Leuten aus unserem Freundes- und Kollegenkreis, die mit KI experimentieren, scheint stets ein Gefühl mitzuschwingen: KI ist teuer. Die Kosten können leicht aus dem Ruder laufen, bevor überhaupt etwas in Produktion geht oder ein Nutzen daraus gezogen wird. Mit unserer KI-Plattform wollen wir die Kosten erschwinglich machen. Aber – was vielleicht noch wichtiger ist – wir wollen Ihnen nur das berechnen, was Sie auch wirklich nutzen. Unabhängig davon, ob Sie Workers AI direkt oder unser AI Gateway verwenden, möchten wir Ihnen die nötige Transparenz und die Tools zur Verfügung stellen, damit Ihre Kosten für KI nicht aus dem Ruder laufen.</p>
	<h3 id="datenschutzfreundliche-ki">Datenschutzfreundliche KI</h3>
	<p>Wenn Sie KI in den Mittelpunkt Ihres Kundenerlebnisses und Ihrer Geschäftsabläufe stellen, möchten Sie sicher sein, dass alle Daten, die über sie laufen, in sicheren Händen sind. Wie schon immer bei Cloudflare steht auch bei uns der Datenschutz an erster Stelle. Wir können unseren Kunden und Kundinnen versichern, dass wir keine ihrer Daten, die über Cloudflare laufen, für Inferenzen zum Trainieren großer Sprachmodelle verwenden werden.</p>
	<h3 id="aber-im-ernst-%E2%80%93-wir-fangen-gerade-erst-an">Aber im Ernst – wir fangen gerade erst an</h3>
	<p>Wir fangen gerade erst mit KI an, und wir stehen vor einer wilden Fahrt! Während wir die Vorteile dieser Technologie weiter erschließen, können wir nicht anders, als ein Gefühl der Ehrfurcht und des Staunens über die unendlichen Möglichkeiten zu empfinden, die vor uns liegen. Von der Revolutionierung des Gesundheitswesens bis zur Veränderung unserer Arbeitsweise – KI steht kurz davor, das Spiel auf Weisen zu verändern, die wir nie für möglich gehalten hätten. Also schnallt euch an, Leute, denn die Zukunft der KI sieht heller aus denn je – und wir können es kaum erwarten, zu sehen, was als Nächstes kommt!</p>
	<p>Diese Zusammenfassung mag zwar von einer KI übersetzt worden sein, aber das Gefühl ist echt – dies ist erst der Anfang, und wir können es kaum erwarten, Ihre Arbeit zu sehen.</p>
</div>