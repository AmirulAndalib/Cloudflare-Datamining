<div class="mb2 gray5">9 min read</div>
<div class="post-content lh-copy gray1">
	<p>Vor zwei Wochen berichteten wir über <a href="https://blog.cloudflare.com/cloudflares-response-to-csam-online">Cloudflares Ansatz im Umgang mit kinderpornograpfischem Online-Material (child sexual abuse material, CSAM)</a>. Bereits wenige Monate nach dem Start von Cloudflare im Jahr 2010 begannen wir unsere Zusammenarbeit mit dem National Center for Missing and Exploited Children (NCMEC), einer in den USA ansässigen Organisation, die als Clearingstelle für die Entfernung dieser abscheulichen Inhalte fungiert. In den letzten neun Jahren hat unser Trust &amp; Safety-Team mit <a href="http://www.missingkids.com">NCMEC</a>, <a href="https://www.interpol.int/en/Crimes/Crimes-against-children">Interpol</a> und fast 60 anderen öffentlichen und privaten Agenturen auf der ganzen Welt gearbeitet, um unser Programm zu gestalten. Und wir sind stolz auf die Arbeit, die wir zur Entfernung von CSAM geleistet haben.</p>
	<p>Die anstößigsten Fälle sind für uns in gewisser Weise am leichtesten zu bearbeiten. Cloudflare ist nicht in der Lage, Inhalte zu entfernen, die von anderen gehostet werden. Aber wir ergreifen Maßnahmen, um die Dienste für eine Website zu beenden, wenn klar wird, dass die Website der gemeinsamen Nutzung von CSAM gewidmet ist. Oder wenn die Betreiber der Website und ihr Hoster keine geeigneten Maßnahmen zur Entfernung von CSAM-Inhalten zu ergreifen. Wenn wir die Unterstützung für Websites beenden, leeren wir unsere Caches – was innerhalb von Sekunden global wirksam wird – und sperren die Website, sodass sie nie wieder ein Netzwerk von Cloudflare nutzen kann.</p>
	<h3 id="schweren-f-lle-bew-ltigen">Schweren Fälle bewältigen</h3>
	<p>Ein schwerer Fall liegen vor, wenn ein Kunde von uns einen Dienst betreibt, der nutzergenerierte Inhalte erlaubt (wie z. B. ein Diskussionsforum) und ein Nutzer CSAM hochlädt; oder wenn er gehackt wird oder ein Mitarbeiter CSAM auf den Servern speichert. Wir haben viele solcher Fälle erlebt, in denen Dienste, die nichts Böses im Sinn hatten, überraschend CSAM-Inhalte auf ihren Sites fanden. Auch wenn Kunden in diesen Fällen weder vorsätzlich noch böswillig gehandelt haben, besteht weiterhin die Notwendigkeit, diese Inhalte schnell zu identifizieren und zu entfernen.</p>
	<p>Heute können wir mit Stolz verkünden, dass wir einen weiteren Schritt unternehmen, diese schweren Fälle zu bewältigen. Ab heute kann jeder Cloudflare-Kunde direkt in seinem Dashboard den Zugriff auf das CSAM-Scanning-Tool ermöglichen. Während das CSAM-Scanning-Tool von der Entwicklung in die Produktion geht, überprüft das Tool alle Websites und Webapplikationen, die das Scannen nach diesen illegalen Inhalten ermöglicht haben. Cloudflare sendet automatisch eine Benachrichtigung an Sie, wenn es CSAM-Material entdeckt, blockiert den Zugriff auf diesen Inhalt (mit dem Statuscode 451 „blocked for legal reasons“, „aus juristischen Gründen blockiert“) und unternimmt Schritte, um eine ordnungsgemäße Berichterstattung über diesen Inhalt in Übereinstimmung mit den rechtlichen Verpflichtungen zu unterstützen.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2020/01/image3-15.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>CSAM-Scanning wird über das Cloudflare-Dashboard für alle Kunden unabhängig von ihrem Abo kostenlos verfügbar sein. Sie finden dieses Tool unter der Registerkarte „Caching“ in Ihrem Dashboard. Indem wir dieses Tool allen unseren Kunden kostenlos zur Verfügung stellen, hoffen wir, einen noch größeren Beitrag gegen CSAM-Material im Internet zu leisten – und unsere Kunden vor dem juristischem Risiko und Reputationsschäden zu schützen, die CSAM für ein Unternehmen darstellen kann.</p>
	<p>Wir mussten einen langen Weg zurücklegen, bis wir Millionen von Nutzern diesen Dienst anbieten konnten. Um zu verstehen, was wir tun und warum es aus technischer und politischer Sicht eine Herausforderung war, braucht man etwas Hintergrundwissen über den Stand der Technik bei der Verfolgung von CSAM.</p>
	<h3 id="-hnliche-bilder-finden">Ähnliche Bilder finden</h3>
	<p>Etwa zur gleichen Zeit als Cloudflare 2009 erstmals konzipiert wurde, arbeitete ein Professor aus Dartmouth namens Hany Farid an einer Software, die Bilder mit einer von NCMEC geführten Liste von Hashes vergleichen konnte. Microsoft übernahm die Führung bei der Entwicklung des Werkzeugs PhotoDNA, das die Arbeit von Prof. Farid zur automatischen Identifizierung von CSAM nutzte.</p>
	<p>In seinen Anfängen nutzte Microsoft PhotoDNA für seine Dienste intern. Ende 2009 <a href="http://blogs.msdn.com/b/microsoftuseducation/archive/2009/12/17/microsoft-donates-photodna-technology-to-make-the-internet-safer-for-kids.aspx">spendete das Unternehmen die Technologie an NCMEC</a>, um bei der Verwaltung seiner Nutzung durch andere Organisationen zu helfen. Soziale Netzwerke gehörten zu den ersten Nutzern. Im Jahr 2011, <a href="http://www.huffingtonpost.com/2011/05/20/facebook-photodna-microsoft-child-pornography_n_864695.html">führte Facebook eine Implementierung</a> der Technologie als Teil ihres Umgangs mit Missbrauch ein. <a href="https://www.theguardian.com/technology/2013/jul/22/twitter-photodna-child-abuse">Twitter nahm sie 2014 auf.</a>.</p>
	<p>Der Prozess ist als Fuzzy Hash bekannt. Herkömmliche Hash-Algorithmen wie MD5, SHA1 und SHA256 nehmen eine Datei (z. B. ein Bild oder ein Dokument) beliebiger Länge und geben eine Zahl fester Länge aus, quasi der digitale Fingerabdruck der Datei. Wenn Sie zum Beispiel den MD5 dieses Bildes nehmen, dann lautet der daraus resultierende Fingerabdruck <strong>605c83bf1bba62e85f4f5fccc56bc128</strong>.</p>
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cloudflare.com/content/images/2020/01/image4-3.jpg" class="kg-image" alt="" loading="lazy">
		<figcaption>Das ursprüngliche Bild</figcaption>
	</figure>
	<p>Wenn wir jetzt ein einzelnes Pixel von rein Weiß zu Cremeweiß ändern, bleibt das Bild optisch identisch, aber der Fingerabdruck ändert sich vollständig in <strong>42ea4fb30a440d8787477c6c37b9daed</strong>. Wie Sie an den beiden Fingerabdrücken sehen können, führt eine kleine Veränderung des Bildes zu einer massiven und unvorhersehbaren Veränderung des traditionellen Hashs.</p>
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cloudflare.com/content/images/2020/01/image5-3.jpg" class="kg-image" alt="" loading="lazy">
		<figcaption>Das ursprüngliche Bild mit einem einzigen veränderten Pixel</figcaption>
	</figure>
	<p>Für einige Verwendungszwecke des Hashings ist diese Funktion ideal. Beispielsweise, wenn Sie definitiv feststellen möchten, ob das Dokument, das Sie sich ansehen, genau dasselbe ist wie das, das Sie zuvor gesehen haben. Wenn z. B. einem digitalen Vertrag eine zusätzliche Null hinzugefügt wird, sollte der Hash-Wert des Dokuments, der in seiner Signatur verwendet wird, nicht mehr gültig sein.</p>
	<h3 id="fuzzy-hashing">Fuzzy Hashing</h3>
	<p>Im Fall von CSAM ist dieses Merkmal des traditionellen Hashings jedoch eine Schwachstelle. Um nicht entdeckt zu werden, ändern die CSAM-Kriminellen die Bildgröße, fügen Rauschen hinzu oder verändern das Bild auf andere Weise so, dass es zwar gleich aussieht, aber zu einem radikal anderen Hash führt.</p>
	<p>Fuzzy Hashing funktioniert anders. Anstatt zu bestimmen, ob zwei Fotos identisch sind, versucht Fuzzy Hashing stattdessen, der Substanz eines Fotos auf den Grund zu gehen. Dies ermöglicht es der Software, Hashes für zwei Bilder zu berechnen und dann den „Abstand“ zwischen den beiden Bildern zu vergleichen. Zwar können sich die Fuzzy Hashes zwischen zwei Fotos, die verändert wurden, immer noch unterschieden, aber anders als beim traditionellen Hashing können Sie die beiden Hashes vergleichen und erkennen, wie sehr sich die Bilder ähneln.</p>
	<p>Auf den beiden oberen Fotos oben lautet der Fuzzy Hash des ersten Bildes</p><!--kg-card-begin: markdown-->
	<pre><code>00e308346a494a188e1042333147267a
653a16b94c33417c12b433095c318012
5612442030d1484ce82c613f4e224733
1dd84436734e4a5c6e25332e507a8218
6e3b89174e30372d
</code></pre>
	<!--kg-card-end: markdown-->
	<p>und der des zweiten Bildes</p><!--kg-card-begin: markdown-->
	<pre><code>00e308346a494a188e1042333147267a
653a16b94c33417c12b433095c318012
5612442030d1484ce82c613f4e224733
1dd84436734e4a5c6e25332e507a8218
6e3b89174e30372d
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Die Bilder unterscheiden sich nur sehr geringfügig, was Pixel angeht, und die beiden Fuzzy Hashes sind daher identisch.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2020/01/image6-3.jpg" class="kg-image" alt="" loading="lazy"></figure>
	<p>Das Grundbild nach Erhöhung der Sättigung, Wechsel zu Sepia, Hinzufügen eines Randes und Zufallsrauschen.</p>
	<p>Das Fuzzy-Hashing ist so konzipiert, dass es in der Lage ist, Bilder zu identifizieren, die sich im Wesentlichen ähnlich sind. Zum Beispiel haben wir das Hundebild modifiziert, indem wir zuerst die Farbe verstärkt, dann in Sepia geändert und dann einen Rand und schließlich Zufallsrauschen hinzugefügt haben. &nbsp;Der Fuzzy Hash des neuen Bildes lautet</p><!--kg-card-begin: markdown-->
	<pre><code>00d9082d6e454a19a20b4e3034493278
614219b14838447213ad3409672e7d13
6e0e4a2033de545ce731664646284337
1ecd4038794a485d7c21233f547a7d2e
663e7c1c40363335
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Das sieht ganz anders aus als der Hash des unveränderten Bildes oben, aber Fuzzy Hashes vergleicht man, indem man schaut, wie nahe sie beieinander liegen.</p>
	<p>Der größtmögliche Abstand zwischen zwei Bildern beträgt etwa 5m Einheiten. Diese beiden Fuzzy Hashes liegen nur 4,913 Einheiten auseinander (je kleiner die Zahl, desto ähnlicher die Bilder), was anzeigt, dass es sich im Wesentlichen um das gleiche Bild handelt.</p>
	<p>Vergleichen Sie das mit zwei Fotos, die nichts miteinander zu tun haben. Der Fuzzy Hash des nächsten Bildes lautet</p><!--kg-card-begin: markdown-->
	<pre><code>011a0d0323102d048148c92a4773b60d
0d343c02120615010d1a47017d108b14
d36fff4561aebb2f088a891208134202
3e21ff5b594bff5eff5bff6c2bc9ff77
1755ff511d14ff5b
</code></pre>
	<!--kg-card-end: markdown-->
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2020/01/image7-3.jpg" class="kg-image" alt="" loading="lazy"></figure>
	<p>Der Fuzzy Hash des folgenden Fotos lautet</p><!--kg-card-begin: markdown-->
	<pre><code>062715154080356b8a52505955997751
9d221f4624000209034f1227438a8c6a
894e8b9d675a513873394a2f3d000722
781407ff475a36f9275160ff6f231eff
465a17f1224006ff
</code></pre>
	<!--kg-card-end: markdown-->
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2020/01/image8-9.png" class="kg-image" alt="" loading="lazy"></figure>
	<p>Der Abstand zwischen den beiden Hashes wird mit 713,061 berechnet. Durch Experimentieren ist es möglich, einen Abstands-Threshold festzulegen, unterhalb derer Sie zwei Fotos als wahrscheinlich verwandt betrachten können.</p>
	<h3 id="fuzzy-hashing-ist-bewusst-eine-black-box">Fuzzy Hashing ist bewusst eine Black Box</h3>
	<p>Wie funktioniert Fuzzy Hashing? Zwar wurde viel zum Thema Fuzzy Hashing veröffentlicht, aber der Kern des Prozesses wird bewusst nebulös gehalten. Die New York Times veröffentlichte kürzlich <a href="https://www.nytimes.com/interactive/2019/11/09/us/internet-child-sex-abuse.html">einen Artikel</a>, der wahrscheinlich die größte öffentliche Diskussion über die Funktionsweise einer solchen Technologie war. Die Herausforderung bestand darin, dass kriminelle Produzenten und Vertreiber von CSAM, in der Lage sein könnten, ihre Bilder erfolgreich zu verändern, wenn sie wüssten, wie diese Tools genau funktionieren. Es steht außer Frage: Cloudflare wird das CSAM-Screening-Tool im Namen des Website-Betreibers von unseren sicheren Points of Presence aus betreiben. Wir werden die Software nicht direkt an die Nutzer verteilen. Wir werden weiterhin wachsam auf einen möglichen Missbrauchsversuch der Plattform achten und gegebenenfalls unverzüglich Maßnahmen ergreifen.</p>
	<h3 id="kompromiss-zwischen-falsch-negativen-und-falsch-positiven-ergebnissen">Kompromiss zwischen falsch-negativen und falsch-positiven Ergebnissen</h3>
	<p>Wir haben mit einer Reihe von Behörden daran gearbeitet, wie wir diese Funktionalität am besten für unsere Kunden bereitstellen können. Für ein Netzwerk mit einem so vielfältigen Kundenkreis wie dem von Cloudflare besteht eine der Herausforderungen in der Frage nach den angemessenen Thresholds für die Vergleichsdistanz zwischen den Fuzzy Hashes.</p>
	<p>Wenn der Threshold zu streng ist, bedeutet es, dass er näher an einem traditionellen Hash liegt und zwei Bilder praktisch identisch sein müssen, um eine Übereinstimmung auszulösen. In diesem Fall ist es wahrscheinlicher, dass Sie viele falsch-negative Ergebnisse haben (d.&amp;nbsp;h. CSAM-Inhalt, der nicht markiert wird). Wenn der Threshold zu niedrig ist, dann erhalten Sie möglicherweise viele falsch-positive Ergebnisse. Falsch-positive Ergebnisse mögen wie das kleinere Übel erscheinen, aber es gibt berechtigte Bedenken, dass eine Erhöhung der Wahrscheinlichkeit falsch-positiver Ergebnisse in großem Maßstab begrenzte Ressourcen verschwenden und außerdem <a href="https://www.nytimes.com/interactive/2019/09/28/us/child-sex-abuse.html">das vorhandene Ökosystem überwältigen</a> würde. Wir werden daran arbeiten, das CSAM-Scanning-Tool zu iterieren, um dem Website-Eigentümer eine genauere Kontrolle zu ermöglichen und gleichzeitig die anhaltende Wirksamkeit des Ökosystems zu unterstützen. Wir glauben, unseren Kunden heute eine gute erste Serie von Optionen anbieten zu können. Diese Optionen ermöglichen es uns, CSAM schneller zu kennzeichnen, ohne die Ressourcen des Ökosystems zu überfordern.</p>
	<h3 id="unterschiedliche-thresholds-f-r-unterschiedliche-kunden">Unterschiedliche Thresholds für unterschiedliche Kunden</h3>
	<p>Der gleiche Wunsch nach einem granularen Ansatz spiegelte sich auch in Gesprächen mit unseren Kunden wider. Als wir fragten, was für sie angemessen sei, variierte die Antwort radikal, je nach Art des Unternehmens, danach wie ausgefeilt der bestehende Umgang mit Missbrauchsfällen war und der wahrscheinlichen Expositionshöhe und Toleranz gegenüber dem Risiko der Veröffentlichung von CSAM auf der Website der Kunden.</p>
	<p>Zum Beispiel könnte sich ein soziales Netzwerk mit einem erfahrenen Team für Missbrauchsfälle einen niedrigen Threshold wünschen, aber nicht, dass das Material automatisch blockiert wird, weil das Netzwerk über die Ressourcen verfügt, das markierte Material, manuell zu überprüfen.</p>
	<p>Ein neues Startup, das jungen Eltern ein Forum bietet, möchte vielleicht, dass der Threshold ziemlich niedrig gesetzt wird und dass alle Treffer automatisch blockiert werden, weil sie noch kein erfahrendes Team für Missbrauchsfälle aufgebaut haben und CSAM-Material ein zu hohes Reputationsrisiko für ihre Marke darstellt – selbst wenn die niedrige Schwelle es manchmal zu einem Fehlalarm führt.</p>
	<p>Ein kommerzielles Finanzinstitut könnte den Threshold ziemlich streng setzen, weil nutzergenerierte Inhalte weniger wahrscheinlich sind, mit einer geringen Toleranz für falsch-positive Ergebnisse. Sie könnten dann aber alles, was entdeckt wird, automatisch blockieren, denn wenn ihr System auf irgendeine Weise kompromittiert wird, um bekanntes CSAM zu hosten, wollen sie es sofort stoppen.</p>
	<h3 id="unterschiedliche-anforderungen-f-r-verschiedene-rechtsordnungen">Unterschiedliche Anforderungen für verschiedene Rechtsordnungen</h3>
	<p>Auch die Standorte unserer Kunden und die Vorschriften, die für sie gelten, können zu einer Herausforderung werden. Ja nach Standort des Unternehmens eines Kunden und je nachdem, wo sich seine Nutzer befinden, kann der Kunde sich für eine, mehrere oder alle verfügbaren Hash-Listen entscheiden.</p>
	<p>Mit anderen Worten, es gibt keine Einheitslösung für alle. Im Idealfall glauben wir, dass weniger falsch-negative Ergebnisse entstehen (also, dass mehr CSAM-Inhalte aufgespürt werden), wenn man den einzelnen Website-Eigentümern die Möglichkeit gibt, die Parameter einzustellen, die für ihren jeweiligen Standort am sinnvollsten sind, anstatt zu versuchen, einen globalen Standard für jeden unserer Kunden festzulegen.</p>
	<h3 id="das-tool-im-laufe-der-zeit-verbessern">Das Tool im Laufe der Zeit verbessern</h3>
	<p>Wir hoffen, dass wir im Laufe der Zeit das CSAM-Screening für unsere Kunden verbessern können. Wir gehen davon aus, dass wir zusätzliche Listen mit Hashes von zahlreichen globalen Agenturen hinzufügen werden, die unsere Kunden abonnieren können. Wir möchten diese Flexibilität ermöglichen, ohne das Ökosystem übermäßig zu belasten, das zur Bekämpfung dieses schrecklichen Verbrechens eingerichtet wurde.</p>
	<p>Schließlich glauben wir, dass es eine Gelegenheit geben könnte, beim Aufbau der nächsten Generation von Fuzzy Hashing zu helfen. Beispielsweise kann die Software nur Bilder scannen, die sich im Speicher einer Maschine befinden, nicht aber solche, die per Streaming übertragen werden. Mit Hany Farid, dem ehemaligen Professor aus Dartmouth, der jetzt in Berkeley, Kalifornien, lehrt, überlegen wir, wie wir ein flexibleres Fuzzy-Hashing-System aufbauen können, um Bilder zu kennzeichnen, bevor sie überhaupt gepostet werden.</p>
	<h3 id="bedenken-und-verantwortung">Bedenken und Verantwortung</h3>
	<p>Eine Frage, die uns durch den Kopf ging, als wir mit der Bereitstellung von CSAM-Scanning begannen, war: Sind wir überhaupt der richtige Ort dafür? Wir teilen die allgemeine Besorgnis über die Verbreitung von Darstellungen von schrecklichen Verbrechen an Kindern und sind der Meinung, dass so etwas keinen Platz im Internet haben sollte. Doch Cloudflare ist ein Anbieter von Netzinfrastrukturen, keine Inhaltsplattform.</p>
	<p>Aber wir waren schließlich der Meinung, dass wir in diesem Raum eine angemessene Rolle spielen sollten. Im Wesentlichen liefert Cloudflare unseren mehr als 2 Millionen Kunden Tools, die bisher nur den Internet-Giganten vorbehalten waren. Die Sicherheits-, Performance- und Zuverlässigkeitsdienste, die wir – oft kostenlos – anbieten, wären ohne uns extrem teuer oder nur für Internet-Giganten wie Facebook und Google zugänglich gewesen.</p>
	<p>Heute arbeiten Startups am nächsten Internet-Giganten und konkurrieren mit Facebook und Google. Und das weil sie Cloudflare nutzen können, um sicher, schnell und zuverlässig online zu sein. Da jedoch die regulatorischen Hürden bei der Behandlung unglaublich schwieriger Themen wie CSAM weiter zunehmen, fehlt vielen von ihnen der Zugang zu ausgefeilten Instrumenten, um proaktiv nach CSAM zu suchen. Sie müssen erst groß werden, bevor sie in den Club kommen, der ihnen diese Instrumente ermöglicht. Und die Clubmitgliedschaft wird zunehmend eine Voraussetzung, um groß zu werden.</p>
	<p>Wenn wir mehr Wettbewerb für die Internet-Giganten bieten wollen, müssen wir diese Instrumente breiter und auch für kleinere Organisationen verfügbar machen. Aus dieser Perspektive halten wir es für absolut sinnvoll, dass wir zur Demokratisierung dieses mächtigen Instruments im Kampf gegen CSAM beitragen.</p>
	<p>Wir möchten dazu beitragen, dass unsere Kunden gute Moderationsteams aufbauen können, die zu ihren eigenen Communities passen. Und dass sie auf verantwortungsvolle Weise skalieren können, um mit den Internet-Giganten von heute konkurrieren zu können. Das steht in direktem Zusammenhang mit unserer Mission, ein besseren Internet aufzubauen, und deshalb werden wir diesen Dienst allen unseren Kunden kostenlos zur Verfügung stellen.</p>
</div>