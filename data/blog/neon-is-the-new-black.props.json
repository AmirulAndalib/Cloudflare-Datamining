{
	"locale": "en-us",
	"post": {
		"access": true,
		"authors": [
			{
				"id": "5d1644b141acde0011a94f4e",
				"name": "Vlad Krasnov",
				"slug": "vlad-krasnov",
				"profile_image": "http://blog.cloudflare.com/content/images/2017/03/165e069.jpg",
				"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-131.png",
				"bio": null,
				"website": null,
				"location": null,
				"facebook": null,
				"twitter": null,
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/vlad-krasnov/"
			}
		],
		"canonical_url": null,
		"codeinjection_foot": null,
		"codeinjection_head": null,
		"comment_id": "5ac41a26943509002252254b",
		"comments": false,
		"created_at": "2018-04-04T01:19:50.000+01:00",
		"custom_excerpt": "Cloudflare's jpegtran implementation was optimized for Intel CPUs. Now that we intend to integrate ARMv8 processors, new optimizations for those are required.",
		"custom_template": null,
		"email_subject": null,
		"excerpt": "Cloudflare's jpegtran implementation was optimized for Intel CPUs. Now that we intend to integrate ARMv8 processors, new optimizations for those are required.",
		"feature_image": "http://blog.cloudflare.com/content/images/2018/04/2535574361_30730d9a7b_o.jpg",
		"feature_image_alt": null,
		"feature_image_caption": null,
		"featured": false,
		"frontmatter": null,
		"html": "<!--kg-card-begin: markdown--><p>As engineers at Cloudflare quickly adapt our software stack to run on ARM, a few parts of our software stack have not been performing as well on ARM processors as they currently do on our XeonÂ® Silver 4116 CPUs. For the most part this is a matter of Intel specific optimizations some of which utilize SIMD or other special instructions.</p>\n<p>One such example is the venerable jpegtran, one of the workhorses behind our Polish image optimization service.</p>\n<p>A while ago I <a href=\"http://blog.cloudflare.com/doubling-the-speed-of-jpegtran/\">optimized</a> our version of jpegtran for Intel processors. So when I ran a comparison on my <a href=\"http://blog.cloudflare.com/content/images/2015/10/print_poster_0025.jpg\">test image</a>, I was expecting that the Xeon would outperform ARM:</p>\n<pre><code>vlad@xeon:~$ time  ./jpegtran -outfile /dev/null -progressive -optimise -copy none test.jpg\n\nreal    0m2.305s\nuser    0m2.059s\nsys     0m0.252s\n</code></pre>\n<pre><code>vlad@arm:~$ time ./jpegtran -outfile /dev/null -progressive -optimise -copy none test.jpg\n\nreal    0m8.654s\nuser    0m8.433s\nsys     0m0.225s\n</code></pre>\n<p>Ideally we want to have the ARM performing at or above 50% of the Xeon performance per core. This would make sure we have no performance regressions, and net performance gain, since the ARM CPUs have double the core count as our current 2 socket setup.</p>\n<p>In this case, however, I was disappointed to discover an almost 4X slowdown.</p>\n<p>Not one to despair, I figured out that applying the same optimizations I did for Intel would be trivial. Surely the NEON instructions map neatly to the SSE instructions I used before?</p>\n<p><img src=\"https://farm4.staticflickr.com/3063/2535574361_30730d9a7b_o_d.jpg\" alt=\"\" loading=\"lazy\"><br>\n<small><a href=\"https://creativecommons.org/licenses/by-sa/2.0/\">CC BY-SA 2.0</a> <a href=\"https://www.flickr.com/photos/vizzzual-dot-com/2535574361/in/photolist-4S4tVk-8yutL-ge8Q-5fXXRQ-wJcRY-yrvgf-9vXGvq-hGx4N-4NZ4L-5cjA7-iJrnwJ-7VXhaz-866BCb-auGuG-68BjeT-92L4-9rXsu-3Pfaz-5GZs6n-oAKSY-LhdKa-7BLZ96-VRGZ2H-ofm5ZJ-8xC1bv-5DePNQ-ZouKg-afu4r-49ThBC-7VyeQT-qfr6P3-4zpUM8-hgPbs-naTubk-S7khvM-6hTftH-ByLAg-9sNftz-8G4os2-zsq8-oBEf6L-5y1nxR-7aTZfD-9fYMH-wJcnp-8yhgk-wJcDS-qsPXVn-kbW5A6-bPx3gX\">image</a> by <a href=\"https://www.flickr.com/photos/vizzzual-dot-com/\">viZZZual.com</a></small></p>\n<h3 id=\"whatisneon\">What is NEON</h3>\n<p>NEON is the ARMv8 version of SIMD, Single Instruction Multiple Data instruction set, where a single operation performs (generally) the same operation on several operands.</p>\n<p>NEON operates on 32 dedicated 128-bit registers, similarly to Intel SSE. It can perform operations on 32-bit and 64-bit floating point numbers, or 8-bit, 16-bit, 32-bit and 64-bit signed or unsigned integers.</p>\n<p>As with <a href=\"https://www.cloudflare.com/learning/access-management/security-service-edge-sse/\">SSE</a> you can program either in the assembly language, or in C using intrinsics. The intrinsics are usually easier to use, and depending on the application and the compiler can provide better performance, however intrinsics based code tends to be quite verbose.</p>\n<p>If you opt to use the NEON intrinsics you have to include  <code>&lt;arm_neon.h&gt;</code>. While SSE intrinsic use __m128i for all SIMD integer operations, the intrinsics for NEON have distinct type for each integer and float width. For example operations on signed 16-bit integers use the int16x8_t type, which we are going to use. Similarly there is a uint16x8_t type for unsigned integer, as well as int8x16_t, int32x4_t and int64x2_t and their uint derivatives, that are self explanatory.</p>\n<h3 id=\"gettingstarted\">Getting started</h3>\n<p>Running perf tells me that the same two culprits are responsible for most of the CPU time spent:</p>\n<pre><code>perf record ./jpegtran -outfile /dev/null -progressive -optimise -copy none test.jpeg\nperf report\n  71.24%  lt-jpegtran  libjpeg.so.9.1.0   [.] encode_mcu_AC_refine\n  15.24%  lt-jpegtran  libjpeg.so.9.1.0   [.] encode_mcu_AC_first\n</code></pre>\n<p>Aha, <code>encode_mcu_AC_refine</code> and <code>encode_mcu_AC_first</code>, my old nemeses!</p>\n<h3 id=\"thestraightforwardapproach\">The straightforward approach</h3>\n<br>\n<h4 id=\"encode_mcu_ac_refine\">encode_mcu_AC_refine</h4>\n<p>Let's recoup the optimizations we applied to <code>encode_mcu_AC_refine</code> previously. The function has two loops, with the heavier loop performing the following operation:</p>\n<pre><code class=\"language-c\">for (k = cinfo-&gt;Ss; k &lt;= Se; k++) {\n  temp = (*block)[natural_order[k]];\n  if (temp &lt; 0)\n    temp = -temp;      /* temp is abs value of input */\n  temp &gt;&gt;= Al;         /* apply the point transform */\n  absvalues[k] = temp; /* save abs value for main pass */\n  if (temp == 1)\n    EOB = k;           /* EOB = index of last newly-nonzero coef */\n}\n</code></pre>\n<p>And the SSE solution to this problem was:</p>\n<pre><code class=\"language-c\">__m128i x1 = _mm_setzero_si128(); // Load 8 16-bit values sequentially\nx1 = _mm_insert_epi16(x1, (*block)[natural_order[k+0]], 0);\nx1 = _mm_insert_epi16(x1, (*block)[natural_order[k+1]], 1);\nx1 = _mm_insert_epi16(x1, (*block)[natural_order[k+2]], 2);\nx1 = _mm_insert_epi16(x1, (*block)[natural_order[k+3]], 3);\nx1 = _mm_insert_epi16(x1, (*block)[natural_order[k+4]], 4);\nx1 = _mm_insert_epi16(x1, (*block)[natural_order[k+5]], 5);\nx1 = _mm_insert_epi16(x1, (*block)[natural_order[k+6]], 6);\nx1 = _mm_insert_epi16(x1, (*block)[natural_order[k+7]], 7);\n\nx1 = _mm_abs_epi16(x1);       // Get absolute value of 16-bit integers\nx1 = _mm_srli_epi16(x1, Al);  // &gt;&gt; 16-bit integers by Al bits\n\n_mm_storeu_si128((__m128i*)&amp;absvalues[k], x1);   // Store\n\nx1 = _mm_cmpeq_epi16(x1, _mm_set1_epi16(1));     // Compare to 1\nunsigned int idx = _mm_movemask_epi8(x1);        // Extract byte mask\nEOB = idx? k + 16 - __builtin_clz(idx)/2 : EOB;  // Compute index\n</code></pre>\n<p>For the most part the transition to NEON is indeed straightforward.</p>\n<p>To initialize a register to all zeros, we can use the <code>vdupq_n_s16</code> intrinsic, that duplicates a given value across all lanes of a register. The insertions are performed with the <code>vsetq_lane_s16</code> intrinsic. Use <code>vabsq_s16</code> to get the absolute values.</p>\n<p>The shift right instruction made me pause for a while. I simply couldn't find an instruction that can shift right by a non constant integer value. It doesn't exist. However the solution is very simple, you shift left by a negative amount! The intrinsic for that is <code>vshlq_s16</code>.</p>\n<blockquote>\n<p>The absence of a right shift instruction is no coincidence. Unlike the x86 instruction set, that can theoretically support arbitrarily long instructions, and thus don't have to think twice before adding a new instruction, no matter how specialized or redundant it is, ARMv8 instruction set can only support 32-bit long instructions, and have a very limited opcode space. For this reason the instruction set is much more concise, and many instructions are in fact aliases to other instruction. Even the most basic MOV instruction is an alias for ORR (binary or). That means that programming for ARM and NEON sometimes requires greater creativity.</p>\n</blockquote>\n<p>The final step of the loop, is comparing each element to 1, then getting the mask. Comparing for equality is performed with <code>vceqq_s16</code>. But again there is no operation to extract the mask. That is a problem. However, instead of getting a bitmask, it is possible to extract a whole byte from every lane into a 64-bit value, by first applying <code>vuzp1q_u8</code> to the comparison result. <code>vuzp1q_u8</code> interleaves the even indexed bytes of two vectors (whereas <code>vuzp2q_u8</code> interleaves the odd indexes). So the solution would look something like that:</p>\n<pre><code class=\"language-c\">int16x8_t zero = vdupq_n_s16(0);\nint16x8_t al_neon = vdupq_n_s16(-Al);\nint16x8_t x0 = zero;\nint16x8_t x1 = zero;\n\n// Load 8 16-bit values sequentially\nx1 = vsetq_lane_s16((*block)[natural_order[k+0]], x1, 0);\n// Interleave the loads to compensate for latency\nx0 = vsetq_lane_s16((*block)[natural_order[k+1]], x0, 1);\nx1 = vsetq_lane_s16((*block)[natural_order[k+2]], x1, 2);\nx0 = vsetq_lane_s16((*block)[natural_order[k+3]], x0, 3);\nx1 = vsetq_lane_s16((*block)[natural_order[k+4]], x1, 4);\nx0 = vsetq_lane_s16((*block)[natural_order[k+5]], x0, 5);\nx1 = vsetq_lane_s16((*block)[natural_order[k+6]], x1, 6);\nx0 = vsetq_lane_s16((*block)[natural_order[k+7]], x0, 7);\nint16x8_t x = vorrq_s16(x1, x0);\n\nx = vabsq_s16(x);            // Get absolute value of 16-bit integers\nx = vshlq_s16(x, al_neon);   // &gt;&gt; 16-bit integers by Al bits\n\nvst1q_s16(&amp;absvalues[k], x); // Store\nuint8x16_t is_one = vreinterpretq_u8_u16(vceqq_s16(x, one));  // Compare to 1\nis_one = vuzp1q_u8(is_one, is_one);  // Compact the compare result into 64 bits\n\nuint64_t idx = vgetq_lane_u64(vreinterpretq_u64_u8(is_one), 0); // Extract\nEOB = idx ? k + 8 - __builtin_clzl(idx)/8 : EOB;                // Get the index\n</code></pre>\n<p>Note the intrinsics for explicit type casts. They don't actually emit any instructions, since regardless of the type the operands always occupy the same registers.</p>\n<p>On to the second loop:</p>\n<pre><code class=\"language-c\">if ((temp = absvalues[k]) == 0) {\n  r++;\n  continue;\n}\n</code></pre>\n<p>The SSE solution was:</p>\n<pre><code class=\"language-c\">__m128i t = _mm_loadu_si128((__m128i*)&amp;absvalues[k]);\nt = _mm_cmpeq_epi16(t, _mm_setzero_si128()); // Compare to 0\nint idx = _mm_movemask_epi8(t);              // Extract byte mask\nif (idx == 0xffff) {                         // Skip all zeros\n  r += 8;\n  k += 8;\n  continue;\n} else {                                     // Skip up to the first nonzero\n  int skip = __builtin_ctz(~idx)/2;\n  r += skip;\n  k += skip;\n  if (k&gt;Se) break;      // Stop if gone too far\n}\ntemp = absvalues[k];    // Load the next nonzero value\n</code></pre>\n<p>But we already know that there is no way to extract the byte mask. Instead of using NEON I chose to simply skip four zero values at a time, using 64-bit integers, like so:</p>\n<pre><code class=\"language-c\">uint64_t tt, *t = (uint64_t*)&amp;absvalues[k];\nif ( (tt = *t) == 0) while ( (tt = *++t) == 0); // Skip while all zeroes\nint skip = __builtin_ctzl(tt)/16 + ((int64_t)t - \n           (int64_t)&amp;absvalues[k])/2;           // Get index of next nonzero\nk += skip;\nr += skip;\ntemp = absvalues[k];\n</code></pre>\n<p>How fast are we now?</p>\n<pre><code>vlad@arm:~$ time ./jpegtran -outfile /dev/null -progressive -optimise -copy none test.jpg\n\nreal    0m4.008s\nuser    0m3.770s\nsys     0m0.241s\n</code></pre>\n<p>Wow, that is incredible. Over 2X speedup!</p>\n<h4 id=\"encode_mcu_ac_first\">encode_mcu_AC_first</h4>\n<p>The other function is quite similar, but the logic slightly differs on the first pass:</p>\n<pre><code class=\"language-c\">temp = (*block)[natural_order[k]];\nif (temp &lt; 0) {\n  temp = -temp;             // Temp is abs value of input\n  temp &gt;&gt;= Al;              // Apply the point transform\n  temp2 = ~temp;\n} else {\n  temp &gt;&gt;= Al;              // Apply the point transform\n  temp2 = temp;\n}\nt1[k] = temp;\nt2[k] = temp2;\n</code></pre>\n<p>Here it is required to assign the absolute value of temp to <code>t1[k]</code>, and its inverse to <code>t2[k]</code> if temp is negative, otherwise <code>t2[k]</code> assigned the same value as <code>t1[k]</code>.</p>\n<p>To get the inverse of a value, we use the <code>vmvnq_s16</code> intrinsic, to check if the values are negative we need to compare with zero using the <code>vcgezq_s16</code> and finally selecting based on the mask using <code>vbslq_s16</code>.</p>\n<pre><code class=\"language-c\">int16x8_t zero = vdupq_n_s16(0);\nint16x8_t al_neon = vdupq_n_s16(-Al);\n\nint16x8_t x0 = zero;\nint16x8_t x1 = zero;\n\n// Load 8 16-bit values sequentially\nx1 = vsetq_lane_s16((*block)[natural_order[k+0]], x1, 0);\n// Interleave the loads to compensate for latency\nx0 = vsetq_lane_s16((*block)[natural_order[k+1]], x0, 1);\nx1 = vsetq_lane_s16((*block)[natural_order[k+2]], x1, 2);\nx0 = vsetq_lane_s16((*block)[natural_order[k+3]], x0, 3);\nx1 = vsetq_lane_s16((*block)[natural_order[k+4]], x1, 4);\nx0 = vsetq_lane_s16((*block)[natural_order[k+5]], x0, 5);\nx1 = vsetq_lane_s16((*block)[natural_order[k+6]], x1, 6);\nx0 = vsetq_lane_s16((*block)[natural_order[k+7]], x0, 7);\nint16x8_t x = vorrq_s16(x1, x0);\n\nuint16x8_t is_positive = vcgezq_s16(x); // Get positive mask\n\nx = vabsq_s16(x);                 // Get absolute value of 16-bit integers\nx = vshlq_s16(x, al_neon);        // &gt;&gt; 16-bit integers by Al bits\nint16x8_t n = vmvnq_s16(x);       // Binary inverse\nn = vbslq_s16(is_positive, x, n); // Select based on positive mask\n\nvst1q_s16(&amp;t1[k], x); // Store\nvst1q_s16(&amp;t2[k], n);\n</code></pre>\n<p>And the moment of truth:</p>\n<pre><code>vlad@arm:~$ time ./jpegtran -outfile /dev/null -progressive -optimise -copy none test.jpg\n\nreal    0m3.480s\nuser    0m3.243s\nsys     0m0.241s\n</code></pre>\n<p>Overall 2.5X speedup from the original C implementation, but still 1.5X slower than Xeon.</p>\n<h3 id=\"batchbenchmark\">Batch benchmark</h3>\n<p>While the improvement for the single image was impressive, it is not necessarily representative of all jpeg files. To understand the impact on overall performance I ran jpegtran over a set of 34,159 actual images from one of our caches. The total size of those images was 3,325,253KB. The total size after jpegtran was 3,067,753KB, or 8% improvement on average.</p>\n<p>Using one thread, the Intel Xeon managed to process all those images in 14 minutes and 43 seconds. The original jpegtran on our ARM server took 29 minutes and 34 seconds. The improved jpegtran took only 13 minutes and 52 seconds, slightly outperforming even the Xeon processor, despite losing on the test image.</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2018/04/jpegtran.png\" alt=\"jpegtran\" loading=\"lazy\"></p>\n<h3 id=\"goingdeeper\">Going deeper</h3>\n<p>3.48 seconds, down from 8.654 represents a respectful 2.5X speedup.</p>\n<p>It definitely meets the goal of being at least 50% as fast as Xeon, and it is faster in the batch benchmark, but it still feels like it is slower than it could be.</p>\n<p>While going over the ARMv8 NEON instruction set, I found several unique instructions, that have no equivalent in SSE.</p>\n<p>The first such instruction is <code>TBL</code>. It works as a lookup table, that can lookup 8 or 16 bytes from one to four consecutive registers. In the single register variant it is similar to the <code>pshufb</code> SSE instruction. In the four register variant, however, it can simultaneously lookup 16 bytes in a 64 byte table! What sorcery is that?</p>\n<p>The intrinsic to use the 4 register variant is <code>vqtbl4q_u8</code>. Interestingly there is an instruction that can lookup 64 bytes in AVX-512, but we don't want to <a href=\"http://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/\">use that</a>.</p>\n<p>The next interesting thing I found, are instructions that can load or store and de/interleave data at the same time. They can load or store up to four registers simultaneously, while de/interleaving two, three or even four elements, of any supported width. The specifics are well presented in <a href=\"https://community.arm.com/processors/b/blog/posts/coding-for-neon---part-1-load-and-stores\">here</a>. The load intrinsics used are of the form: <code>vldNq_uW</code>, where N can be 1,2,3,4 to indicate the interleave factor and W can be 8, 16, 32 or 64. Similarly <code>vldNq_sW</code> is used for signed types.</p>\n<p>Finally very interesting instructions are the shift left/right and insert <code>SLI</code> and <code>SRI</code>. What they do is they shift the elements left or right, like a regular shift would, however instead of shifting in zero bits, the zeros are replaced with the original bits of the destination register! An intrinsic for that would look like <code>vsliq_n_u16</code> or <code>vsriq_n_u32</code>.</p>\n<h4 id=\"applyingthenewinstructions\">Applying the new instructions</h4>\n<p>It might not be visible at first how those new instruction can help. Since I didn't have much time to dig into libjpeg or the jpeg spec, I had to resolve to heuristics.</p>\n<p>From a quick look it became apparent that <code>*block</code> is defined as an array of 64 16-bit values. <code>natural_order</code> is an array of 32-bit integers that varies in length depending on the real block size, but is always padded with 16 entries. Also, despite the fact that it uses integers, the values are indexes in the range [0..63].</p>\n<p>Another interesting observation is that blocks of size 64 are the most common by far for both <code>encode_mcu_AC_refine</code> and <code>encode_mcu_AC_first</code>. And it always makes sense to optimize for the most common case.</p>\n<p>So essentially what we have here, is a 64 entry lookup table <code>*block</code> that uses <code>natural_order</code> as indices. Hmm, 64 entry lookup table, where did I see that before? Of course, the <code>TBL</code> instruction. Although <code>TBL</code> looks up bytes, and we need to lookup shorts, it is easy to do, since NEON lets us load and deinterleave the short into bytes in a single instruction using <code>LD2</code>, then we can use two lookups for each byte individually, and finally interleave again with <code>ZIP1</code> and <code>ZIP2</code>. Similarly despite the fact that the indices are integers, and we only need the least significant byte of each, we can use <code>LD4</code> to deinterleave them into bytes (the kosher way of course would be to rewrite the library to use bytes, but I wanted to avoid big changes).</p>\n<p>After the data loading step is done, the point transforms for both functions remain the same, but in the end, to get a single bitmask for all 64 values we can use <code>SLI</code> and <code>SRI</code> to intelligently align the bits such that only one bit of each comparison mask remains, using <code>TBL</code> again to combine them.</p>\n<p>For whatever reason, the compiler in that case produces somewhat suboptimal code, so I had to revert to assembly language for this specific optimization.</p>\n<p>The code for <code>encode_mcu_AC_refine</code>:</p>\n<pre><code class=\"language-asm\">    # Load and deintreleave the block\n    ld2 {v0.16b - v1.16b}, [x0], 32\n    ld2 {v16.16b - v17.16b}, [x0], 32\n    ld2 {v18.16b - v19.16b}, [x0], 32\n    ld2 {v20.16b - v21.16b}, [x0]\n\n    mov v4.16b, v1.16b\n    mov v5.16b, v17.16b\n    mov v6.16b, v19.16b\n    mov v7.16b, v21.16b\n    mov v1.16b, v16.16b\n    mov v2.16b, v18.16b\n    mov v3.16b, v20.16b\n    # Load the order \n    ld4 {v16.16b - v19.16b}, [x1], 64\n    ld4 {v17.16b - v20.16b}, [x1], 64\n    ld4 {v18.16b - v21.16b}, [x1], 64\n    ld4 {v19.16b - v22.16b}, [x1]\n    # Table lookup, LSB and MSB independently\n    tbl v20.16b, {v0.16b - v3.16b}, v16.16b\n    tbl v16.16b, {v4.16b - v7.16b}, v16.16b\n    tbl v21.16b, {v0.16b - v3.16b}, v17.16b\n    tbl v17.16b, {v4.16b - v7.16b}, v17.16b\n    tbl v22.16b, {v0.16b - v3.16b}, v18.16b\n    tbl v18.16b, {v4.16b - v7.16b}, v18.16b\n    tbl v23.16b, {v0.16b - v3.16b}, v19.16b\n    tbl v19.16b, {v4.16b - v7.16b}, v19.16b\n    # Interleave MSB and LSB back\n    zip1 v0.16b, v20.16b, v16.16b\n    zip2 v1.16b, v20.16b, v16.16b\n    zip1 v2.16b, v21.16b, v17.16b\n    zip2 v3.16b, v21.16b, v17.16b\n    zip1 v4.16b, v22.16b, v18.16b\n    zip2 v5.16b, v22.16b, v18.16b\n    zip1 v6.16b, v23.16b, v19.16b\n    zip2 v7.16b, v23.16b, v19.16b\n    # -Al\n    neg w3, w3\n    dup v16.8h, w3\n    # Absolute then shift by Al\n    abs v0.8h, v0.8h\n    sshl v0.8h, v0.8h, v16.8h\n    abs v1.8h, v1.8h\n    sshl v1.8h, v1.8h, v16.8h\n    abs v2.8h, v2.8h\n    sshl v2.8h, v2.8h, v16.8h\n    abs v3.8h, v3.8h\n    sshl v3.8h, v3.8h, v16.8h\n    abs v4.8h, v4.8h\n    sshl v4.8h, v4.8h, v16.8h\n    abs v5.8h, v5.8h\n    sshl v5.8h, v5.8h, v16.8h\n    abs v6.8h, v6.8h\n    sshl v6.8h, v6.8h, v16.8h\n    abs v7.8h, v7.8h\n    sshl v7.8h, v7.8h, v16.8h\n    # Store\n    st1 {v0.16b - v3.16b}, [x2], 64\n    st1 {v4.16b - v7.16b}, [x2]\n    # Constant 1\n    movi v16.8h, 0x1\n    # Compare with 0 for zero mask\n    cmeq v17.8h, v0.8h, #0\n    cmeq v18.8h, v1.8h, #0\n    cmeq v19.8h, v2.8h, #0\n    cmeq v20.8h, v3.8h, #0\n    cmeq v21.8h, v4.8h, #0\n    cmeq v22.8h, v5.8h, #0\n    cmeq v23.8h, v6.8h, #0\n    cmeq v24.8h, v7.8h, #0\n    # Compare with 1 for EOB mask\n    cmeq v0.8h, v0.8h, v16.8h\n    cmeq v1.8h, v1.8h, v16.8h\n    cmeq v2.8h, v2.8h, v16.8h\n    cmeq v3.8h, v3.8h, v16.8h\n    cmeq v4.8h, v4.8h, v16.8h\n    cmeq v5.8h, v5.8h, v16.8h\n    cmeq v6.8h, v6.8h, v16.8h\n    cmeq v7.8h, v7.8h, v16.8h\n    # For both masks -&gt; keep only one byte for each comparison\n    uzp1 v0.16b, v0.16b, v1.16b\n    uzp1 v1.16b, v2.16b, v3.16b\n    uzp1 v2.16b, v4.16b, v5.16b\n    uzp1 v3.16b, v6.16b, v7.16b\n\n    uzp1 v17.16b, v17.16b, v18.16b\n    uzp1 v18.16b, v19.16b, v20.16b\n    uzp1 v19.16b, v21.16b, v22.16b\n    uzp1 v20.16b, v23.16b, v24.16b\n    # Shift left and insert (int16) to get a single bit from even to odd bytes\n    sli v0.8h, v0.8h, 15\n    sli v1.8h, v1.8h, 15\n    sli v2.8h, v2.8h, 15\n    sli v3.8h, v3.8h, 15\n\n    sli v17.8h, v17.8h, 15\n    sli v18.8h, v18.8h, 15\n    sli v19.8h, v19.8h, 15\n    sli v20.8h, v20.8h, 15\n    # Shift right and insert (int32) to get two bits from off to even indices\n    sri v0.4s, v0.4s, 18\n    sri v1.4s, v1.4s, 18\n    sri v2.4s, v2.4s, 18\n    sri v3.4s, v3.4s, 18\n\n    sri v17.4s, v17.4s, 18\n    sri v18.4s, v18.4s, 18\n    sri v19.4s, v19.4s, 18\n    sri v20.4s, v20.4s, 18\n    # Regular shift right to align the 4 bits at the bottom of each int64\n    ushr v0.2d, v0.2d, 12\n    ushr v1.2d, v1.2d, 12\n    ushr v2.2d, v2.2d, 12\n    ushr v3.2d, v3.2d, 12\n\n    ushr v17.2d, v17.2d, 12\n    ushr v18.2d, v18.2d, 12\n    ushr v19.2d, v19.2d, 12\n    ushr v20.2d, v20.2d, 12\n    # Shift left and insert (int64) to combine all 8 bits into one byte\n    sli v0.2d, v0.2d, 36\n    sli v1.2d, v1.2d, 36\n    sli v2.2d, v2.2d, 36\n    sli v3.2d, v3.2d, 36\n\n    sli v17.2d, v17.2d, 36\n    sli v18.2d, v18.2d, 36\n    sli v19.2d, v19.2d, 36\n    sli v20.2d, v20.2d, 36\n    # Combine all the byte mask insto a bit 64-bit mask for EOB and zero masks\n    ldr d4, .shuf_mask\n    tbl v5.8b, {v0.16b - v3.16b}, v4.8b\n    tbl v6.8b, {v17.16b - v20.16b}, v4.8b\n    # Extract lanes\n    mov x0, v5.d[0]\n    mov x1, v6.d[0]\n    # Compute EOB\n    rbit x0, x0\n    clz x0, x0\n    mov x2, 64\n    sub x0, x2, x0\n    # Not of zero mask (so 1 bits indecates non-zeroes)\n    mvn x1, x1\n    ret\n</code></pre>\n<p>If you look carefully at the code, you will see, that I decided that while generating the mask to find EOB is useful, I can use the same method to generate the mask for zero values, and then I can find the next nonzero value, and zero runlength this way:</p>\n<pre><code>uint64_t skip =__builtin_clzl(zero_mask &lt;&lt; k);\nr += skip;\nk += skip;\n</code></pre>\n<p>Similarly for <code>encode_mcu_AC_first</code>:</p>\n<pre><code class=\"language-asm\">    # Load the block\n    ld2 {v0.16b - v1.16b}, [x0], 32\n    ld2 {v16.16b - v17.16b}, [x0], 32\n    ld2 {v18.16b - v19.16b}, [x0], 32\n    ld2 {v20.16b - v21.16b}, [x0]\n\n    mov v4.16b, v1.16b\n    mov v5.16b, v17.16b\n    mov v6.16b, v19.16b\n    mov v7.16b, v21.16b\n    mov v1.16b, v16.16b\n    mov v2.16b, v18.16b\n    mov v3.16b, v20.16b\n\n    # Load the order \n    ld4 {v16.16b - v19.16b}, [x1], 64\n    ld4 {v17.16b - v20.16b}, [x1], 64\n    ld4 {v18.16b - v21.16b}, [x1], 64\n    ld4 {v19.16b - v22.16b}, [x1]\n    # Table lookup, LSB and MSB independently\n    tbl v20.16b, {v0.16b - v3.16b}, v16.16b\n    tbl v16.16b, {v4.16b - v7.16b}, v16.16b\n    tbl v21.16b, {v0.16b - v3.16b}, v17.16b\n    tbl v17.16b, {v4.16b - v7.16b}, v17.16b\n    tbl v22.16b, {v0.16b - v3.16b}, v18.16b\n    tbl v18.16b, {v4.16b - v7.16b}, v18.16b\n    tbl v23.16b, {v0.16b - v3.16b}, v19.16b\n    tbl v19.16b, {v4.16b - v7.16b}, v19.16b\n    # Interleave MSB and LSB back\n    zip1 v0.16b, v20.16b, v16.16b\n    zip2 v1.16b, v20.16b, v16.16b\n    zip1 v2.16b, v21.16b, v17.16b\n    zip2 v3.16b, v21.16b, v17.16b\n    zip1 v4.16b, v22.16b, v18.16b\n    zip2 v5.16b, v22.16b, v18.16b\n    zip1 v6.16b, v23.16b, v19.16b\n    zip2 v7.16b, v23.16b, v19.16b\n    # -Al\n    neg w4, w4\n    dup v24.8h, w4\n    # Compare with 0 to get negative mask\n    cmge v16.8h, v0.8h, #0\n    # Absolute value and shift by Al\n    abs v0.8h, v0.8h\n    sshl v0.8h, v0.8h, v24.8h\n    cmge v17.8h, v1.8h, #0\n    abs v1.8h, v1.8h\n    sshl v1.8h, v1.8h, v24.8h\n    cmge v18.8h, v2.8h, #0\n    abs v2.8h, v2.8h\n    sshl v2.8h, v2.8h, v24.8h\n    cmge v19.8h, v3.8h, #0\n    abs v3.8h, v3.8h\n    sshl v3.8h, v3.8h, v24.8h\n    cmge v20.8h, v4.8h, #0\n    abs v4.8h, v4.8h\n    sshl v4.8h, v4.8h, v24.8h\n    cmge v21.8h, v5.8h, #0\n    abs v5.8h, v5.8h\n    sshl v5.8h, v5.8h, v24.8h\n    cmge v22.8h, v6.8h, #0\n    abs v6.8h, v6.8h\n    sshl v6.8h, v6.8h, v24.8h\n    cmge v23.8h, v7.8h, #0\n    abs v7.8h, v7.8h\n    sshl v7.8h, v7.8h, v24.8h\n    # ~\n    mvn v24.16b, v0.16b\n    mvn v25.16b, v1.16b\n    mvn v26.16b, v2.16b\n    mvn v27.16b, v3.16b\n    mvn v28.16b, v4.16b\n    mvn v29.16b, v5.16b\n    mvn v30.16b, v6.16b\n    mvn v31.16b, v7.16b\n    # Select\n    bsl v16.16b, v0.16b, v24.16b\n    bsl v17.16b, v1.16b, v25.16b\n    bsl v18.16b, v2.16b, v26.16b\n    bsl v19.16b, v3.16b, v27.16b\n    bsl v20.16b, v4.16b, v28.16b\n    bsl v21.16b, v5.16b, v29.16b\n    bsl v22.16b, v6.16b, v30.16b\n    bsl v23.16b, v7.16b, v31.16b\n    # Store t1\n    st1 {v0.16b - v3.16b}, [x2], 64\n    st1 {v4.16b - v7.16b}, [x2]\n    # Store t2\n    st1 {v16.16b - v19.16b}, [x3], 64\n    st1 {v20.16b - v23.16b}, [x3]\n    # Compute zero mask like before\n    cmeq v17.8h, v0.8h, #0\n    cmeq v18.8h, v1.8h, #0\n    cmeq v19.8h, v2.8h, #0\n    cmeq v20.8h, v3.8h, #0\n    cmeq v21.8h, v4.8h, #0\n    cmeq v22.8h, v5.8h, #0\n    cmeq v23.8h, v6.8h, #0\n    cmeq v24.8h, v7.8h, #0\n\n    uzp1 v17.16b, v17.16b, v18.16b\n    uzp1 v18.16b, v19.16b, v20.16b\n    uzp1 v19.16b, v21.16b, v22.16b\n    uzp1 v20.16b, v23.16b, v24.16b\n\n    sli v17.8h, v17.8h, 15\n    sli v18.8h, v18.8h, 15\n    sli v19.8h, v19.8h, 15\n    sli v20.8h, v20.8h, 15\n\n    sri v17.4s, v17.4s, 18\n    sri v18.4s, v18.4s, 18\n    sri v19.4s, v19.4s, 18\n    sri v20.4s, v20.4s, 18\n\n    ushr v17.2d, v17.2d, 12\n    ushr v18.2d, v18.2d, 12\n    ushr v19.2d, v19.2d, 12\n    ushr v20.2d, v20.2d, 12\n\n    sli v17.2d, v17.2d, 36\n    sli v18.2d, v18.2d, 36\n    sli v19.2d, v19.2d, 36\n    sli v20.2d, v20.2d, 36\n\n    ldr d4, .shuf_mask\n    tbl v6.8b, {v17.16b - v20.16b}, v4.8b\n\n    mov x0, v6.d[0]\n    mvn x0, x0\n    ret\n</code></pre>\n<h2 id=\"finalresultsandpower\">Final results and power</h2>\n<p>The final version of our jpegtran managed to reduce the test image in 2.756 seconds. Or an extra 1.26X speedup, that gets it incredibly close to the performance of the Xeon on that image. As a bonus batch performance also improved!</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2018/04/jpegtran-asm-1.png\" alt=\"jpegtran-asm-1\" loading=\"lazy\"></p>\n<p>Another favorite part of mine, working with the Qualcomm Centriq CPU is the ability to take power readings, and be pleasantly surprised every time.</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2018/04/jpegtran-power-1.png\" alt=\"jpegtran-power-1\" loading=\"lazy\"></p>\n<p>With the new implementation Centriq outperforms the Xeon at batch reduction for every number of workers. We usually run Polish with four workers, for which Centriq is now 1.3 times faster while also 6.5 times more power efficient.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>It is evident that the Qualcomm Centriq is a powerful processor, that definitely provides a good bang for a buck. However, years of Intel leadership in the server and desktop space mean that a lot of software is better optimized for Intel processors.</p>\n<p>For the most part writing optimizations for ARMv8 is not difficult, and we will be adjusting our software as needed, and publishing our efforts as we go.</p>\n<p>You can find the updated code on our <a href=\"https://github.com/cloudflare/jpegtran\">Github</a> page.</p>\n<h3 id=\"usefulresources\">Useful resources</h3>\n<ul>\n<li><a href=\"https://developer.arm.com/docs/100069/latest\">Arm Compiler armasm User Guide</a></li>\n<li><a href=\"http://infocenter.arm.com/help/topic/com.arm.doc.ihi0055b/IHI0055B_aapcs64.pdf\">Procedure Call Standard for the ARM 64-bit Architecture</a></li>\n<li><a href=\"http://infocenter.arm.com/help/topic/com.arm.doc.ihi0073a/IHI0073A_arm_neon_intrinsics_ref.pdf\">ARM NEON Intrinsics Reference</a></li>\n<li><a href=\"https://community.arm.com/processors/b/blog/posts/coding-for-neon---part-1-load-and-stores\">Coding for NEON</a></li>\n</ul>\n<!--kg-card-end: markdown-->",
		"id": "5d16453b41acde0011a956e1",
		"meta_description": null,
		"meta_title": null,
		"og_description": null,
		"og_image": null,
		"og_title": null,
		"primary_author": {
			"id": "5d1644b141acde0011a94f4e",
			"name": "Vlad Krasnov",
			"slug": "vlad-krasnov",
			"profile_image": "http://blog.cloudflare.com/content/images/2017/03/165e069.jpg",
			"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-131.png",
			"bio": null,
			"website": null,
			"location": null,
			"facebook": null,
			"twitter": null,
			"meta_title": null,
			"meta_description": null,
			"url": "http://blog.cloudflare.com/author/vlad-krasnov/"
		},
		"primary_tag": {
			"id": "5d16450341acde0011a95160",
			"name": "Speed & Reliability",
			"slug": "speed-and-reliability",
			"description": null,
			"feature_image": "http://blog.cloudflare.com/content/images/2020/10/Speed---Reliability-1.png",
			"visibility": "public",
			"meta_title": "Cloudflare Blog: Speed & Reliability",
			"meta_description": "Collection of Cloudflare blog posts tagged 'Speed & Reliability'.",
			"og_image": null,
			"og_title": null,
			"og_description": null,
			"twitter_image": null,
			"twitter_title": null,
			"twitter_description": null,
			"codeinjection_head": null,
			"codeinjection_foot": null,
			"canonical_url": null,
			"accent_color": null,
			"url": "http://blog.cloudflare.com/tag/speed-and-reliability/"
		},
		"published_at": "2018-04-13T17:38:47.000+01:00",
		"reading_time": 17,
		"slug": "neon-is-the-new-black",
		"tags": [
			{
				"id": "5d16450341acde0011a95160",
				"name": "Speed & Reliability",
				"slug": "speed-and-reliability",
				"description": null,
				"feature_image": "http://blog.cloudflare.com/content/images/2020/10/Speed---Reliability-1.png",
				"visibility": "public",
				"meta_title": "Cloudflare Blog: Speed & Reliability",
				"meta_description": "Collection of Cloudflare blog posts tagged 'Speed & Reliability'.",
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/speed-and-reliability/"
			},
			{
				"id": "5d16450341acde0011a95186",
				"name": "Cloudflare Polish",
				"slug": "cloudflare-polish",
				"description": "Polish image optimization tool",
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/cloudflare-polish/"
			}
		],
		"title": "NEON is the new black: fast JPEG optimization on ARM server",
		"twitter_description": null,
		"twitter_image": null,
		"twitter_title": null,
		"updated_at": "2023-08-24T21:29:31.000+01:00",
		"url": "http://blog.cloudflare.com/neon-is-the-new-black/",
		"uuid": "f489cc1f-eeff-4d72-b419-57bf9beeda6e",
		"visibility": "public"
	}
}