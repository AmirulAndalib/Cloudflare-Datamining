<div class="mb2 gray5">5 min read</div>
<div class="post-content lh-copy gray1"><!--kg-card-begin: markdown-->
	<p><small>This post is also available in <a href="https://blog.cloudflare.com/zh-cn/workers-ai-streaming-zh-cn">ç®€ä½“ä¸­æ–‡</a>, <a href="https://blog.cloudflare.com/zh-tw/workers-ai-streaming-zh-tw">ç¹é«”ä¸­æ–‡</a>, <a href="https://blog.cloudflare.com/ja-jp/workers-ai-streaming-ja-jp">æ—¥æœ¬èªž</a>, <a href="https://blog.cloudflare.com/de-de/workers-ai-streaming-de-de">Deutsch</a>, <a href="https://blog.cloudflare.com/fr-fr/workers-ai-streaming-fr-fr">FranÃ§ais</a>, <a href="https://blog.cloudflare.com/es-es/workers-ai-streaming-es-es">EspaÃ±ol</a>, and <a href="https://blog.cloudflare.com/ko-kr/workers-ai-streaming-ko-kr">í•œêµ­ì–´</a>.</small></p>
	<!--kg-card-end: markdown-->
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--3--2.png" class="kg-image" alt="Streaming LLMs and longer context lengths available in Workers AI" loading="lazy" width="1600" height="901"></figure>
	<p>Workers AI is our serverless GPU-powered inference platform running on top of Cloudflareâ€™s global network. It provides a growing catalog of off-the-shelf models that run seamlessly with Workers and enable developers to build powerful and scalable AI applications in minutes. Weâ€™ve already seen developers doing amazing things with Workers AI, and we canâ€™t wait to see what they do as we continue to expand the platform. To that end, today weâ€™re excited to announce some of our most-requested new features: streaming responses for all <a href="https://www.cloudflare.com/learning/ai/what-is-large-language-model">Large Language Models</a> (LLMs) on Workers AI, larger context and sequence windows, and a full-precision <a href="https://developers.cloudflare.com/workers-ai/models/llm">Llama-2</a> model variant.</p>
	<p>If youâ€™ve used ChatGPT before, then youâ€™re familiar with the benefits of response streaming, where responses flow in token by token. LLMs work internally by generating responses sequentially using a process of repeated inference â€” the full output of a LLM model is essentially a sequence of hundreds or thousands of individual prediction tasks. For this reason, while it only takes a few milliseconds to generate a single token, generating the full response takes longer, on the order of seconds. The good news is we can start displaying the response as soon as the first tokens are generated, and append each additional token until the response is complete. This yields a much better experience for the end user â€” Â&nbsp;displaying text incrementally as it's generated not only provides instant responsiveness, but also gives the end-user time to read and interpret the text.</p>
	<p>As of today, you can now use response streaming for any LLM model in our catalog, including the very popular <a href="https://developers.cloudflare.com/workers-ai/models/llm">Llama-2 model</a>. Hereâ€™s how it works.</p>
	<h3 id="server-sent-events-a-little-gem-in-the-browser-api">Server-sent events: a little gem in the browser API</h3>
	<p><a href="https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events">Server-sent events</a> are easy to use, simple to implement on the server side, standardized, and broadly available across many platforms natively or as a polyfill. Server-sent events fill a niche of handling a stream of updates from the server, removing the need for the boilerplate code that would otherwise be necessary to handle the event stream.</p><!--kg-card-begin: html-->
	<style type="text/css">
		.tg {
			border-collapse: collapse;
			border-color: #ccc;
			border-spacing: 0;
		}

		.tg td {
			background-color: #fff;
			border-color: #ccc;
			border-style: solid;
			border-width: 1px;
			color: #333;
			font-family: Arial, sans-serif;
			font-size: 14px;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg th {
			background-color: #f0f0f0;
			border-color: #ccc;
			border-style: solid;
			border-width: 1px;
			color: #333;
			font-family: Arial, sans-serif;
			font-size: 14px;
			font-weight: normal;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg .tg-lt9p {
			background-color: #F3F3F3;
			text-align: left;
			vertical-align: top
		}

		.tg .tg-9qck {
			background-color: #F3F3F3;
			font-weight: bold;
			text-align: left;
			vertical-align: top
		}

		.tg .tg-0lax {
			text-align: left;
			vertical-align: top
		}
	</style>
	<table class="tg" width="100%">
		<thead>
			<tr>
				<th class="tg-lt9p"></th>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Easy-to-use</span></th>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Streaming</span></th>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Bidirectional</span></th>
			</tr>
		</thead>
		<tbody>
			<tr>
				<td class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">fetch</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">âœ…</span></td>
				<td class="tg-0lax"></td>
				<td class="tg-0lax"></td>
			</tr>
			<tr>
				<td class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Server-sent events</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">âœ…</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">âœ…</span></td>
				<td class="tg-0lax"></td>
			</tr>
			<tr>
				<td class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Websockets</span></td>
				<td class="tg-0lax"></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">âœ…</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">âœ…</span></td>
			</tr>
		</tbody>
	</table><!--kg-card-end: html--><!--kg-card-begin: markdown-->
	<center><small><sup>Comparing fetch, server-sent events, and websockets</sup></small></center><!--kg-card-end: markdown-->
	<p>To get started using streaming on Workers AIâ€™s text generation models with server-sent events, set the â€œstreamâ€ parameter to true in the input of request. This will change the response format and <code>mime-type</code> to <code>text/event-stream</code>.</p>
	<p>Hereâ€™s an example of using streaming with the <a href="https://developers.cloudflare.com/workers-ai/get-started/rest-api">REST API</a>:</p>
	<pre><code class="language-bash">curl -X POST \
"https://api.cloudflare.com/client/v4/accounts/&lt;account&gt;/ai/run/@cf/meta/llama-2-7b-chat-int8" \
-H "Authorization: Bearer &lt;token&gt;" \
-H "Content-Type:application/json" \
-d '{ "prompt": "where is new york?", "stream": true }'

data: {"response":"New"}

data: {"response":" York"}

data: {"response":" is"}

data: {"response":" located"}

data: {"response":" in"}

data: {"response":" the"}

...

data: [DONE]</code></pre>
	<p>And hereâ€™s an example using a Worker script:</p>
	<pre><code class="language-TypeScript">import { Ai } from "@cloudflare/ai";
export default {
    async fetch(request, env, ctx) {
        const ai = new Ai(env.AI, { sessionOptions: { ctx: ctx } });
        const stream = await ai.run(
            "@cf/meta/llama-2-7b-chat-int8",
            { prompt: "where is new york?", stream: true  }
        );
        return new Response(stream,
            { headers: { "content-type": "text/event-stream" } }
        );
    }
}</code></pre>
	<p>If you want to consume the output event-stream from this Worker in a browser page, the client-side JavaScript is something like:</p>
	<pre><code class="language-typescript">const source = new EventSource("/worker-endpoint");
source.onmessage = (event) =&gt; {
    if(event.data=="[DONE]") {
        // SSE spec says the connection is restarted
        // if we don't explicitly close it
        source.close();
        return;
    }
    const data = JSON.parse(event.data);
    el.innerHTML += data.response;
}</code></pre>
	<p>You can use this simple code with any simple HTML page, complex SPAs using React or other Web frameworks.</p>
	<p>This creates a much more interactive experience for the user, who now sees the page update as the response is incrementally created, instead of waiting with a spinner until the entire response sequence has been generated. Try it out streaming on <a href="https://ai.cloudflare.com">ai.cloudflare.com</a>.</p>
	<figure class="kg-card kg-image-card kg-width-wide"><img src="https://blog.cloudflare.com/content/images/2023/10/llama-streaming.gif" class="kg-image" alt="" loading="lazy" width="1518" height="610"></figure>
	<p>Workers AI supports streaming text responses for the <a href="https://developers.cloudflare.com/workers-ai/models/llm">Llama-2</a> model and any future LLM models we are adding to our catalog.</p>
	<p>But this is not all.</p>
	<h3 id="higher-precision-longer-context-and-sequence-lengths">Higher precision, longer context and sequence lengths</h3>
	<p>Another top request we heard from our community after the launch of Workers AI was for longer questions and answers in our Llama-2 model. In LLM terminology, this translates to higher context length (the number of tokens the model takes as input before making the prediction) and higher sequence length (the number of tokens the model generates in the response.)</p>
	<p>Weâ€™re listening, and in conjunction with streaming, today we are adding a higher 16-bit full-precision Llama-2 variant to the catalog, and increasing the context and sequence lengths for the existing 8-bit version.</p><!--kg-card-begin: html-->
	<style type="text/css">
		.tg {
			border-collapse: collapse;
			border-spacing: 0;
		}

		.tg td {
			border-color: black;
			border-style: solid;
			border-width: 1px;
			font-family: Arial, sans-serif;
			font-size: 14px;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg th {
			border-color: black;
			border-style: solid;
			border-width: 1px;
			font-family: Arial, sans-serif;
			font-size: 14px;
			font-weight: normal;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg .tg-9qck {
			background-color: #F3F3F3;
			font-weight: bold;
			text-align: left;
			vertical-align: top
		}

		.tg .tg-0lax {
			text-align: left;
			vertical-align: top
		}
	</style>
	<table class="tg" width="100%">
		<thead>
			<tr>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Model</span></th>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Context length (in)</span></th>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Sequence length (out)</span></th>
			</tr>
		</thead>
		<tbody>
			<tr>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">@cf/meta/llama-2-7b-chat-int8</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">2048 (768 before)</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">1800 (256 before)</span></td>
			</tr>
			<tr>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">@cf/meta/llama-2-7b-chat-fp16</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">3072</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">2500</span></td>
			</tr>
		</tbody>
	</table><!--kg-card-end: html-->
	<p>Streaming, higher precision, and longer context and sequence lengths provide a better user experience and enable new, richer applications using large language models in Workers AI.</p>
	<p>Check the Workers AI <a href="https://developers.cloudflare.com/workers-ai">developer documentation</a> for more information and options. If you have any questions or feedback about Workers AI, please come see us in the <a href="https://community.cloudflare.com">Cloudflare Community</a> and the <a href="https://discord.gg/cloudflaredev">Cloudflare Discord</a>.<br>If you are interested in machine learning and serverless AI, the Cloudflare Workers AI team is building a global-scale platform and tools that enable our customers to run fast, low-latency inference tasks on top of our network. Check our <a href="https://www.cloudflare.com/careers/jobs">jobs page</a> for opportunities.</p>
</div>