{
	"post": {
		"id": "5d16453b41acde0011a955d6",
		"uuid": "b7f23f7c-3de4-439a-a271-196411b09f32",
		"title": "Debugging war story: the mystery of NXDOMAIN",
		"slug": "debugging-war-story-the-mystery-of-nxdomain",
		"html": "<!--kg-card-begin: markdown--><p>The following blog post describes a debugging adventure on Cloudflare's <a href=\"https://mesos.apache.org/\">Mesos</a>-based cluster. This internal cluster is primarily used to process log file information so that Cloudflare customers have analytics, and for our systems that detect and respond to attacks.</p>\n<p>The problem encountered didn't have any effect on our customers, but did have engineers scratching their heads...</p>\n<h3 id=\"theproblem\">The Problem</h3>\n<p>At some point in one of our cluster we started seeing errors like this (an NXDOMAIN for an existing domain on our internal DNS):</p>\n<pre><code>lookup some.existing.internal.host on 10.1.0.9:53: no such host\n</code></pre>\n<p>This seemed very weird, since the domain did indeed exist. It was one of our internal domains! Engineers had mentioned that they'd seen this behaviour, so we decided to investigate deeper. Queries triggering this error were varied and ranged from dynamic SRV records managed by mesos-dns to external domains looked up from inside the cluster.</p>\n<p>Our first naive attempt was to run the following in a loop:</p>\n<pre><code>while true; do dig some.existing.internal.host &gt; /tmp/dig.txt || break; done\n</code></pre>\n<p>Running this for a while on one server did not reproduce the problem: all the lookups were successful. Then we took our service logs for a day and did a grep for “no such host” and similar messages. Errors were happening sporadically. There were hours between errors and no obvious pattern that could lead us to any conclusion. Our investigation discarded the possibility that the error lay in Go, which we use for lots of our services, since errors were coming from Java services too.</p>\n<h3 id=\"intotherabbithole\">Into the rabbit hole</h3>\n<p>We used to run <a href=\"https://www.unbound.net/\">Unbound</a> on a single IP across a few machines for our cluster DNS resolver. BGP is then responsible for announcing internal routes from the machines to the router. We decided to try to find a pattern by sending lots of requests from different machines and recording errors. Here’s what our load testing program looked like at first:</p>\n<pre><code>package main\n\nimport (\n\t&quot;flag&quot;\n\t&quot;fmt&quot;\n\t&quot;net&quot;\n\t&quot;os&quot;\n\t&quot;time&quot;\n)\n\nfunc main() {\n\tn := flag.String(&quot;n&quot;, &quot;&quot;, &quot;domain to look up&quot;)\n\tp := flag.Duration(&quot;p&quot;, time.Millisecond*10, &quot;pause between lookups&quot;)\n\n\tflag.Parse()\n\n\tif *n == &quot;&quot; {\n\t\tflag.PrintDefaults()\n\t\tos.Exit(1)\n\t}\n\n\tfor {\n\t\t_, err := net.LookupHost(*n)\n\t\tif err != nil {\n\t\t\tfmt.Println(&quot;error:&quot;, err)\n\t\t}\n\n\t\ttime.Sleep(*p)\n\t}\n}\n</code></pre>\n<p>We run <code>net.LookupHost</code> in a loop with small pauses and log errors; that’s it. Packaging this into a <a href=\"https://www.docker.com/\">Docker</a> container and running on Marathon was an obvious choice for us, since that is how we run other services anyway. Logs get shipped to <a href=\"https://kafka.apache.org/\">Kafka</a> and then to <a href=\"https://www.elastic.co/products/kibana\">Kibana</a>, where we can analyze them. Running this program on 65 machines doing lookups every 50ms shows the following error distribution (from high to low) across hosts:</p>\n<p><a href=\"http://blog.cloudflare.com/content/images/2016/12/image01.png\"><img src=\"http://blog.cloudflare.com/content/images/2016/12/image01.png\" alt=\"\" loading=\"lazy\"></a></p>\n<p>We saw no strong correlation to racks or specific machines. Errors happened on many hosts, but not on all of them and in different time windows errors happen on different machines. Putting time on X axis and number of errors on Y axis showed the following:</p>\n<p><a href=\"http://blog.cloudflare.com/content/images/2016/12/image00.png\"><img src=\"http://blog.cloudflare.com/content/images/2016/12/image00.png\" alt=\"\" loading=\"lazy\"></a></p>\n<p>To see if some particular DNS recursor had gone crazy, we stopped all load generators on regular machines and started the load generation tool on the recursors themselves. There were no errors in a few hours, which suggested that Unbound was perfectly healthy.</p>\n<p>We started to suspect that packet loss was the issue, but why would “no such host” occur? It should only happen when an NXDOMAIN error is in a DNS response, but our theory was that replies didn’t come back at all.</p>\n<h3 id=\"themissing\">The Missing</h3>\n<p>To test the hypothesis that losing packets can lead to a “no such host” error, we first tried blocking outgoing traffic on port 53:</p>\n<pre><code>sudo iptables -A OUTPUT -p udp --dport 53 -j DROP\n</code></pre>\n<p>In this case, dig and similar tools just time out, but don’t return “no such host”:</p>\n<pre><code>; &lt;&lt;&gt;&gt; DiG 9.9.5-9+deb8u3-Debian &lt;&lt;&gt;&gt; cloudflare.com\n;; global options: +cmd\n;; connection timed out; no servers could be reached\n</code></pre>\n<p>Go is a bit smarter and tells you more about what’s going on, but doesn't return “no such host” either:</p>\n<pre><code>error: lookup cloudflare.com on 10.1.0.9:53: write udp 10.1.14.20:47442-&gt;10.1.0.9:53: write: operation not permitted\n</code></pre>\n<p>Since the Linux kernel tells the sender that it dropped packets, we had to point the nameserver to some black hole in the network that does nothing with packets to mimic packet loss. Still no luck:</p>\n<pre><code>error: lookup cloudflare.com on 10.1.2.9:53: read udp 10.1.14.20:39046-&gt;10.1.2.9:53: i/o timeout\n</code></pre>\n<p>To continue <em>blaming the network</em> we had to support our assumptions somehow, so we added timing information to our lookups:</p>\n<pre><code>s := time.Now()\n_, err := net.LookupHost(*n)\ne := time.Now().Sub(s).Seconds()\nif err != nil {\n    log.Printf(&quot;error after %.4fs: %s&quot;, e, err)\n} else if e &gt; 1 {\n    log.Printf(&quot;success after %.4fs&quot;, e)\n}\n</code></pre>\n<p>To be honest, we started by timing errors and added success timing later. Errors were happening after 10s, comparatively many successful responses were coming after 5s. It does look like packet loss, but still does not tell us why “no such host” happens.</p>\n<p>Since now we were in a place when we knew which hosts were more likely to be affected by this, we ran the following two commands in parallel in two <code>screen</code> sessions:</p>\n<pre><code>while true; do dig cloudflare.com &gt; /tmp/dig.log || break; done; date; sudo killall tcpdump\nsudo tcpdump -w /state/wtf.cap port 53\n</code></pre>\n<p>The point was to get a network dump with failed resolves. In there, we saw the following queries:</p>\n<pre><code>00.00s A cloudflare.com\n05.00s A cloudflare.com\n10.00s A cloudflare.com.in.our.internal.domain\n</code></pre>\n<p>Two queries time out without any answer, but the third one gets lucky and succeeds. Naturally, we don’t have cloudflare.com in our internal domain, so Unbound rightfully gives NXDOMAIN in reply, 10s after the lookup was initiated.</p>\n<h3 id=\"bingo\">Bingo</h3>\n<p>Let’s look at /etc/resolv.conf to understand more:</p>\n<pre><code>nameserver 10.1.0.9\nsearch in.our.internal.domain\n</code></pre>\n<p>Using the search keyword allows us to use short hostnames instead of FQDN, making <code>myhost</code> transparently equivalent to <code>myhost.in.our.internal.domain</code>.</p>\n<p>For the DNS resolver it means the following: for any DNS query ask the nameserver 10.1.0.9, if this fails, append <code>.in.our.internal.domain</code> to the query and retry. It doesn’t matter what failure occurs for the original DNS query. Usually it is NXDOMAIN, but in our case it’s a read timeout due to packet loss.</p>\n<p>The following events seemed to have to occur for a “no such host” error to appear:</p>\n<ol>\n<li>The original DNS request has to be lost</li>\n<li>The retry that is sent after 5 seconds has to be lost</li>\n<li>The subsequent query for the internal domain (caused by the <code>search</code> option) has to succeed and return NXDOMAIN</li>\n</ol>\n<p>On the other hand, to observe a timed out DNS query instead of NXDOMAIN, you have to lose four packets sent 5 seconds one after another (2 for the original query and 2 for the internal version of your domain), which is a much smaller probability. In fact, we only saw an NXDOMAIN after 15s once and never saw an error after 20s.</p>\n<p>To validate that assumption, we built a proof-of-concept DNS server that drops all requests for cloudflare.com, but sends an NXDOMAIN for existing domains:</p>\n<pre><code>package main\n\nimport (\n\t&quot;github.com/miekg/dns&quot;\n\t&quot;log&quot;\n)\n\nfunc main() {\n\tserver := &amp;dns.Server{Addr: &quot;:53&quot;, Net: &quot;udp&quot;}\n\n\tdns.HandleFunc(&quot;.&quot;, func(w dns.ResponseWriter, r *dns.Msg) {\n\t\tm := &amp;dns.Msg{}\n\t\tm.SetReply(r)\n\n\t\tfor _, q := range r.Question {\n\t\t\tlog.Printf(&quot;checking %s&quot;, q.Name)\n\t\t\tif q.Name == &quot;cloudflare.com.&quot; {\n\t\t\t\tlog.Printf(&quot;ignoring %s&quot;, q.Name)\n\t\t\t\t// just ignore\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\n\t\tw.WriteMsg(m)\n\t})\n\n\tlog.Printf(&quot;listening..&quot;)\n\n\tif err := server.ListenAndServe(); err != nil {\n\t\tlog.Fatalf(&quot;error listening: %s&quot;, err)\n\t}\n}\n</code></pre>\n<p>Finally, we found what was going on and had a way of reliably replicating that behaviour.</p>\n<h3 id=\"solutions\">Solutions</h3>\n<p>Let's think about how we can improve our client to better handle these transient network issues, making it more resilient. The man page for resolv.conf tells you that you have two knobs: the <code>timeout</code> and <code>retries</code> options. The default values are 5 and 2 respectively.</p>\n<p>Unless you keep your DNS server very busy, it is very unlikely that it would take it more than 1 second to reply. In fact, if you happen to have a network device on the Moon, you can expect it to reply in 3 seconds. If your nameserver lives in the next rack and is reachable over a high-speed network, you can safely assume that if there is no reply after 1 second, your DNS server did not get your query. If you want to have less weird “no such domain” errors that make you scratch your head, you might as well increase retries. The more times you retry with transient packet loss, the less chance of failure. The more often you retry, the higher chances to finish faster.</p>\n<p>Imagine that you have truly random 1% packet loss.</p>\n<ul>\n<li>2 retries, 5s timeout: max 10s wait before error, 0.001% chance of failure</li>\n<li>5 retries, 1s timeout: max 5s wait before error, 0.000001% chance of failure</li>\n</ul>\n<p>In real life, the distribution would be different due to the fact that packet loss is not random, but you can expect to wait much less for DNS to reply with this type of change.</p>\n<p>As you know many system libraries that provide DNS resolution like glibc, nscd, systemd-resolved are not hardened to handle being on the internet or in a environment with packet losses. We have faced the challenge of creating a reliable and fast DNS resolution environment a number of times as we have grown, only to later discover that the solution is not perfect.</p>\n<h3 id=\"overtoyou\">Over to you</h3>\n<p>Given what you have read in this article about packet loss and split-DNS/private-namespace, how would you design a fast and reliable resolution setup? What software would you use and why? What tuning changes from standard configuration would you use?</p>\n<p>We'd love to hear your ideas in the comments. And, if you like working on problems like send us your resume. We are <a href=\"https://www.cloudflare.com/join-our-team/\">hiring</a>.</p>\n<!--kg-card-end: markdown-->",
		"comment_id": "4872",
		"feature_image": "http://blog.cloudflare.com/content/images/2018/08/image01-2.png",
		"featured": false,
		"visibility": "public",
		"created_at": "2016-10-31T20:03:54.000+00:00",
		"updated_at": "2018-08-28T21:51:24.000+01:00",
		"published_at": "2016-12-07T14:11:49.000+00:00",
		"custom_excerpt": "The following blog post describes a debugging adventure on Cloudflare's Mesos-based cluster. This internal cluster is primarily used to process log file information so that Cloudflare customers have analytics, and for our systems that detect and respond to attacks.",
		"codeinjection_head": null,
		"codeinjection_foot": null,
		"custom_template": null,
		"canonical_url": null,
		"tags": [
			{
				"id": "5d16450341acde0011a951d6",
				"name": "DNS",
				"slug": "dns",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/dns/"
			},
			{
				"id": "5d16450341acde0011a951ce",
				"name": "Reliability",
				"slug": "reliability",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/reliability/"
			}
		],
		"authors": [
			{
				"id": "5d1644b141acde0011a94f77",
				"name": "Ivan Babrou",
				"slug": "ivan",
				"profile_image": "http://blog.cloudflare.com/content/images/2022/09/Ivan-Babrou.png",
				"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-84.png",
				"bio": null,
				"website": null,
				"location": null,
				"facebook": null,
				"twitter": null,
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/ivan/"
			}
		],
		"primary_author": {
			"id": "5d1644b141acde0011a94f77",
			"name": "Ivan Babrou",
			"slug": "ivan",
			"profile_image": "http://blog.cloudflare.com/content/images/2022/09/Ivan-Babrou.png",
			"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-84.png",
			"bio": null,
			"website": null,
			"location": null,
			"facebook": null,
			"twitter": null,
			"meta_title": null,
			"meta_description": null,
			"url": "http://blog.cloudflare.com/author/ivan/"
		},
		"primary_tag": {
			"id": "5d16450341acde0011a951d6",
			"name": "DNS",
			"slug": "dns",
			"description": null,
			"feature_image": null,
			"visibility": "public",
			"meta_title": null,
			"meta_description": null,
			"og_image": null,
			"og_title": null,
			"og_description": null,
			"twitter_image": null,
			"twitter_title": null,
			"twitter_description": null,
			"codeinjection_head": null,
			"codeinjection_foot": null,
			"canonical_url": null,
			"accent_color": null,
			"url": "http://blog.cloudflare.com/tag/dns/"
		},
		"url": "http://blog.cloudflare.com/debugging-war-story-the-mystery-of-nxdomain/",
		"excerpt": "The following blog post describes a debugging adventure on Cloudflare's Mesos-based cluster. This internal cluster is primarily used to process log file information so that Cloudflare customers have analytics, and for our systems that detect and respond to attacks.",
		"reading_time": 7,
		"access": true,
		"comments": false,
		"og_image": null,
		"og_title": null,
		"og_description": null,
		"twitter_image": null,
		"twitter_title": null,
		"twitter_description": null,
		"meta_title": null,
		"meta_description": null,
		"email_subject": null,
		"frontmatter": null,
		"feature_image_alt": null,
		"feature_image_caption": null
	},
	"locale": "en-us"
}