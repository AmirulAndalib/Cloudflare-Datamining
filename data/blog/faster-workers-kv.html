<div class="mb2 gray5">7 min read</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/4bRdCswtLjBJq8kr1Z2jzK/7170ee5412f53391b8a9d3f961d8528e/BLOG-2518_1.png" alt="">
<div class="post-content lh-copy gray1">
	<p>Speed is a critical factor that dictates Internet behavior. Every additional millisecond a user spends waiting for your web page to load results in them abandoning your website. The old adage remains as true as ever: <a href="https://www.cloudflare.com/learning/performance/more/website-performance-conversion-rates"><u>faster websites result in higher conversion rates</u></a>. And with such outcomes tied to Internet speed, we believe a faster Internet is a better Internet.</p>
	<p>Customers often use <a href="https://developers.cloudflare.com/kv"><u>Workers KV</u></a> to provide <a href="https://developers.cloudflare.com/workers"><u>Workers</u></a> with key-value data for configuration, routing, personalization, experimentation, or serving assets. Many of Cloudflare’s own products rely on KV for just this purpose: <a href="https://developers.cloudflare.com/pages"><u>Pages</u></a> stores static assets, <a href="https://developers.cloudflare.com/cloudflare-one/policies/access"><u>Access</u></a> stores authentication credentials, <a href="https://developers.cloudflare.com/ai-gateway"><u>AI Gateway</u></a> stores routing configuration, and <a href="https://developers.cloudflare.com/images"><u>Images</u></a> stores configuration and assets, among others. So KV’s speed affects the latency of every request to an application, throughout the entire lifecycle of a user session.&nbsp;</p>
	<p>Today, we’re announcing up to 3x faster KV hot reads, with all KV operations faster by up to 20ms. And we want to pull back the curtain and show you how we did it.&nbsp;</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/67VzWOTRpMd9Dbc6ZM7M4M/7322595ea48f7cff0bacc1db41f49597/BLOG-2518_2.png" alt="BLOG-2518 2" class="kg-image" width="1683" height="966" loading="lazy">
	</figure>
	<p><sup><i>Workers KV read latency (ms) by percentile measured from Pages</i></sup></p>
	<h2>Optimizing Workers KV’s architecture to minimize latency</h2>
	<p>At a high level, Workers KV is itself a Worker that makes requests to central storage backends, with many layers in between to properly cache and route requests across Cloudflare’s network. You can rely on Workers KV to support operations made by your Workers at any scale, and KV’s architecture will seamlessly handle your required throughput.&nbsp;</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/3pcw5pO2eeGJ1RriESJFSB/00e5a7627904a522b8fe7098f0de0945/BLOG-2518_3.png" alt="BLOG-2518 3" class="kg-image" width="732" height="437" loading="lazy">
	</figure>
	<p><sup><i>Sequence diagram of a Workers KV operation</i></sup></p>
	<p>When your Worker makes a read operation to Workers KV, your Worker establishes a network connection within its Cloudflare region to KV’s Worker. The KV Worker then accesses the <a href="https://developers.cloudflare.com/workers/runtime-apis/cache"><u>Cache API</u></a>, and in the event of a cache miss, retrieves the value from the storage backends.&nbsp;</p>
	<p>Let’s look one level deeper at a simplified trace:&nbsp;</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/7mCpF8NRgSg70p8VNTFXu8/b3407b4385ece34d72ac957e846185b1/BLOG-2518_4.png" alt="BLOG-2518 4" class="kg-image" width="986" height="476" loading="lazy">
	</figure>
	<p><sup><i>Simplified trace of a Workers KV operation</i></sup></p>
	<p>From the top, here are the operations completed for a KV read operation from your Worker:</p>
	<ol>
		<li>
			<p>Your Worker makes a connection to Cloudflare’s network in the same data center. This incurs ~5 ms of network latency.</p>
		</li>
		<li>
			<p>Upon entering Cloudflare’s network, a service called <a href="https://blog.cloudflare.com/upgrading-one-of-the-oldest-components-in-cloudflare-software-stack"><u>Front Line (FL)</u></a> is used to process the request. This incurs ~10 ms of operational latency.</p>
		</li>
		<li>
			<p>FL proxies the request to the KV Worker. The KV Worker does a cache lookup for the key being accessed. This, once again, passes through the Front Line layer, incurring an additional ~10 ms of operational latency.</p>
		</li>
		<li>
			<p>Cache is stored in various backends within each region of Cloudflare’s network. A service built upon <a href="https://blog.cloudflare.com/pingora-open-source"><u>Pingora</u></a>, our open-sourced Rust framework for proxying HTTP requests, routes the cache lookup to the proper cache backend.</p>
		</li>
		<li>
			<p>Finally, if the cache lookup is successful, the KV read operation is resolved. Otherwise, the request reaches our storage backends, where it gets its value.</p>
		</li>
	</ol>
	<p>Looking at these flame graphs, it became apparent that a major opportunity presented itself to us: reducing the FL overhead (or eliminating it altogether) and reducing the cache misses across the Cloudflare network would reduce the latency for KV operations.</p>
	<h3>Bypassing FL layers between Workers and services to save ~20ms</h3>
	<p>A request from your Worker to KV doesn’t need to go through FL. Much of FL’s responsibility is to process and route requests from outside of Cloudflare — that’s more than is needed to handle a request from the KV binding to the KV Worker. So we skipped the Front Line altogether in both layers.</p>
	<div style="position: relative; padding-top: 44.41056910569105%;">
		<iframe src="https://customer-eq7kiuol0tk9chox.cloudflarestream.com/b6c7da4a7f6d88c1d53e45988a1b7cac/iframe?loop=true&amp;autoplay=true&amp;poster=https%3A%2F%2Fcustomer-eq7kiuol0tk9chox.cloudflarestream.com%2Fb6c7da4a7f6d88c1d53e45988a1b7cac%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D%26height%3D600" loading="lazy" style="border: none; position: absolute; top: 0; left: 0; height: 100%; width: 100%;" allow="accelerometer; gyroscope; autoplay; encrypted-media; picture-in-picture;" allowfullscreen="true"></iframe>
	</div>
	<p><sup><i>Reducing latency in a Workers KV operation by removing FL layers</i></sup></p>
	<p>To bypass the FL layer from the KV binding in your Worker, we modified the KV binding to connect directly to the KV Worker within the same Cloudflare location. Within the Workers host, we configured a C++ subpipeline to allow code from bindings to establish a direct connection with the proper routing configuration and authorization loaded.&nbsp;</p>
	<p>The KV Worker also passes through the FL layer on its way to our internal <a href="https://blog.cloudflare.com/pingora-open-source"><u>Pingora</u></a> service. In this case, we were able to use an internal Worker binding that allows Workers for Cloudflare services to bind directly to non-Worker services within Cloudflare’s network. With this fix, the KV Worker sets the proper cache control headers and establishes its connection to Pingora without leaving the network.&nbsp;</p>
	<p>Together, both of these changes reduced latency by ~20 ms for every KV operation.&nbsp;</p>
	<h3>Implementing tiered cache to minimize requests to storage backends</h3>
	<p>We also optimized KV’s architecture to reduce the amount of requests that need to reach our centralized storage backends. These storage backends are further away and incur network latency, so improving the cache hit rate in regions close to your Workers significantly improves read latency.</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1791aSxXPH1lgOIr3RQrLz/282215437513f13e65ba1a79d98dae0c/BLOG-2518_5.png" alt="BLOG-2518 5" class="kg-image" width="1999" height="1527" loading="lazy">
	</figure>
	<p><sup><i>Workers KV uses Tiered Cache to resolve operations closer to your users</i></sup></p>
	<p>To accomplish this, we used <a href="https://developers.cloudflare.com/cache/how-to/tiered-cache/#custom-tiered-cache"><u>Tiered Cache</u></a>, and implemented a cache topology that is fine-tuned to the usage patterns of KV. With a tiered cache, requests to KV’s storage backends are cached in regional tiers in addition to local (lower) tiers. With this architecture, KV operations that may be cache misses locally may be resolved regionally, which is especially significant if you have traffic across an entire region spanning multiple Cloudflare data centers.&nbsp;</p>
	<p>This significantly reduced the amount of requests that needed to hit the storage backends, with ~30% of requests resolved in tiered cache instead of storage backends.</p>
	<h2>KV’s new architecture</h2>
	<p>As a result of these optimizations, KV operations are now simplified:</p>
	<ol>
		<li>
			<p>When you read from KV in your Worker, the <a href="https://developers.cloudflare.com/kv/concepts/kv-bindings"><u>KV binding</u></a> binds directly to KV’s Worker, saving 10 ms.&nbsp;</p>
		</li>
		<li>
			<p>The KV Worker binds directly to the Tiered Cache service, saving another 10 ms.&nbsp;</p>
		</li>
		<li>
			<p>Tiered Cache is used in front of storage backends, to resolve local cache misses regionally, closer to your users.</p>
		</li>
	</ol>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/2cW0MsOH120GKUAlIUvDR4/fe1e59d69c4d234265e7fba9559e52f0/BLOG-2518_6.png" alt="BLOG-2518 6" class="kg-image" width="732" height="437" loading="lazy">
	</figure>
	<p><sup><i>Sequence diagram of KV operations with new architecture</i></sup></p>
	<p>In aggregate, these changes significantly reduced KV’s latency.

		The impact of the direct binding to cache is clearly seen in the wall time of the KV Worker, given this value measures the duration of a retrieval of a key-value pair from cache. The 90th percentile of all KV Worker invocations now resolve in less than 12 ms — before the direct binding to cache, that was 22 ms. That’s a 10 ms decrease in latency.&nbsp;</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1UmcRB0Afk8mHig2DrThsh/e8e3c77e4545b80f97390b2117921e64/BLOG-2518_7.png" alt="BLOG-2518 7" class="kg-image" width="1683" height="970" loading="lazy">
	</figure>
	<p><sup><i>Wall time (ms) within the KV Worker by percentile</i></sup></p>
	<p>These KV read operations resolve quickly because the data is cached locally in the same Cloudflare location. But what about reads that aren’t resolved locally? ~30% of these resolve regionally within the tiered cache. Reads from tiered cache are up to 100 ms faster than when resolved at central storage backends, once again contributing to making KV reads faster in aggregate.</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1Gz2IxlcNuDDRp3MhC4m40/8bb3daae68620ed1e2087d1812b7e00d/BLOG-2518_8.png" alt="BLOG-2518 8" class="kg-image" width="1747" height="966" loading="lazy">
	</figure>
	<p><sup><i>Wall time (ms) within the KV Worker for tiered cache vs. storage backends reads</i></sup></p>
	<p>These graphs demonstrate the impact of direct binding from the KV binding to cache, and tiered cache. To see the impact of the direct binding from a Worker to the KV Worker, we need to look at the latencies reported by Cloudflare products that use KV.</p>
	<p><a href="https://developers.cloudflare.com/pages"><u>Cloudflare Pages</u></a>, which serves static assets like HTML, CSS, and scripts from KV, saw load times for fetching assets improve by up to 68%. Workers asset hosting, which we also announced as part of today’s <a href="https://blog.cloudflare.com/builder-day-2024-announcements"><u>Builder Day announcements</u></a>, gets this improved performance from day 1.</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/67VzWOTRpMd9Dbc6ZM7M4M/7322595ea48f7cff0bacc1db41f49597/BLOG-2518_2.png" alt="BLOG-2518 2" class="kg-image" width="1683" height="966" loading="lazy">
	</figure>
	<p><sup><i>Workers KV read operation latency measured within Cloudflare Pages by percentile</i></sup></p>
	<p><a href="https://developers.cloudflare.com/queues"><u>Queues</u></a> and <a href="https://developers.cloudflare.com/cloudflare-one/applications"><u>Access</u></a> also saw their latencies for KV operations drop, with their KV read operations now 2-5x faster. These services rely on Workers KV data for configuration and routing data, so KV’s performance improvement directly contributes to making them faster on each request.&nbsp;</p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1Gz2IxlcNuDDRp3MhC4m40/8bb3daae68620ed1e2087d1812b7e00d/BLOG-2518_8.png" alt="BLOG-2518 8" class="kg-image" width="1747" height="966" loading="lazy">
	</figure>
	<p><sup><i>Workers KV read operation latency measured within Cloudflare Queues by percentile</i></sup></p>
	<figure class="kg-card kg-image-card">
		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1HFapaO1Gv09g9VlODrLAu/d5b27e8ae55dc1b72e77c7c9bce44f6a/BLOG-2518_10.png" alt="BLOG-2518 10" class="kg-image" width="1681" height="926" loading="lazy">
	</figure>
	<p><sup><i>Workers KV read operation latency measured within Cloudflare Access by percentile</i></sup></p>
	<p>These are just some of the direct effects that a faster KV has had on other services. Across the board, requests are resolving faster thanks to KV’s faster response times.&nbsp;</p>
	<p>And we have one more thing to make KV lightning fast.&nbsp;</p>
	<h3>Optimizing KV’s hottest keys with an in-memory cache&nbsp;</h3>
	<p>Less than 0.03% of keys account for nearly half of requests to the Workers KV service across all namespaces. These keys are read thousands of times per second, so making these faster has a disproportionate impact. Could these keys be resolved within the KV Worker without needing additional network hops?</p>
	<p>Almost all of these keys are under 100 KB. At this size, it becomes possible to use the in-memory cache of the KV Worker — a limited amount of memory within the <a href="https://developers.cloudflare.com/workers/reference/how-workers-works/#isolates"><u>main runtime process</u></a> of a Worker sandbox. And that’s exactly what we did. For the highest throughput keys across Workers KV, reads resolve without even needing to leave the Worker runtime process.</p>
	<div style="position: relative; padding-top: 39.63414634146341%;">
		<iframe src="https://customer-eq7kiuol0tk9chox.cloudflarestream.com/5f01e90f833019a5c87a098f6508f1ae/iframe?loop=true&amp;autoplay=true&amp;poster=https%3A%2F%2Fcustomer-eq7kiuol0tk9chox.cloudflarestream.com%2F5f01e90f833019a5c87a098f6508f1ae%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D%26height%3D600" loading="lazy" style="border: none; position: absolute; top: 0; left: 0; height: 100%; width: 100%;" allow="accelerometer; gyroscope; autoplay; encrypted-media; picture-in-picture;" allowfullscreen="true"></iframe>
	</div>
	<p><sup><i>Sequence diagram of KV operations with the hottest keys resolved within an in-memory cache</i></sup></p>
	<p>As a result of these changes, KV reads for these keys, which represent over 40% of Workers KV requests globally, resolve in under a millisecond. We’re actively testing these changes internally and expect to roll this out during October to speed up the hottest key-value pairs on Workers KV.</p>
	<h2>A faster KV for all</h2>
	<p>Most of these speed gains are already enabled with no additional action needed from customers. Your websites that are using KV are already responding to requests faster for your users, as are the other Cloudflare services using KV under the hood and the countless websites that depend upon them.&nbsp;</p>
	<p>And we’re not done: we’ll continue to chase performance throughout our stack to make your websites faster. That’s how we’re going to move the needle towards a faster Internet.&nbsp;</p>
	<p>To see Workers KV’s recent speed gains for your own KV namespaces, head over to your dashboard and check out the <a href="https://developers.cloudflare.com/kv/observability/metrics-analytics"><u>new KV analytics</u></a>, with latency and cache status detailed per namespace.</p>
</div>