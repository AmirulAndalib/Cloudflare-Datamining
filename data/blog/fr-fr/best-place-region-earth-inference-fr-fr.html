<div class="mb2 gray5">10 min read</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/09/unnamed-2.png" class="kg-image" alt="The best place on Region: Earth for inference" loading="lazy" width="1600" height="900"></figure>
	<p>Aujourd'hui, la plateforme Workers de Cloudflare est utilisée par plus d'un million de développeurs pour créer des applications full-stack sophistiquées qui n'auraient pas été réalisables auparavant.</p>
	<p>Bien sûr, Workers n'a pas commencé comme ça. C'était à l'origine, à l'occasion d'un jour comme aujourd'hui, l'annonce d'une <a href="https://blog.cloudflare.com/introducing-cloudflare-workers">Semaine anniversaire</a>. Certes, elle ne disposait pas de toutes les fonctionnalités qui existent aujourd'hui, mais si vous avez eu l'occasion d'essayer Workers lors de son lancement, vous avez eu le sentiment qu'elle était différente de la concurrence et qu'elle allait changer les choses. Tout d'un coup, passer de rien à une application mondiale entièrement évolutive prenait quelques <em>secondes</em>, et non des heures, des jours, des semaines ou même des mois. Cela marqua le début d'une nouvelle façon de créer des applications.</p>
	<p>Si vous avez joué avec l'IA générative au cours des derniers mois, vous avez peut-être ressenti la même chose. En interrogeant quelques amis et collègues, nous avons tous vécu des moments un peu différents, mais le sentiment général qui prévaut actuellement dans le secteur est unanime : c'est différent et cela va changer les choses.</p>
	<p>Aujourd'hui, nous sommes ravis de proposer une série d'annonces qui auront, nous le pensons, un impact similaire à celui de Workers sur l'avenir de l'informatique. Sans vouloir mentionner les plus intéressantes, en voici quelques unes :</p>
	<ul>
		<li><strong>Workers AI</strong> (anciennement connue sous le nom de Constellation), <strong>fonctionnant avec des GPU NVIDIA sur le réseau globall de Cloudflare</strong>, apportant le modèle serverless à l'IA — Payez seulement ce que vous utilisez, passez moins de temps sur l'infrastructure, et plus sur votre application.</li>
		<li><strong>Vectorize, notre base de données vectorielles</strong>, qui permet d'indexer et de stocker des vecteurs de manière simple, rapide et abordable, afin de soutenir les cas d'utilisation qui nécessitent un accès non seulement aux modèles en cours d'exécution, mais aussi aux données personnalisées.</li>
		<li><strong>AI Gateway</strong> fournit aux entreprises les outils indispensables pour <strong>assurer la mise en cache, la limitation du débit et l'observation</strong> de leurs déploiements d'IA, quel que soit l'endroit où ils s'exécutent.</li>
	</ul>
	<p>Mais ce n'est pas tout.</p>
	<p>Réaliser de grandes choses est un sport d'équipe, et nous ne voulons pas le faire seuls. Comme pour la plupart de nos activités, nous nous appuyons sur les épaules de géants. Nous sommes ravis de nous associer à certains des plus grands acteurs du secteur : <strong>NVIDIA, Microsoft, Hugging Face et Meta</strong>.</p>
	<p>Nos annonces d'aujourd'hui ne marquent que le début du voyage de Cloudflare dans l'espace de l'IA, comme Workers l'a fait il y a six ans.Nous vous encourageons à vous plonger dans chacune de nos annonces aujourd'hui (vous ne serez pas déçus !),mais nous souhaitions également prendre du recul et vous présenter notre vision plus large de l'IA et la manière dont ces annonces s'y intègrent.</p>
	<h3 id="inf%C3%A9rence-lavenir-des-charges-de-travail-dia">Inférence : l'avenir des charges de travail d'IA</h3>
	<p>L'IA comporte deux processus principaux : l'apprentissage et l'inférence.</p>
	<p>L'apprentissage d'un modèle génératif d'IA est un processus de longue haleine (parfois des mois) à forte intensité de calcul, qui aboutit à un modèle. Les charges de travail de formation sont donc mieux adaptées pour être exécutées dans des emplacements traditionnels centralisés dans le cloud. Compte tenu des récents défis liés à l'obtention d'un accès de longue durée aux GPU, qui ont conduit les entreprises à opter pour le multi-cloud, nous avons parlé de la manière dont R2 peut fournir un service essentiel qui élimine les <a href="https://www.cloudflare.com/learning/cloud/what-are-data-egress-fees" target="_blank">frais de sortie</a> pour l'accès aux données de formation à partir de n'importe quel cloud de calcul. Mais ce n'est pas ce dont nous voulons parler aujourd'hui.</p>
	<p>Alors que la formation nécessite de nombreuses ressources au départ, la tâche de calcul liée à l'IA la plus répandue est l'inférence. Si vous avez récemment posé une question à ChatGPT, généré une image ou traduit un texte, vous avez effectué une tâche d'inférence. Étant donné que l'inférence est requise à chaque invocation (plutôt qu'une seule fois), nous nous attendons à ce que l'inférence devienne la charge de travail dominante dans le domaine de l'IA.</p>
	<p>Si la formation est mieux adaptée à un cloud centralisé, quel est le meilleur endroit pour l'inférence ?</p>
	<h3 id="le-r%C3%A9seau%C2%AB-juste-ce-quil-faut-%C2%BB-pour-linf%C3%A9rence">Le réseau - « juste ce qu'il faut » pour l'inférence</h3>
	<p>La caractéristique principale de l'inférence est qu'il y a généralement un utilisateur qui attend à l'autre bout. En d'autres termes, il s'agit d'une tâche sensible à la latence.</p>
	<p>On pourrait penser que le meilleur endroit pour une tâche sensible à la latence est le périphérique. Et c'est peut-être le cas dans certains cas, mais quelques problèmes se posent. Tout d'abord, le hardware des périphériques est loin d'être aussi puissant. Durée de vie de la batterie.</p>
	<p>En revanche, vous disposez de services cloud centralisés. Contrairement aux périphériques, le hardware fonctionnant dans les sites centralisés du cloud n'est rien d'autre qu'une question de puissance. Le problème, bien sûr, c'est qu'il se trouve à des centaines de millisecondes de l'utilisateur. Et parfois, ils se trouvent même au-delà des frontières, ce qui pose son propre lot de problèmes.</p>
	<p>Les périphériques ne sont donc pas encore assez puissants, et le cloud centralisé est trop éloigné. Cela fait du réseau la pierre angulaire de l'inférence. Peu éloigné, doté d'une puissance de calcul adéquate, c'est ce qu'il vous faut.</p>
	<h3 id="le-premier-cloud-dinf%C3%A9rence-fonctionnant-sur-region-earth">Le premier cloud d'inférence, fonctionnant sur Region Earth</h3>
	<p>L'une des leçons que nous avons tirées de la création de notre plateforme de développement est que l'exécution d'applications à l'échelle du réseau permet non seulement d'optimiser les performances et l'échelle (même si c'est évidemment un avantage appréciable !), mais surtout de créer le bon niveau d'abstraction pour permettre aux développeurs d'avancer rapidement.</p>
	<h3 id="workers-ai-pour-linf%C3%A9rence-serverless">Workers AI pour l'inférence serverless</h3>
	<p>Avec l'annonce de <a href="https://blog.cloudflare.com/workers-ai">Workers AI</a>, nous apportons le premier cloud GPU serverless à son partenaire idéal — Region Earth. Pas d'expertise en apprentissage automatique, pas de recherche de GPU. Il vous suffit de choisir l'un de nos modèles et de vous lancer.</p>
	<p>Nous avons beaucoup réfléchi à la conception de Workers AI pour que le déploiement du modèle se fasse le plus facilement possible.</p>
	<p>Et si vous déployez des modèles en 2023, il y a de fortes chances que l'un d'entre eux soit un <a href="https://www.cloudflare.com/learning/ai/what-is-large-language-model" target="_blank">LLM</a>.</p>
	<h3 id="vectorize-pour-stocker-des-vecteurs">Vectorize pour... stocker des vecteurs !</h3>
	<p>Pour construire un chat bot géré par l'IA de bout en bout, vous devez également pouvoir présenter à l'utilisateur une interface utilisateur, analyser le corpus d'informations que vous souhaitez lui transmettre (par exemple votre catalogue de produits), utiliser le modèle pour le convertir en données intégrées et les stocker quelque part. Jusqu'à aujourd'hui, nous proposions les produits nécessaires pour les deux premiers, mais le dernier - le stockage des données intégrées - nécessite une solution unique : une base de données vectorielle.</p>
	<p>Tout comme lorsque nous avons annoncé Workers, nous avons peu après annoncé Workers KV — il n'y a pas grand-chose que vous puissiez faire avec le calcul, sans accès aux données. Il en va de même pour l'IA — pour créer des cas d'utilisation significatifs de l'IA, vous devez donner à l'IA un accès aux données. C'est là qu'une <a href="https://www.cloudflare.com/learning/ai/what-is-vector-database" target="_blank">base de données vectorielle</a> entre en jeu, et c'est pourquoi nous sommes heureux d'annoncer aujourd'hui Vectorize, notre propre base de données vectorielle.</p>
	<h3 id="ai-gateway-pour-la-mise-en-cache-la-limitation-du-d%C3%A9bit-et-la-visibilit%C3%A9-sur-vos-d%C3%A9ploiements-dia">AI Gateway pour la mise en cache, la limitation du débit et la visibilité sur vos déploiements d'IA</h3>
	<p>Chez Cloudflare, lorsque nous cherchons à améliorer quelque chose, la première étape consiste toujours à le mesurer - si vous ne pouvez pas le mesurer, comment pouvez-vous l'améliorer ? Lorsque nous avons entendu parler de clients qui s'efforçaient de maîtriser les coûts de déploiement de l'IA, nous avons réfléchi à la manière dont nous allions aborder la question : mesurer les coûts, puis les optimiser.</p>
	<p>Notre AI Gateway vous aide à faire les deux !</p>
	<p>Les capacités d'observation en temps réel permettent une gestion proactive, facilitant le contrôle, le débogage et l'ajustement des déploiements d'IA. Il est essentiel de l'utiliser pour mettre en cache, limiter le débit et surveiller les déploiements d'IA afin d'optimiser les performances et de gérer les coûts de manière efficace. En mettant en cache les réponses d'IA fréquemment utilisées, elle réduit la latence et renforce la fiabilité du système, tandis que la limitation du débit garantit une allocation efficace des ressources, atténuant ainsi les défis posés par la spirale des coûts de l'IA.</p>
	<h3 id="collaborer-avec-meta-pour-introduire-llama-2-dans-notre-r%C3%A9seau-global">Collaborer avec Meta pour introduire Llama 2 dans notre réseau global</h3>
	<p>Jusqu'à récemment, la seule possibilité qui existait pour avoir accès à un LLM était de faire appel à des modèles propriétaires. La formation des LLM implique un investissement sérieux, en temps, en calculs et en ressources financières, ce qui n'est pas à la portée de la plupart des développeurs. La sortie par Meta de Llama 2, un LLM publiquement accessible, a constitué un tournant majeur, en permettant aux développeurs d'exécuter et de déployer leurs propres LLM. À un détail près, bien sûr : vous devez toujours avoir accès à un GPU pour le faire.</p>
	<p>En intégrant Llama 2 au catalogue de Workers AI, nous espérons donner à tous les développeurs l'accès à un LLM, sans configuration préalable.</p>
	<p>L'existence d'un modèle en cours d'exécution n'est, bien entendu, qu'un des éléments d'une application d'IA.</p>
	<h3 id="les-d%C3%A9veloppeurs-sappuient-sur-onnx-pour-passer-en-toute-transparence-du-cloud-%C3%A0-la-p%C3%A9riph%C3%A9rie-et-au-p%C3%A9riph%C3%A9rique">Les développeurs s'appuient sur ONNX pour passer en toute transparence du cloud à la périphérie et au périphérique.</h3>
	<p>Bien que la périphérie puisse être l'emplacement optimal pour résoudre bon nombre de ces problèmes, nous nous attendons à ce que les applications continuent d'être déployées ailleurs sur le spectre des périphériques, de la périphérie et du cloud centralisé.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/09/image1-22.png" class="kg-image" alt="" loading="lazy" width="1801" height="752"></figure>
	<p>Prenons l'exemple des voitures autopilotées : lorsque vous prenez des décisions où chaque milliseconde compte, vous devez prendre ces décisions sur le périphérique. Inversement, si vous cherchez à exécuter des versions de modèles comportant des centaines de milliards de paramètres, le cloud centralisé sera mieux adapté à votre charge de travail.</p>
	<p>La question qui se pose alors est la suivante : comment naviguer aisément entre ces différents sites ?</p>
	<p>Depuis notre première version de Constellation (maintenant appelée Workers AI), une technologie qui nous a particulièrement enthousiasmés est ONNX Runtime. ONNX Runtime crée un environnement standardisé pour l'exécution des modèles, ce qui permet d'exécuter différents modèles dans différents sites.</p>
	<p>Nous avons déjà parlé de la périphérie comme d'un site idéal pour l'exécution de l'inférence elle-même, mais c'est aussi une excellente couche de routage pour aider à guider les charges de travail en douceur à travers les trois sites, en fonction du cas d'utilisation et de ce que vous cherchez à optimiser - qu'il s'agisse de latence, de précision, de coût, de conformité ou de respect de la vie privée.</p>
	<h3 id="partenariat-avec-hugging-face-pour-mettre-%C3%A0-votre-disposition-des-mod%C3%A8les-optimis%C3%A9s">Partenariat avec Hugging Face pour mettre à votre disposition des modèles optimisés</h3>
	<p>Rien ne peut évidemment aider les développeurs à aller plus vite que de les accompagner là où ils sont, c'est pourquoi nous nous <a href="https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable">associons à Hugging Face</a> pour apporter l'inférence serverless aux modèles disponibles, là où les développeurs les explorent.</p>
	<h3 id="partenariat-avec-databricks-pour-cr%C3%A9er-des-mod%C3%A8les-dia">Partenariat avec Databricks pour créer des modèles d'IA</h3>
	<p>En collaboration avec Databricks, nous allons apporter la puissance de MLflow aux scientifiques et ingénieurs des données. MLflow est une plateforme open-source qui permet de gérer le cycle de vie de l'apprentissage automatique de bout en bout, et ce partenariat permettra aux utilisateurs de déployer et de gérer plus facilement des modèles d'apprentissage automatique à grande échelle. Grâce à ce partenariat, les développeurs qui créent à partir de Workers AI de Cloudflare seront en mesure de profiter de modèles compatibles avec MLflow pour procéder facilement à des déploiements dans le réseau global de Cloudflare. Les développeurs peuvent utiliser MLflow pour regrouper, mettre en œuvre, déployer et suivre efficacement un modèle directement dans la plateforme pour développeurs serverless de Cloudflare.</p>
	<p>L'IA n'empêche pas votre DSI, votre directeur financier ou votre directeur juridique de dormir.</p>
	<p>Les choses évoluent rapidement dans le domaine de l'IA, et il est important de donner aux développeurs les outils dont ils ont besoin pour progresser, mais il est difficile d'aller vite lorsqu'il y a des considérations importantes à prendre en compte. Qu'en est-il de la conformité, des coûts, de la protection de la vie privée ?</p>
	<h3 id="lia-respectueuse-de-la-conformit%C3%A9">L'IA respectueuse de la conformité</h3>
	<p>Même si la plupart d'entre nous préfèrent ne pas y penser, l'IA et la résidence des données sont de plus en plus réglementées par les gouvernements. Les gouvernements exigeant que les données soient traitées localement ou que les données de leurs résidents soient stockées dans le pays, les entreprises doivent réfléchir à ce sujet dans le contexte de l'exécution des charges de travail d'inférence. En ce qui concerne la latence, la périphérie du réseau permet d'aller aussi loin que possible. En matière de conformité, la puissance d'un réseau qui s'étend sur 300 villes et d'une offre comme notre Data Localization Suite, c'est qu'elle permet la granularité nécessaire pour maintenir les déploiements d'IA au niveau local.</p>
	<h3 id="lia-%C3%A0-petit-prix">L'IA à petit prix</h3>
	<p>En discutant avec nombre de nos amis et collègues qui expérimentent l'IA, un sentiment semble se dégager : l'IA coûte cher.Il est facile de se laisser dépasser par les coûts avant même d'avoir mis quoi que ce soit en production ou d'en avoir tiré une quelconque valeur. L'objectif de notre plateforme d'IA est de rendre les coûts abordables, mais surtout de ne vous facturer que ce que vous utilisez. Que vous utilisiez directement Workers AI ou notre passerelle AI, nous voulons vous fournir la visibilité et les outils nécessaires pour éviter que les dépenses liées à l'IA ne vous échappent.</p>
	<h3 id="une-ia-respectueuse-de-la-vie-priv%C3%A9e">Une IA respectueuse de la vie privée</h3>
	<p>Si vous placez l'IA au cœur de vos expériences client et de vos opérations commerciales, vous voulez être sûr que toutes les données qui y transitent sont en de bonnes mains. Comme cela a toujours été le cas avec Cloudflare, nous adoptons une approche axée sur la protection de la vie privée. Nous garantissons à nos clients que nous n'utiliserons pas les données des clients passant par Cloudflare pour l'inférence en vue de former de grands modèles de langage.</p>
	<h3 id="non-mais-vraimentce-nest-quun-d%C3%A9but">Non, mais vraiment - ce n'est qu'un début !</h3>
	<p>L'IA n'en est qu'à ses balbutiements, et il y a de quoi s'attendre à une sacrée aventure ! Alors que nous continuons à découvrir les avantages de cette technologie, nous ne pouvons nous empêcher d'éprouver un sentiment d'admiration et d'émerveillement face aux possibilités infinies qui s'offrent à nous. Qu'il s'agisse de révolutionner les soins de santé ou de transformer notre façon de travailler, l'IA est prête à changer la donne d'une manière que nous n'aurions jamais imaginée. Alors, attachez vos ceintures, car l'avenir de l'IA est plus prometteur que jamais - et nous sommes impatients de voir ce qui nous attend !</p>
	<p>Ce message de clôture a peut-être été généré par l'IA, mais le sentiment est authentique - ce n'est que le début, et nous sommes impatients de voir ce que vous allez construire.</p>
</div>