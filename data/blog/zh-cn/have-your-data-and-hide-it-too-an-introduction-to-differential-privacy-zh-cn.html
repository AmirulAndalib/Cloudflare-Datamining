<div class="mb2 gray5 ">22 分钟（阅读时间）</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/12/image1-9.png" class="kg-image" alt="Have your data and hide it too: An introduction to differential privacy" loading="lazy" width="1201" height="676"></figure>
	<p>许多应用程序都依赖用户数据来提供有用的功能。例如，浏览器遥测技术可以通过收集和汇总个人数据来识别网络错误或有缺陷的网站。然而，浏览历史可能很敏感，共享这些信息会带来隐私风险。有趣的是，这些应用程序通常对单个数据点不感兴趣（例如，某一特定用户在尝试访问 Wikipedia 时是否遇到网络错误），而只关心汇总数据（例如，无法连接 Wikipedia 的用户总数）。</p>
	<p><a href="https://datatracker.ietf.org/doc/draft-ietf-ppm-dap">分布式聚合协议 (DAP)</a> 允许在不泄露任何个人数据点的情况下聚合数据。对于数据收集器对总体趋势感兴趣但无法访问敏感数据的应用程序来说，它非常有用。DAP 有很多用例，从 <a href="https://www.abetterinternet.org/post/prio-services-for-covid-en">COVID-19 暴露通知</a>到 <a href="https://hacks.mozilla.org/2023/10/built-for-privacy-partnering-to-deploy-oblivious-http-and-prio-in-firefox">Firefox 中的遥测</a>，再到 <a href="https://machinelearning.apple.com/research/scenes-differential-privacy">iOS 中的个性化相册</a>等等。Cloudflare 正在帮助标准化 DAP 及其底层原语。我们正在致力于 <a href="https://github.com/cloudflare/daphne">DAP 的开源实施</a>，且正在构建一项与当前和未来合作伙伴一起运行的服务。查看<a href="https://blog.cloudflare.com/deep-dive-privacy-preserving-measurement">这篇博文</a>，了解有关 DAP 工作原理的更多信息。</p>
	<p>DAP 在正确的方向上迈出了重要的一步，但仅靠私人聚合往往不足以保护隐私。在本篇文章中，我们将解释 DAP 的不足之处，以及如何通过添加差分隐私来改进 DAP。</p>
	<h3 id="%E9%97%AE%E9%A2%98%EF%BC%9A%E7%A7%81%E4%BA%BA%E8%81%9A%E5%90%88%E5%B9%B6%E4%B8%8D%E8%B6%B3%E5%A4%9F">问题：私人聚合并不足够</h3>
	<p>DAP 使用一种称为<em>多方计算</em>的加密技术。在高层，多方计算通过将聚合计算分配给多个服务器来提高隐私性，这样任何服务器都不会看到任何个人的数据。（<a href="https://blog.cloudflare.com/deep-dive-privacy-preserving-measurement">有关多方计算的入门知识，请参阅我们之前关于 DAP 的博客文章</a>。）乍一看，这似乎应该足以保护每个单独用户的隐私：数据收集器仅了解其<em>需要</em>的信息（即 ，聚合信息），而不是用于计算它的基础数据。不幸的是，情况往往并非如此，因为聚合信息本身有时会泄露大量私人信息。</p>
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cloudflare.com/content/images/2023/12/image9.jpg" class="kg-image" alt="" loading="lazy" width="921" height="734">
		<figcaption><em>8 英尺 11.1 英寸（2.72 米）高的罗伯特·瓦德洛 (Robert Wadlow) 摆姿势拍全家福，1939 年。图片来源：</em><a href="https://www.flickr.com/photos/paille-fr/24559019804"><em>Paille</em></a> <em>// CC BY-SA 2.0。</em></figcaption>
	</figure>
	<p>举一个简单的例子，假设有一组数字，其中所有的值都相等，那么计算这一组数字的平均值，即可得出这一组数字中每个数的值。但是，知道一些数字的总和也可以得出这一组数字中是否存在特别大或特别小的数字。例如，假设我们正在计算一群人的平均身高。如果这群人中的某个成员特别高（如上图所示），那么知道这群人中有多少人以及预期的平均身高，我们就可以推断出有关该人身高的大量信息。</p>
	<p>更普遍地说，如果发布太多有关一个数据库的精确聚合数据，则可能使得攻击者能够<a href="https://differentialprivacy.org/reconstruction-theory">重建整个数据库</a>。</p>
	<p>这种攻击在现实生活中是存在的。例如，<a href="https://www2.census.gov/about/training-workshops/2021/2021-05-07-das-presentation.pdf">针对美国人口普查的</a>去匿名化攻击已经得到了可靠的证实。<a href="https://www.cloudflare.com/learning/ai/what-is-large-language-model">大型语言模型</a>（如 ChatGPT）也很容易受到攻击；<a href="https://www.cloudflare.com/learning/ai/what-is-machine-learning">机器学习模型</a>可以看作是在训练数据集上计算的一种特殊类型的统计集合。下面是一个攻击实例，研究人员给 GPT-2 下达了一条特殊指令，提取了一个真实个体的姓名、地址和电话号码，而这个个体的数据在训练数据集中只出现过一次：</p>
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cloudflare.com/content/images/2023/12/Screenshot-2023-12-22-at-15.49.54.png" class="kg-image" alt="" loading="lazy" width="3398" height="1548">
		<figcaption><em>图 1 摘自“Extracting Training Data from Large Language Models”（从大型语言模型中提取训练数据），USENIX Security ‘21，</em><a href="https://arxiv.org/pdf/2012.07805.pdf"><em>https://arxiv.org/pdf/2012.07805.pdf</em></a><em>。</em></figcaption>
	</figure>
	<p>保护模型训练输入的一种方法是称为<em>联合学习</em>的技术，其中数据保存在最终用户设备上，模型更新由中央服务器聚合。（聚合步骤甚至可以在 DAP 中完成。）然而，即使这些系统也容易受到巧妙攻击的影响，此类攻击可以利用最终模型以及一些中间版本来重建敏感数据。</p>
	<p>我们用下图来说明这一想法，该图来自最近的一篇论文，描述了对正在进行图像分类训练的机器学习模型的攻击。在此示例中，我们从 8 个用户开始，每个用户都有一个带标签的图像（例如，一张被标记为“猫”的猫的图像）。这些起始图像被称为<em>基本事实</em>。每个用户都让他们的图像通过图像分类模型，看看它是否可以准确地标记照片中的内容。当模型失败时，它会生成模型更新——一组数据，告诉模型如何自我改进，以便下次更可靠地识别该用户的照片。</p>
	<p>所有八个用户都在本地生成自己的模型更新，然后使用联合学习来提取这些模型改进的平均值，这种方式表面上是为了避免提取任何单独的更新或照片。不过，研究人员能够利用这种方法。</p>
	<p>攻击的目标是在只能访问联合的平均更新的情况下，重建图中最下面一行的 8 个基本事实图像。攻击者从随机猜测（“初始”）开始，并逐步改进猜测，使其更新方向与真正的平均更新相似。经过足够多的迭代后，我们看到攻击者猜测（“完全泄露”）中的图像与真正的基本事实数据集中的图像接近，尽管顺序可能不同。 </p>
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cloudflare.com/content/images/2023/12/Screenshot-2023-12-22-at-8.02.00-AM.png" class="kg-image" alt="" loading="lazy" width="1868" height="686">
		<figcaption><em>图 4 来自“Deep Leakage from Gradients”，NeurIPS ‘19，</em><a href="https://arxiv.org/pdf/1906.08935.pdf"><em>https://arxiv.org/pdf/1906.08935.pdf</em></a><em>。</em></figcaption>
	</figure>
	<p>这些攻击表明私人聚合（使用 DAP 或通过其他方式）不足以保护隐私。幸运的是，有一条前进的道路：包括 <a href="https://machinelearning.apple.com/research/scenes-differential-privacy">Apple</a>、<a href="https://blog.research.google/2022/02/federated-learning-with-formal.html">Google</a>、<a href="https://www2.census.gov/library/publications/decennial/2020/census-briefs/c2020br-03.pdf">美国人口普查局</a>以及<a href="https://desfontain.es/privacy/real-world-differential-privacy.html">越来越多的行业和政府参与者</a>在内的各种组织现在都在使用<strong>差分隐私</strong>来保护他们的数据。</p>
	<h3 id="%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81">差分隐私</h3>
	<p><strong>差分隐私 (DP) 是一种统计框架，可为安全聚合系统提供额外的数据保护层</strong>。它会向聚合数据添加干扰信息，防止攻击者过多地了解任何个人的信息。粗略地说，随机添加的量与隐私参数成反比，隐私参数通常用希腊字母 𝜖（发音为“epsilon”）表示：小 𝜖 值表示隐私性更强，但结果有干扰信息，而大 𝜖 值表示隐私性较差，但更准确。通过这种方式，𝜖 严格量化聚合数据所揭示的信息量。</p>
	<p>平心而论，即使没有差分隐私，一些对敏感数据集进行计算统计的部署也已经通过施加某些限制来防止最公然的隐私侵犯。例如，DAP 有一种机制，可以防止数据收集器聚合过小输入批次。在其他情况下，当某些属性在数据集中不常出现时，可以对其进行编辑，或者限制数据点的聚合次数。不过，<strong>这种临时性的隐私保护措施很容易对数据做出无效的假设</strong>。</p>
	<p>首先，这些限制本质上是针对一些明显攻击的“补丁”，但不一定涵盖所有可能的攻击。例如，某些聚合任务对异常值特别敏感，但仍然可能泄漏特别不寻常的测量值是否是聚合集的一部分。此外，虽然简单的聚合（例如总和）更容易通过自定义规则来保护，但多维和结构化统计数据（例如（平均）神经网络更新）可能会泄漏大量信息，如上一节所示。相反，<strong>差分隐私是一种通用属性，可以保护复杂的统计数据，甚至可以抵御我们一无所知的对手。</strong></p>
	<p>其次，差分隐私的另一个好处是它可以协调跨应用程序的安全参数：隐私保证表示为 𝜖 的特定值，可以在从位数到联合学习的用例之间进行比较。𝜖 的值也可以公开传达或与 DP 专家讨论。虽然设置 𝜖 仍然是一件复杂的事情（见下文），但它至少比设置参数（例如要聚合的测量值数量）更少依赖于应用程序。事实上，<strong>差分隐私参数构成了一个额外的自由度，它将隐私与其他特定于应用程序的参数分开</strong>，从而更好地控制实用性和隐私之间的权衡（例如，可以先修复 𝜖，然后再独立决定聚合任务的批量大小）。</p>
	<p>最后，大多数人工设计的保护和匿名技术无法提供与差分隐私相同的优雅和实用属性。例如，当多组报告相互关联时，或者当相同的基础数据被多次聚合时（这在联合学习等某些应用程序中至关重要），<strong>DP 可以保证柔性降级</strong>，而临时方法或 k 匿名 (k-anonymity) 等定义在这些情况下<a href="https://www.usenix.org/conference/usenixsecurity22/presentation/cohen">可能会灾难性地失败</a>。DP 还具有其他可取的属性，例如抵御边信息的能力（例如，一些现实生活中的隐私攻击可能会利用从数据经纪人处购买的数据集）。</p>
	<p>从根本上说，差分隐私将隐私工程的猫捉老鼠游戏转变为严格的数学框架，在该框架中，隐私得到证明，而不仅仅是声称。</p>
	<h3 id="%E9%9A%90%E7%A7%81%E5%B7%A5%E7%A8%8B%E7%A7%91%E5%AD%A6%EF%BC%9A%E4%BD%BF-dap-%E5%85%B7%E6%9C%89%E5%B7%AE%E5%BC%82%E6%80%A7%E9%9A%90%E7%A7%81">隐私工程科学：使 DAP 具有差异性隐私</h3>
	<p>作为 Cloudflare 的实习生（2023 年夏季），我的任务是设计一种赋予 DAP 差分隐私的策略，同时针对我们将在本文中探讨的 Cloudflare 的一些 DAP 用例进行优化。有很多方法可以做到这一点。我们重点介绍三种技术，它们具有不同的威胁模型：</p>
	<ul>
		<li>最简单的方法是照常计算聚合，然后添加干扰信息。DAP 协议定义了一个称为“收集器”的角色，其是聚合结果的预期接收者。收集器本身可以添加干扰信息（我们可以将其称为收集器随机化或 <strong>Central DP</strong>）。当然，问题是从收集器的角度来看，聚合结果不是 DP。</li>
		<li>另一种方法称为 <strong>Local DP</strong>（或 DAP 上下文中的客户端随机化），要求每个客户端在提交报告之前添加干扰信息。这提供了强有力的隐私保证（即使所有聚合器和收集器都是恶意的，DP 仍然有效），但通常会牺牲准确性。这是因为为了实现隐私，必须在每个测量值中添加更多干扰信息。不过，<a href="https://differentialprivacy.org/privacy-doona">最近的进展</a>使得此类协议在某些情况下变得实用。</li>
		<li>DAP 涉及另一个称为“聚合器”的角色，它计算聚合结果的一部分。（将各聚合器的份额合并即可得到聚合结果。）作为中间方案，<strong>我们可以让每个聚合器向其聚合份额添加干扰信息，从而确保从收集器的角度来看聚合结果是 DP</strong>。当然，我们必须相信至少有一个聚合器是诚实的，并且会通过正确的分配增加干扰信息。</li>
	</ul>
	<p>第三种方法，我们称之为聚合器随机化，是在我实习期间大家决定研究的方法。它实施起来很简单，计算开销与基本 Central DP 大致相同，并且满足与 DAP 相同的威胁模型（稍后会详细介绍！）。聚合器随机化如下图所示。</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/12/image4-1.png" class="kg-image" alt="" loading="lazy" width="1308" height="483"></figure>
	<p>尽管我们的 DAP 聚合器随机化版本看起来很简单，但我们需要确信它提供了我们期望的隐私保证。这意味着要写下协议，并对其保证进行正式分析。</p>
	<p>有趣的是，DP 的<a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">传统定义和标准证明技术</a>并不立即适用于我们的设置。事实上，与大多数 DP 机制不同，<strong>DAP 是一种交互协议，涉及分布在互联网上的许多方</strong>（客户端、聚合器、收集器），其中一些可能是恶意的。此外，<strong>DAP 的安全性基于计算假设</strong>（我们假设如破解 AES 之类的某些加密问题成本高昂），其中考虑了可能在“合理”时间内运行的对手。DP 的标准概念考虑了具有任意运行时间的对手。</p>
	<p>幸运的是，过去已经研究过将差分隐私与多方计算相结合的其他协议，而且在<a href="https://privacytools.seas.harvard.edu/files/privacytools/files/cdp.pdf"><strong>计算差分隐私</strong></a><strong>的大框架下也有合适的定义。</strong>DP 的这一定义使我们有可能模拟一个计算受限的对手与现实世界中包含加密组件的协议进行交互。然而，我们还需要做更多的工作，才能建立一个通用框架，将 DP 机制与现有的 DAP 子程序（具有经过证明的安全保证）组合起来。</p>
	<h3 id="%E7%A4%BA%E4%BE%8B%EF%BC%9A%E5%B0%86%E7%BD%91%E7%BB%9C%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95%E8%AE%BE%E4%B8%BA%E7%A7%81%E6%9C%89">示例：将网络错误日志记录设为私有</h3>
	<p>为了使问题具体化并获得一些实验数据，我们寻找了现实生活中的 DAP 使用案例，在这些案例中，差分隐私可能有用，并且可以立即应用。考虑一种协议，它可以私下汇总客户端连接错误并报告给源站，作为<a href="https://developers.cloudflare.com/support/network/understanding-network-error-logging">网络错误日志记录 (NEL)</a> 的隐私保护替代方案。对于 DAP 而言，这是一个很好的用例，因为收集汇总统计数据（例如，特定域的 tcp.timed_out 的错误数量，或过去 24 小时内错误最多的域）是可取的，但个人报告可能会泄露有关浏览习惯的敏感信息。</p>
	<p>为简单起见，让我们重点关注域列表已知的情况，例如，跟踪单个域或一组封闭的付费客户的连接错误。对于博客文章的其余部分，您可以假设我们正在使用 DAP 部署来计算单个域的连接错误直方图（但 DAP 规范中的其他统计数据也具有相同的结构，因此适用于类似的 DP 机制 ）。真正的聚合数据可能看起来像这样：</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/12/Screenshot-2023-12-22-at-15.55.38.png" class="kg-image" alt="" loading="lazy" width="1612" height="902"></figure>
	<p>从图中我们可以看到 1000 份报告中典型的错误分布情况。实际错误类型无关紧要，因此这里只用数字表示。error_type = 8 是目前最常见的错误类型，但我们也观察到一些其他错误类型。</p>
	<p>正如我们前面所看到的，这种聚合信息仍有可能泄露敏感信息——例如，如果一种错误类型只出现一次，我们就有可能知道某个用户是否访问了某个网站。现在，我们的目标是修改 DAP，以输出一个略带干扰信息的直方图版本，这样就不会泄露单个报告的信息了。</p>
	<p><a href="https://github.com/divviup/libprio-rs">libprio-rs</a> 是 DAP 中使用的加密原语的广泛使用的 Rust 实现。为了添加 DP，我们首先<a href="https://github.com/divviup/libprio-rs/pull/607">设计</a>一个任何聚合器随机化方案都应满足的通用 API，并使用对象来表示隐私预算和干扰信息添加。然后，我们为具体的统计聚合<a href="https://github.com/divviup/libprio-rs/pull/578">实施了</a> DP API。在回顾了各种 DP 机制之后，我们选择了离散高斯 (discrete Gaussian) 和离散拉普拉斯 (discrete Laplace) 机制，因为它们有明确的保证并且适合模块化环运算。我们利用 <a href="https://github.com/opendp/opendp">OpenDP</a> 库用 Rust 编写的安全干扰信息采样器。在对 DAP 的 <a href="https://github.com/cloudflare/daphne">Daphne</a> 实现进行调整后，我们能够在网络错误日志记录数据上运行具有差分隐私的 DAP 玩具部署！</p>
	<h3 id="%E9%9A%90%E7%A7%81%E5%B7%A5%E7%A8%8B%E7%9A%84%E8%89%BA%E6%9C%AF%EF%BC%9A%E6%8E%A2%E7%B4%A2%E9%9A%90%E7%A7%81%E6%95%88%E7%94%A8%E6%9D%83%E8%A1%A1">隐私工程的艺术：探索隐私效用权衡</h3>
	<p>回想一下，差分隐私涉及一个参数 𝜖，它决定了我们的系统所能达到的隐私程度。从代码的角度来看，任何正实数都是有效的选择：越小越私人，但也越无用。那么，我们应该如何选择 𝜖 呢？</p>
	<p>到目前为止，我们已将 DP 工程视为一门科学，但选择 𝜖 在某种程度上仍是一门艺术。为了说明这一点，让我们回到 NEL 数据，计算一个域上网络错误的 DP 直方图。以下是 𝜖 不同值的干扰信息直方图：</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/12/image3-1.png" class="kg-image" alt="" loading="lazy" width="946" height="480"></figure><!--kg-card-begin: markdown-->
	<p>我们观察到，减少 𝜖 会产生包含更多干扰信息的结果，甚至在某些情况下给出负计数——尽管我们始终可以在不失去隐私的情况下截断或舍入结果。由于此示例有许多报告和一些可能的错误，因此很容易掩盖任何单个人的数据。在这里我们可以看到，𝜖 = 1 似乎是这个用例的合理选择。我们仍然可以观察到，罕见事件（例如 error_type = 15）的相对误差高于常见错误（例如 error_type = 8）的相对误差。</p>
	<!--kg-card-end: markdown-->
	<p>让我们重点观察这两个事件，看看 𝜖 如何影响准确性：</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/12/Screenshot-2023-12-22-at-15.58.14.png" class="kg-image" alt="" loading="lazy" width="1588" height="742"></figure><!--kg-card-begin: markdown-->
	<p>越来越明确的是，准确性会随着 𝜖 值的增加而增加——干扰信息较少的聚合结果更不隐私，但更加准确。这种权衡被称为隐私效用权衡。您可以注意到，权衡取决于我们测量的内容。如果我们只对 error_type = 8 感兴趣并且最多可以容忍 2% 的相对误差，那么使用 𝜖 = 0.01 就足够了。如果我们对 error_type = 15 感兴趣，那么我们需要使用 𝜖 = 0.1 才能达到相同的准确度水平。</p>
	<!--kg-card-end: markdown-->
	<p>我们还可以查看其他参数（如批量大小）如何影响固定隐私保证的准确性：</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/12/Screenshot-2023-12-22-at-15.58.38.png" class="kg-image" alt="" loading="lazy" width="1598" height="734"></figure>
	<p>直观地说，如果我们在计算聚合结果之前等待更长的时间，我们就能在相同的隐私级别下得到更准确的结果，因为如果一份数据被淹没在一大批数据中，就更容易被掩盖。如前所述，我们可以预先设置隐私级别，然后再调整聚合参数。如果我们试图针对大小为 1 的批次收集聚合数据，那么 DP 结果将接近随机，从而保护该批次中单个报告的值。在准确性特别重要的应用中，这种柔性降级（针对小批次输出无用的结果，而不是破坏隐私）可能会成为一个问题，但可以通过选择一个满足准确性和隐私性之间舒适权衡的 𝜖 来控制。<strong>如果我们愿意付出隐私代价，则始终能够获得更准确的结果。</strong></p>
	<p>这让我们回到了这个问题：对于 𝜖 而言，怎样算是一个不错的值？不幸的是，正如 Cynthia Dwork（DP 发明者之一）、Kohli 和 Mulligan <a href="https://journalprivacyconfidentiality.org/index.php/jpc/article/view/689">在 2019 年发表的论文</a>中指出的那样，<strong>专家们仍未就决定 𝜖 值的正确方法达成共识</strong>。事实上，𝜖 的某些值适用于某些部署、算法、数据集或威胁模型，但不适合其他部署、算法、数据集和威胁模型。与迫使攻击者猜测可能具有 2^128 个值的密钥的加密应用不同，𝜖 必须设置为某个不可忽略的值才能从数据中了解到有用的信息。归根结底，是否有用以及怎么样会造成隐私损害取决于应用。</p>
	<p>在我们有更好的理解之前，对于非专家来说，<strong>一个简单的方法是搜索类似应用中使用的“标准” 𝜖，并在隐私约束下最大限度地提高准确性</strong>。如果我们可以访问“<a href="https://journalprivacyconfidentiality.org/index.php/jpc/article/view/689">𝜖 注册表</a>”（针对各种用例和威胁模型的详细部署列表），那么这种方法就能奏效。美国人口普查局有一个<a href="https://csrc.nist.gov/CSRC/media/Projects/pec/documents/stppa-01-20200127-talk03-Garfinkel-diff-priv-census.pdf">内部注册表</a>，但到目前为止只发布了一些用例。NIST 在<a href="https://www.nist.gov/blogs/cybersecurity-insights/differential-privacy-future-work-open-challenges">这篇非正式博文</a>中给出了一些建议范围（tldr：0 &lt; 𝜖 &lt; 5 是强值，5 &lt; 𝜖 &lt; 20 在实践中可能已经足够）。<a href="https://desfontain.es/privacy/real-world-differential-privacy.html">这篇博文</a>列出了 Apple、Google 和其他公司部署的 𝜖。</p>
	<p>还有<a href="https://git.gnunet.org/bibliography.git/plain/docs/Choosing-%CE%B5-2011Lee.pdf">其他</a><a href="https://haeberlen.cis.upenn.edu/papers/epsilon-csf2014.pdf">策略</a>可用于设置或评估 𝜖 的<a href="https://arxiv.org/pdf/2006.07709.pdf">经验保证</a>。一种补充方法是确定您愿意接受的最大误差（例如计数上 5% 的相对误差），利用标准 DP 机制的简单属性找到 𝜖 的相应值，并检查它是否在可接受的范围内。</p>
	<h3 id="%E6%80%BB%E7%BB%93">总结</h3>
	<p>随着 DAP 等安全聚合协议越来越多地部署在现实世界的应用中，我们必须记住，安全聚合并不总是能满足用户对隐私的期望。差分隐私为这些协议增加了一层额外的保护。<strong>简而言之，安全聚合保护“</strong><em><strong>方式</strong></em><strong>”（如何根据一组报告计算聚合结果），而 DP 保护“</strong><em><strong>内容</strong></em><strong>”（我们应该发布什么样的含干扰信息聚合结果以避免泄漏过多信息）。</strong></p>
	<p>得益于越来越多的开源实现、应用研究以及针对差分隐私和安全聚合的标准化工作，现在有了一条整合 DP 和 DAP 的清晰路径，从而加强了实际测量任务的隐私保证。有趣的是，在分析过程中，我们发现 DAP 协议的某些部分可能会对某些形式的 DP 保证造成问题，例如聚合器可以访问测量次数或客户端的 IP 地址。这些发现以及对协议逻辑的更多思考，促进了 <a href="https://datatracker.ietf.org/doc/draft-wang-ppm-differential-privacy">IETF 围绕这一主题和其他主题的讨论</a>。</p>
	<p>我们还发现了许多在 DP 文献中经常被忽视的细节，如模块化运算、API 考虑、安全采样或定时攻击。总之，<strong>密码学专家和差分隐私专家之间可以就能产生实际影响的协议开展富有成效的合作</strong>。</p>
	<p>如果您对差分隐私、DAP 或 Cloudflare 的其他隐私项目感兴趣，可以考虑申请<a href="https://research.cloudflare.com/outreach/academic-programs/interns">研究团队的实习机会</a>。</p>
	<h3 id="%E8%87%B4%E8%B0%A2">致谢</h3>
	<p>我要感谢我出色的导师 Christopher Patton 在整个夏天对我进行了指导，从加密细节到 IETF 标准，我学到了很多东西，并且一路上获得了很多乐趣。感谢 Josh Brown 和 Tanya Verma 与我们的讨论，感谢 Avani Wildani 和研究团队其他成员的大力支持！</p>
</div>