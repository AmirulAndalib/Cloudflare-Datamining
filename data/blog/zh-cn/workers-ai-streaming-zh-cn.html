<div class="mb2 gray5">6 min read</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--3--2.png" class="kg-image" alt="Streaming LLMs and longer context lengths available in Workers AI" loading="lazy" width="1600" height="901"></figure>
	<p>Workers AI 是我们的无服务器 GPU 驱动的推理平台，在 Cloudflare 的全球网络之上运行。它提供了越来越多的现成模型目录，这些模型可以使用 Workers 无缝运行，并使开发人员能够在几分钟内构建强大且可扩展的 AI 应用程序。我们已经看到开发人员使用 Workers AI 构建出了惊人的作品，随着我们继续扩展平台，我们迫不及待地想看到他们的最新成果。为此，今天我们很高兴地宣布推出一些饱受期待的新功能：Workers AI 上所有<a href="https://www.cloudflare.com/learning/ai/what-is-large-language-model">大型语言模型</a> (LLM) 的流式响应、更长的背景信息和序列窗口以及全精度 <a href="https://developers.cloudflare.com/workers-ai/models/llm">Llama-2</a> 模型变体 。</p>
	<p>如果您以前使用过 ChatGPT，那么您就会熟悉响应流的好处，其中响应按令牌逐个流动。LLM 的内部工作方式是使用重复推理过程按顺序生成响应，LLM 模型的完整输出本质上是数百或数千个单独预测任务的序列。因此，生成一个令牌只需要几毫秒，而生成完整的响应则需要更长的时间，大约需要几秒钟。好消息是，我们可以在生成第一个令牌后立即开始显示响应，并追加每一个额外的令牌，直到响应完成。这将为最终用户带来更好的体验——随着文本的生成逐步显示文本，不仅能提供即时响应，还能让最终用户有时间阅读和解释文本。 </p>
	<p>截至今天，您可以对我们目录中的任何 LLM 模型使用响应流，包括非常流行的 <a href="https://developers.cloudflare.com/workers-ai/models/llm">Llama-2 模型</a>。下面是它的工作原理。</p>
	<h3 id="server-sent-events%EF%BC%9A%E6%B5%8F%E8%A7%88%E5%99%A8-api-%E4%B8%AD%E7%9A%84%E5%AE%9D%E8%97%8F">Server-sent events：浏览器 API 中的宝藏</h3>
	<p><a href="https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events">Server-sent events</a> 易于使用、易于在服务器端实施、合乎标准，并且可在许多平台上原生或作为填充程序广泛使用。Server-sent events 填补了处理来自服务器的更新流的空白，处理事件流原本所需的样板代码如今已不再需要。</p><!--kg-card-begin: html-->
	<style type="text/css">
		.tg {
			border-collapse: collapse;
			border-color: #ccc;
			border-spacing: 0;
		}

		.tg td {
			background-color: #fff;
			border-color: #ccc;
			border-style: solid;
			border-width: 1px;
			color: #333;
			font-family: Arial, sans-serif;
			font-size: 14px;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg th {
			background-color: #f0f0f0;
			border-color: #ccc;
			border-style: solid;
			border-width: 1px;
			color: #333;
			font-family: Arial, sans-serif;
			font-size: 14px;
			font-weight: normal;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg .tg-lt9p {
			background-color: #F3F3F3;
			text-align: left;
			vertical-align: top
		}

		.tg .tg-9qck {
			background-color: #F3F3F3;
			font-weight: bold;
			text-align: left;
			vertical-align: top
		}

		.tg .tg-0lax {
			text-align: left;
			vertical-align: top
		}
	</style>
	<table class="tg" width="100%">
		<thead>
			<tr>
				<th class="tg-lt9p"></th>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">易于使用</span></th>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">流传输</span></th>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">双向</span></th>
			</tr>
		</thead>
		<tbody>
			<tr>
				<td class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">fetch</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">✅</span></td>
				<td class="tg-0lax"></td>
				<td class="tg-0lax"></td>
			</tr>
			<tr>
				<td class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Server-sent events</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">✅</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">✅</span></td>
				<td class="tg-0lax"></td>
			</tr>
			<tr>
				<td class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">Websockets</span></td>
				<td class="tg-0lax"></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">✅</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">✅</span></td>
			</tr>
		</tbody>
	</table><!--kg-card-end: html--><!--kg-card-begin: markdown-->
	<center><small><sup>比较 fetch、server-sent events 和 websocket</sup></small></center><!--kg-card-end: markdown-->
	<p>要开始在WorkersAI的文本生成模型上通过 server-sentevents 使用流式传输，请在请求输入中将“stream”参数设置为true。这会将响应格式和 <code>mime-type</code> 更改为 <code>text/event-stream</code>。</p>
	<p>下面是通过 <a href="https://developers.cloudflare.com/workers-ai/get-started/rest-api">REST API</a> 使用流式传输的示例：</p>
	<pre><code class="language-bash">curl -X POST \
"https://api.cloudflare.com/client/v4/accounts/&lt;account&gt;/ai/run/@cf/meta/llama-2-7b-chat-int8" \
-H "Authorization: Bearer &lt;token&gt;" \
-H "Content-Type:application/json" \
-d '{ "prompt": "where is new york?", "stream": true }'

data: {"response":"New"}

data: {"response":" York"}

data: {"response":" is"}

data: {"response":" located"}

data: {"response":" in"}

data: {"response":" the"}

...

data: [DONE]</code></pre>
	<p>下面是使用 Worker 脚本的示例：</p>
	<pre><code class="language-TypeScript">import { Ai } from "@cloudflare/ai";
export default {
    async fetch(request, env, ctx) {
        const ai = new Ai(env.AI, { sessionOptions: { ctx: ctx } });
        const stream = await ai.run(
            "@cf/meta/llama-2-7b-chat-int8",
            { prompt: "where is new york?", stream: true  }
        );
        return new Response(stream,
            { headers: { "content-type": "text/event-stream" } }
        );
    }
}</code></pre>
	<p>如果您想在浏览器页面中使用此 Worker 的输出事件流，则客户端 JavaScript 类似于：</p>
	<pre><code class="language-typescript">const source = new EventSource("/worker-endpoint");
source.onmessage = (event) =&gt; {
    if(event.data=="[DONE]") {
        // SSE spec says the connection is restarted
        // if we don't explicitly close it
        source.close();
        return;
    }
    const data = JSON.parse(event.data);
    el.innerHTML += data.response;
}</code></pre>
	<p>您可以将此简单的代码与任何简单的 HTML 页面、使用 React 或其他 Web 框架的复杂 SPA 结合使用。</p>
	<p>这为用户创造了更具交互性的体验，现在，随着响应的增量创建，用户会看到页面更新，而不是看到一个旋转画面，等待整个响应序列生成。尝试在 <a href="https://ai.cloudflare.com">ai.cloudflare.com</a> 上进行流式传输。</p>
	<figure class="kg-card kg-image-card kg-width-wide"><img src="https://blog.cloudflare.com/content/images/2023/10/llama-streaming.gif" class="kg-image" alt="" loading="lazy" width="1518" height="610"></figure>
	<p>Workers AI 支持 <a href="https://developers.cloudflare.com/workers-ai/models/llm">Llama-2</a> 模型以及我们未来添加到目录中的任何 LLM 模型的流式文本响应。</p>
	<p>但这还不是全部。</p>
	<h3 id="%E6%9B%B4%E9%AB%98%E7%9A%84%E7%B2%BE%E5%BA%A6%E4%BB%A5%E5%8F%8A%E6%9B%B4%E9%95%BF%E7%9A%84%E8%83%8C%E6%99%AF%E4%BF%A1%E6%81%AF%E5%92%8C%E5%BA%8F%E5%88%97%E9%95%BF%E5%BA%A6">更高的精度以及更长的背景信息和序列长度</h3>
	<p>Workers AI 推出后，我们从社区听到的另一个最重要的请求是在我们的 Llama-2 模型中提供更长的问题和答案。在 LLM 术语中，这意味着更高的背景信息长度（模型在进行预测之前作为输入的令牌数量）和更高的序列长度（模型在响应中生成的令牌数量）。</p>
	<p>我们听取了建议，并结合流式传输，今天我们在目录中添加了更高的 16 位全精度 Llama-2 变体，并增加了现有 8 位版本的背景信息和序列长度。</p><!--kg-card-begin: html-->
	<style type="text/css">
		.tg {
			border-collapse: collapse;
			border-spacing: 0;
		}

		.tg td {
			border-color: black;
			border-style: solid;
			border-width: 1px;
			font-family: Arial, sans-serif;
			font-size: 14px;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg th {
			border-color: black;
			border-style: solid;
			border-width: 1px;
			font-family: Arial, sans-serif;
			font-size: 14px;
			font-weight: normal;
			overflow: hidden;
			padding: 10px 5px;
			word-break: normal;
		}

		.tg .tg-9qck {
			background-color: #F3F3F3;
			font-weight: bold;
			text-align: left;
			vertical-align: top
		}

		.tg .tg-0lax {
			text-align: left;
			vertical-align: top
		}
	</style>
	<table class="tg" width="100%">
		<thead>
			<tr>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">模型</span></th>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">背景信息长度（输入）</span></th>
				<th class="tg-9qck"><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">序列长度（输出）</span></th>
			</tr>
		</thead>
		<tbody>
			<tr>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">@cf/meta/llama-2-7b-chat-int8</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">2048（之前为 768）</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">1800（之前为 256）</span></td>
			</tr>
			<tr>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">@cf/meta/llama-2-7b-chat-fp16</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">3072</span></td>
				<td class="tg-0lax"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">2500</span></td>
			</tr>
		</tbody>
	</table><!--kg-card-end: html-->
	<p>流式传输、更高的精度以及更长的背景信息和序列长度可提供更好的用户体验，并使用 Workers AI 中的大型语言模型启用新的、更丰富的应用程序。</p>
	<p>查看 Workers AI <a href="https://developers.cloudflare.com/workers-ai">开发人员文档</a>以获取更多信息和选项。如果您对 Workers AI 有任何疑问或反馈，请在 <a href="https://community.cloudflare.com">Cloudflare 社区</a>和 <a href="https://discord.gg/cloudflaredev">Cloudflare Discord</a> 中与我们联系。<br>如果您对机器学习和无服务器 AI 感兴趣，Cloudflare Workers AI 团队正在构建一个全球规模的平台和工具，让我们的客户能够在我们的网络之上运行快速、低延迟的推理任务。请查看我们的<a href="https://www.cloudflare.com/careers/jobs">职位页面</a>，了解工作机会。</p>
</div>