<div class="post-content lh-copy gray1">
	<!--kg-card-begin: markdown-->
	<p>Recently I was contacted by Dr. Igor Kozin from <a href="http://www.icr.ac.uk/" target="_blank">The Institute of Cancer Research</a> in London. He asked about the optimal way to compile CloudFlare's open source fork of <a href="https://github.com/cloudflare/zlib" target="_blank">zlib</a>. It turns out that zlib is widely used to compress the <a href="https://en.wikipedia.org/wiki/SAMtools" target="_blank">SAM/BAM</a> files that are used for DNA sequencing. And it turns out our zlib fork is the best open source solution for that file <a href="http://www.htslib.org/benchmarks/zlib.html" target="_blank">format</a>.</p>
	<p><img src="https://blog.cloudflare.com/content/images/2015/07/2653833040_644faff824_z.jpg" alt=""><br>
		<small><a href="https://creativecommons.org/licenses/by-sa/2.0/" target="_blank">CC BY-SA 2.0</a> <a href="https://www.flickr.com/photos/shaury/2653833040/in/photolist-53vA75-53rmFB-7hFuix-bpRbbf-hY8WDp-o3PyM-7KZrSc-a8Yj5r-66zTgT-6H5EcX-83C9AP-4i7Wsj-4i3QFK-bvHJVB-4i3QLV-i49oXB-8xqd9g-buojvP-hy8GiB-5dopRY-2xdo6Y-nGM3PK-nEJrfL-nqhpNL-4gxWpM-6aV2Yg-bgKpqv-b8T29n-5iijjc-pwvN8L-aA67rU-aA3r6R-aA67qW-aA3r7c-nEJpZE-63xBT-uKbHEa-oqrPhv-hWkGhN-4i3QQt-yRRDS-sgiRUL-sxJ9os-sxJ8Ky-sgj7j9-4i3QPa-svAPTm-4i7WBY-4i3QRD-4i7Wtm" target="_blank">image</a> by <a href="https://www.flickr.com/photos/shaury/" target="_blank">Shaury Nash</a></small>
	</p>
	<p>The files used for this kind of research reach hundreds of gigabytes and every time they are compressed and decompressed with our library many important seconds are saved, bringing the cure for cancer that much closer. At least that's what I am going to tell myself when I go to bed.</p>
	<p>This made me realize that the benefits of open source go much farther than one can imagine, and you never know where a piece of code may end up. Open sourcing makes sophisticated algorithms and software accessible to individuals and organizations that would not have the resources to develop them on their own, or the money pay for a proprietary solution.</p>
	<p>It also made me wonder exactly what we did to zlib that makes it stand out from other zlib forks.</p>
	<h3 id="recap">Recap</h3>
	<p>Zlib is a compression library that supports two formats: deflate and gzip. Both formats use the same algorithm also called <a href="https://en.wikipedia.org/wiki/DEFLATE" target="_blank">DEFLATE</a>, but with different headers and checksum functions. The deflate algorithm is described <a href="https://blog.cloudflare.com/improving-compression-with-preset-deflate-dictionary/">here</a>.</p>
	<p>Both formats are supported by the absolute majority of web browsers, and we at CloudFlare compress all text content on the fly using the gzip format. Moreover DEFLATE is also used by the PNG file format, and our fork of zlib also accelerates our image optimization engine <a href="https://blog.cloudflare.com/introducing-polish-automatic-image-optimizati/">Polish</a>. You can find the optimized fork of pngcrush <a href="https://github.com/cloudflare/pngcrush" target="_blank">here</a>.</p>
	<p>Given the amount of traffic we must handle, compression optimization really makes sense for us. Therefore we included several improvements over the default implementation.</p>
	<p>First of all it is important to understand the current state of zlib. It is a very old library, one of the oldest that is still used as is to this day. It is so old it was written in K&amp;R C. It is so old USB was not invented yet. It is so old that DOS was still a thing. It is so old (insert your favorite so old joke here). More precisely it dates back to 1995. Back to the days 16-bit computers with 64KB addressable space were still in use.</p>
	<p>Still it represents one of the best pieces of code ever written, and even modernizing it gives only modest performance boost. Which shows the great skill of its authors and the long way compilers have come since 1995.</p>
	<p>Below is a list of some of the improvements in our fork of zlib. This work was done by me, my colleague Shuxin Yang, and also includes improvements from other sources.</p>
	<ul>
		<li><code>uint64_t</code> as the standard type - the default fork used 16-bit types.</li>
		<li>Using an improved hash function - we use the <a href="https://tools.ietf.org/html/rfc3385" target="_blank">iSCSI CRC32</a> function as the hash function in our zlib. This specific function is implemented as a hardware instruction on Intel processors. It has very fast performance and better collision properties.</li>
		<li>Search for matches of at least 4 bytes, instead the 3 bytes the format suggests. This leads to fewer hash collisions, and less effort wasted on insignificant matches. It also improves the compression rate a little bit for the majority of cases (but not all).</li>
		<li>Using SIMD instructions for window rolling.</li>
		<li>Using the hardware carry-less multiplication instruction <code>PLCMULQDQ</code> for the CRC32 checksum.</li>
		<li>Optimized longest-match function. This is the most performance demanding function in the library. It is responsible for finding the (length, distance) matches in the current window.</li>
	</ul>
	<p>In addition, we have an experimental branch that implements an improved version of the linked list used in zlib. It has much better performance for compression levels 6 to 9, while retaining the same compression ratio. You can find the experimental branch <a href="https://github.com/cloudflare/zlib/tree/experimental" target="_blank">here</a>.</p>
	<h3 id="benchmarking">Benchmarking</h3>
	<p>You can find independent benchmarks of our library <a href="https://www.snellman.net/blog/archive/2015-06-05-updated-zlib-benchmarks/" target="_blank">here</a> and <a href="http://www.htslib.org/benchmarks/zlib.html" target="_blank">here</a>. In addition, I performed some in-house benchmarking, and put the results here for your convenience.</p>
	<p>All the benchmarks were performed on an i5-4278U CPU. The compression was performed from and to a ramdisk. All libraries were compiled with gcc version 4.8.4 with the compilation flags: "-O3 -march=native".</p>
	<p>I tested the performance of the <a href="https://github.com/madler/zlib" target="_blank">main zlib fork</a>, optimized implementation by <a href="https://github.com/jtkukunas/zlib" target="_blank">Intel</a>, our own <a href="https://github.com/cloudflare/zlib" target="_blank">main branch</a>, and our <a href="https://github.com/cloudflare/zlib/tree/experimental" target="_blank">experimental branch</a>.</p>
	<p>Four data sets were used for the benchmarks. The <a href="http://corpus.canterbury.ac.nz/resources/calgary.tar.gz" target="_blank">Calgary corpus</a>, the <a href="http://corpus.canterbury.ac.nz/resources/cantrbry.tar.gz" target="_blank">Canterbury corpus</a>, the <a href="http://corpus.canterbury.ac.nz/resources/large.tar.gz" target="_blank">Large Canterbury corpus</a> and the <a href="http://sun.aei.polsl.pl/~sdeor/corpus/silesia.zip" target="_blank">Silesia corpus</a>.</p>
	<p><strong>Calgary corpus</strong></p>
	<p>Performance:<br>
		<img src="https://blog.cloudflare.com/content/images/2015/07/calgary-1.png" alt=""><br>
		Compression rates:<br>
		<img src="https://blog.cloudflare.com/content/images/2015/07/calgary_c-1.png" alt=""><br>
		For this benchmark, Intel only outperforms our implementation for level 1, but at the cost of 1.39X larger files. This difference is far greater than even the difference between levels 1 and 9, and should probably be regarded as a different compression level. CloudFlare is faster on all other levels, and outperforms significantly for levels 6 to 9. The experimental implementation is even faster for those levels.
	</p>
	<p><strong>Canterbury corpus</strong></p>
	<p>Performance:<br>
		<img src="https://blog.cloudflare.com/content/images/2015/07/canterbry.png" alt=""><br>
		Compression rates:<br>
		<img src="https://blog.cloudflare.com/content/images/2015/07/canterbry_c.png" alt=""><br>
		Here we see a similar situation. Intel at level 1 gets 1.44X larger files. CloudFlare is faster for levels 2 to 9. On level 9, the experimental branch outperforms the reference implementation by 2X.
	</p>
	<p><strong>Large corpus</strong></p>
	<p>Performance:<br>
		<img src="https://blog.cloudflare.com/content/images/2015/07/large.png" alt=""><br>
		Compression rates:<br>
		<img src="https://blog.cloudflare.com/content/images/2015/07/large_c.png" alt=""><br>
		This time Intel is slightly faster for levels 5 and 6 than the CloudFlare implementation. The experimental CloudFlare implementation is faster still on level 6. The compression rate for Intel level 1 is 1.58 lower than CloudFlare. On level 9, the experimental fork is 7.5X(!) faster than reference.
	</p>
	<p><strong>Silesia corpus</strong></p>
	<p>Performance:<br>
		<img src="https://blog.cloudflare.com/content/images/2015/07/seli.png" alt=""><br>
		Compression rates:<br>
		<img src="https://blog.cloudflare.com/content/images/2015/07/seli_c-1.png" alt=""><br>
		Here again, CloudFlare is the fastest on levels 2 to 9. On level 9 the difference in speed between the experimental fork and the reference fork is 2.44X.
	</p>
	<h3 id="conclusion">Conclusion</h3>
	<p>As evident from the benchmarks, the CloudFlare implementation outperforms the competition in the vast majority of settings. We put great effort in making it as fast as possible on our servers.</p>
	<p>If you intend to use our library, you should check for yourself if it delivers the best balance of performance and compression for your dataset. As between different file format and sizes performance can vary.</p>
	<p>And if you like open source software, don't forget to give back to the community, by contributing your own code!</p>
	<!--kg-card-end: markdown-->
</div>