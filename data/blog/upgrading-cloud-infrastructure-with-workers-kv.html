<div class="mb2 gray5">5 min read</div>
<div class="post-content lh-copy gray1">
	<p><em>This is a guest post by </em><a href="https://twitter.com/bcnzer"><em>Ben Chartrand</em></a><em>, who is a Development Manager at </em><a href="https://www.gettimely.com"><em>Timely</em></a><em>. You can check out some of Ben's other Workers projects on his</em><a href="https://github.com/bcnzer"><em> GitHub</em></a><em> and his </em><a href="https://liftcodeplay.com"><em>blog</em></a><em>.</em></p>
	<p>At <a href="https://www.gettimely.com">Timely</a> we started a project to migrate our web applications from legacy Azure services to a modern PaaS offering. In theory it meant no code changes.</p>
	<p>We decided to start with our webhooks. All our endpoints can be grouped into four categories:</p>
	<ol>
		<li>Integration with internal tools i.e. HelpScout, monitoring endpoint for PagerDuty</li>
		<li>Payment confirmations</li>
		<li>Calendar integrations i.e. Google Calendar</li>
		<li>SMS confirmations</li>
	</ol>
	<p>Despite their limited number, these are vitally important. We did a lot of testing but it was clear we’d only really know if everything was working once we had production traffic. How could we migrate traffic?</p>
	<h3 id="option-1">Option 1</h3>
	<p>Change the CNAME to point to the new hosting infrastructure. This is high risk. DNS takes time to propagate so, if we needed to roll back, it would take time. We would also be shifting over everything at once.</p>
	<h3 id="option-2">Option 2</h3>
	<p>Use a traffic manager to shift a percentage of traffic using Cloudflare Load Balancing. We could start at, say, 5% traffic to the new infrastructure and, assuming everything appears to be ok, slowly increase the traffic.</p>
	<p>In our case the vast majority of our traffic goes to our calendar integration endpoints. The other endpoints were unlikely to receive traffic, especially if started with just 5% of traffic. This wasn’t the best option.</p>
	<h3 id="enter-option-3-cloudflare-workers-and-workers-kv">Enter Option 3: Cloudflare Workers and Workers KV</h3>
	<p>I remember thinking: wouldn’t it be great if we could migrate traffic one endpoint at a time? We have about 20. We could start at the low risk endpoints and progressively move our way up.</p>
	<p>We were able to write a Cloudflare Worker script that:</p>
	<ul>
		<li>Detected the path i.e. <em>/webhooks/paypal</em></li>
		<li>If the path matched one our endpoints, we checked Workers KV (Key Value storage) to see if that endpoint was enabled. This was our feature flag / setting</li>
		<li>If it was enabled and the path matched we redirected to the new infrastructure. This involved changing the domain but otherwise keeping the request as-is i.e. <strong>webhooks.currentdomain.com/webhooks/paypal</strong> to <strong>webhooks.newinfrastructure.com/webhooks/paypal</strong></li>
	</ul>
	<p>The first step was to add <code>passThroughOnException</code> mentioned in <a href="https://blog.cloudflare.com/dogfooding-edge-workers">this post</a>.</p>
	<pre><code>addEventListener('fetch', event =&gt; {
 event.passThroughOnException()
 event.respondWith(handleRequest(event))
})</code></pre>
	<p>Next, in the handleRequest method, I created a map of each endpoint (the path) and the corresponding Workers KV key, so I know where to look for the setting.</p>
	<pre><code>const endpoints = new Map()
   endpoints.set('/monitoring', 'monitoring')
   endpoints.set('/paypal', 'payPalIpnWebHook')
   // more endpoints
   endpoints.set('/helpscout', 'helpScoutWebHook')</code></pre>
	<p>Next I inspect the path for each request. If the path matches then we check the setting. If so, we set a redirect flag.</p>
	<pre><code>   for (var [key, value] of endpoints.entries()) {
     if (currentUrl.pathname.startsWith(key)) {
       const flag = await WEBHOOK_SETTINGS.get(value)
       if (flag == 1) {
         console.log(`redirected: ${key}`)
         redirect = true
         break
       }
     }
   }</code></pre>
	<p>If the redirect flag is true we change the hostname in the request but leave everything else as-is. This involves creating a new Request object. If we are not redirecting we fetch the request.</p>
	<pre><code>   // Handle the request
   let response = null
   if (redirect) {
     // Redirect to the new infra
     const newUrl = request.url.replace(currentHost, newHost)
     const init = {
         method: request.method,
         headers: request.headers,
         body: request.body
     }
     console.log(newUrl)
     const redirectedRequest = new Request(newUrl, init)
     console.log(redirectedRequest)

     response = await fetch(redirectedRequest)
   } else {
     // Handle with the existing infra
     response = await fetch(request)
   }
</code></pre>
	<h3 id="complete-code">Complete Code</h3>
	<pre><code>addEventListener('fetch', event =&gt; {
 event.passThroughOnException()
 event.respondWith(handleRequest(event))
})

function postLog(data) {
 return fetch("http://logs-01.loggly.com/inputs/&lt;my id&gt;/tag/http/", {
   method: "POST",
   body: data
 })
}

async function handleRequest(event) {
 try {
   const request = event.request
   const currentHost = 'webhooks.currentdomain.com'
   const newHost = 'webhooks.newinfrastructure.com'

   const currentUrl = new URL(request.url)
   let redirect = false

   // This is a map of the paths and the corresponding KV entry
   const endpoints = new Map()
   endpoints.set('/monitoring', 'monitoring')
   endpoints.set('/paypal', 'payPalIpnWebHook')
   // more endpoints
   endpoints.set('/helpscout', 'helpScoutWebHook')

   for (var [key, value] of endpoints.entries()) {
     if (currentUrl.pathname.startsWith(key)) {
       const flag = await WEBHOOK_SETTINGS.get(value)
       if (flag == 1) {
         console.log(`redirected: ${key}`)
         redirect = true
         break
       }
     }
   }

   // Handle the request
   let response = null
   if (redirect) {
     // Redirect to the new infra
     const newUrl = request.url.replace(currentHost, newHost)
     const init = {
         method: request.method,
         headers: request.headers,
         body: request.body
     }
     console.log(newUrl)
     const redirectedRequest = new Request(newUrl, init)
     console.log(redirectedRequest)

     response = await fetch(redirectedRequest)
   } else {
     // Handle with the existing infra
     response = await fetch(request)
   }

   return response
 } catch (error) {
   event.waitUntil(postLog(error))
   throw error
 }
}
</code></pre>
	<h3 id="why-use-workers-kv">Why use Workers KV?</h3>
	<p>We could have written everything as a hard coded script, which was updated each time to enable/disable redirection of traffic. This would require the team to make code changes and deploy the worker every time we wanted to make a change.</p>
	<p>Using Workers KV, I enabled any member of the team to enable/disable endpoints using the Cloudflare API. To make things easier I created a Postman collection and shared it.</p>
	<h3 id="go-live-problems-and-solutions-">Go Live Problems - and Solutions!</h3>
	<p>We went live with our first endpoint. The Workers script and KV worked fine but I noticed a small number of exceptions were being reported in Workers &gt; Worker Status.</p>
	<p></p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2019/01/Chart1.png" class="kg-image" alt="" loading="lazy"></figure>
	<p></p>
	<p>Cloudflare provides <a href="https://www.google.com/url?q=https%3A%2F%2Fdevelopers.cloudflare.com%2Fworkers%2Fwriting-workers%2Fdebugging-tips%2F&amp;sa=D&amp;ust=1547151883991000">Debugging Tips</a>. I followed the section “Make subrequests to your debug server” and decided to incorporate <a href="https://www.google.com/url?q=https%3A%2F%2Fwww.loggly.com%2F&amp;sa=D&amp;ust=1547151883991000">Loggly</a>. I could now catch the exceptions and send it to Loggly by running a POST using <code>fetch</code> to the URL provided by Loggly. With this I quickly determined what the problem was and corrected the issue.</p>
	<p>Another problem that came up was a plethora of 403s. This was highly visible in the Workers &gt; Status Code graph (the green).</p>
	<p></p>
	<p></p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2019/01/Chart2.png" class="kg-image" alt="" loading="lazy"></figure>
	<p></p>
	<p>Turns out our IIS box had <a href="https://docs.microsoft.com/en-us/iis/configuration/system.webserver/security/dynamicipsecurity/denybyrequestrate">rate limiting</a> setup. Instead of returning a 429 (Too Many Requests), it returned 403 (Forbidden). Phew - it wasn’t an issue with my Worker or the new infrastructure!</p>
	<p>We could have set up the rate limiting on the new infrastructure but we instead opted for <a href="https://www.cloudflare.com/rate-limiting">Cloudflare Rate Limiting</a>. It was cheap, easy to setup and meant the blocked requests didn’t even hit our infrastructure in the first place.</p>
	<h3 id="where-to-from-here">Where to From Here?</h3>
	<p>As I write this we’ve transitioned all traffic. All endpoints are enabled. Once we’re ready to decommission the old infrastructure we will:</p>
	<ul>
		<li>Change the CNAME to point to the new infrastructure</li>
		<li>Disable the worker</li>
		<li>Celebrate!</li>
	</ul>
	<p>We’ll then move onto our new web application, such as our API or main web app. We’re likely to use one of two options:</p>
	<ol>
		<li>Use the traffic manager to migrate a percentage of traffic</li>
		<li>Migrate traffic on a per-customer basis. It would be similar to above except we would store a setting per-customer (KV would store a setting per customer and we know the customer by the request header, which would have the customer ID). We could, for example, start with internal test accounts, then our beta users and, at the very end, migrate our VIPs.</li>
	</ol>
	<p></p>
	<p>Upgrading Cloud Infrastructure Made Easier and Safer Using Cloudflare Workers and Workers KV</p>
</div>