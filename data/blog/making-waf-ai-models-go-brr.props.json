{
	"locale": "en-us",
	"post": {
		"access": true,
		"authors": [
			{
				"id": "5d1644b141acde0011a94fbd",
				"name": "Alex Bocharov",
				"slug": "alex-bocharov",
				"profile_image": "http://blog.cloudflare.com/content/images/2018/02/27b2849.jpg",
				"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-66.png",
				"bio": null,
				"website": null,
				"location": null,
				"facebook": null,
				"twitter": null,
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/alex-bocharov/"
			}
		],
		"canonical_url": null,
		"codeinjection_foot": null,
		"codeinjection_head": null,
		"comment_id": "66a0edd358eb3b000a36905a",
		"comments": false,
		"created_at": "2024-07-24T13:04:35.000+01:00",
		"custom_excerpt": "In this post, we discuss the performance optimizations we've implemented for our WAF ML product. We'll guide you through specific code examples and benchmark numbers, and we'll share the impressive latency reduction numbers observed after the rollout",
		"custom_template": null,
		"email_subject": null,
		"excerpt": "In this post, we discuss the performance optimizations we've implemented for our WAF ML product. We'll guide you through specific code examples and benchmark numbers, and we'll share the impressive latency reduction numbers observed after the rollout",
		"feature_image": "http://blog.cloudflare.com/content/images/2024/07/image1-23.png",
		"feature_image_alt": null,
		"feature_image_caption": null,
		"featured": false,
		"frontmatter": null,
		"html": "<figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2024/07/image1-22.png\" class=\"kg-image\" alt=\"Making WAF ML models go brrr: saving decades of processing time\" loading=\"lazy\" width=\"1600\" height=\"901\"></figure><p>We made our WAF Machine Learning models <strong>5.5x</strong> faster, reducing execution time by approximately <strong>82%</strong>, from <strong>1519</strong> to <strong>275 </strong>microseconds! Read on to find out how we achieved this remarkable improvement.</p><p><a href=\"https://developers.cloudflare.com/waf/about/waf-attack-score/\">WAF Attack Score</a> is Cloudflare's machine learning (ML)-powered layer built on top of our <a href=\"https://developers.cloudflare.com/waf/\">Web Application Firewall (WAF)</a>. Its goal is to complement the WAF and detect attack bypasses that we haven't encountered before. This has proven invaluable in <a href=\"http://blog.cloudflare.com/detecting-zero-days-before-zero-day\">catching zero-day vulnerabilities</a>, like the one detected in <a href=\"http://blog.cloudflare.com/how-cloudflares-ai-waf-proactively-detected-ivanti-connect-secure-critical-zero-day-vulnerability\">Ivanti Connect Secure</a>, before they are publicly disclosed and enhancing our customers' protection against emerging and unknown threats.</p><p>Since its <a href=\"http://blog.cloudflare.com/waf-ml\">launch in 2022</a>, WAF attack score adoption has grown exponentially, now protecting millions of Internet properties and running real-time inference on tens of millions of requests per second. The feature's popularity has driven us to seek performance improvements, enabling even broader customer use and enhancing Internet security.</p><p>In this post, we will discuss the performance optimizations we've implemented for our WAF ML product. We'll guide you through specific code examples and benchmark numbers, demonstrating how these enhancements have significantly improved our system's efficiency. Additionally, we'll share the impressive latency reduction numbers observed after the rollout.</p><p>Before diving into the optimizations, let's take a moment to review the inner workings of the WAF Attack Score, which powers our WAF ML product.</p><h2 id=\"waf-attack-score-system-design\">WAF Attack Score system design</h2><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2024/07/unnamed--4--6.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1600\" height=\"343\"></figure><p>Cloudflare's WAF attack score identifies various traffic types and attack vectors (<a href=\"https://www.cloudflare.com/learning/security/threats/how-to-prevent-sql-injection/\">SQLi</a>, <a href=\"https://www.cloudflare.com/learning/security/how-to-prevent-xss-attacks/\">XSS</a>, Command Injection, etc.) based on structural or statistical content properties. Here's how it works during inference:</p><ol><li><strong>HTTP Request Content</strong>: Start with raw HTTP input.</li><li><strong>Normalization &amp; Transformation</strong>: Standardize and clean the data, applying normalization, content substitutions, and de-duplication.</li><li><strong>Feature Extraction</strong>: Tokenize the transformed content to generate statistical and structural data.</li><li><strong>Machine Learning Model Inference</strong>: Analyze the extracted features with pre-trained models, mapping content representations to classes (e.g., XSS, SQLi or <a href=\"https://www.cloudflare.com/learning/security/what-is-remote-code-execution/\">RCE</a>) or scores.</li><li><strong>Classification Output in WAF</strong>: Assign a score to the input, ranging from 1 (likely malicious) to 99 (likely clean), guiding security actions.</li></ol><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2024/07/cfce15fb-ce84-4489-a05a-6872b9e502b8.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1600\" height=\"991\"></figure><p>Next, we will explore feature extraction and inference optimizations.</p><h2 id=\"feature-extraction-optimizations\">Feature extraction optimizations</h2><p>In the context of the WAF Attack Score ML model, feature extraction or pre-processing is essentially a process of tokenizing the given input and producing a float tensor of 1 x m size:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2024/07/8cc41311-3a09-4c39-b47c-9dc449760ee2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1271\" height=\"805\"></figure><p>In our initial pre-processing implementation, this is achieved via a sliding window of 3 bytes over the input with the help of Rust’s <a href=\"https://doc.rust-lang.org/std/collections/struct.HashMap.html\">std::collections::HashMap</a> to look up the tensor index for a given ngram.</p><h3 id=\"initial-benchmarks\">Initial benchmarks</h3><p>To establish performance baselines, we've set up four benchmark cases representing example inputs of various lengths, ranging from 44 to 9482 bytes. Each case exemplifies typical input sizes, including those for a request body, user agent, and URI. We run benchmarks using the <a href=\"https://bheisler.github.io/criterion.rs/book/getting_started.html\">Criterion.rs</a> statistics-driven micro-benchmarking tool:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">RUSTFLAGS=&quot;-C opt-level=3 -C target-cpu=native&quot; cargo criterion\n</code></pre>\n<!--kg-card-end: markdown--><p>Here are initial numbers for these benchmarks executed on a Linux laptop with a 13th Gen Intel® Core™ i7-13800H processor:</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-lqy6{text-align:right;vertical-align:top}\n.tg .tg-kxn2{background-color:#EFEFEF;font-weight:bold;text-align:center;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Benchmark case</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Pre-processing time, μs</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Throughput, MiB/s</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/long-body-9482</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">248.46</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">36.40</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-body-1000</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">28.19</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">33.83</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-url-44</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">1.45</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">28.94</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-ua-91</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">2.87</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">30.24</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>An important observation from these results is that pre-processing time correlates with the length of the input string, with throughput ranging from 28 MiB/s to 36 MiB/s. This suggests that considerable time is spent iterating over longer input strings. Optimizing this part of the process could significantly enhance performance. The dependency of processing time on input size highlights a key area for performance optimization. To validate this, we should examine where the processing time is spent by analyzing flamegraphs created from a 100-second profiling session visualized using <a href=\"https://www.honeycomb.io/blog/golang-observability-using-the-new-pprof-web-ui-to-debug-memory-usage\">pprof</a>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-rust\">RUSTFLAGS=&quot;-C opt-level=3 -C target-cpu=native&quot; cargo criterion -- --profile-time 100\n \ngo tool pprof -http=: target/criterion/profile/preprocessing/avg-body-1000/profile.pb\n</code></pre>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"http://blog.cloudflare.com/content/images/2024/07/unnamed--5--6.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1600\" height=\"729\"></figure><p>Looking at the pre-processing flamegraph above, it's clear that most of the time was spent on the following two operations:</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-ekg0{background-color:#EFEFEF;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-lqy6{text-align:right;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-ekg0\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Function name</span></th>\n    <th class=\"tg-ekg0\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">% Time spent</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">std::collections::hash::map::HashMap&lt;K,V,S&gt;::get</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">61.8%</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">regex::regex::bytes::Regex::replace_all</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">18.5%</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>Let's tackle the HashMap lookups first. Lookups are happening inside the <em>tensor_populate_ngrams</em> function, where input is split into windows of 3 bytes representing ngram and then lookup inside two hash maps:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">fn tensor_populate_ngrams(tensor: &amp;mut [f32], input: &amp;[u8]) {   \n   // Populate the NORM ngrams\n   let mut unknown_norm_ngrams = 0;\n   let norm_offset = 1;\n \n   for s in input.windows(3) {\n       match NORM_VOCAB.get(s) {\n           Some(pos) =&gt; {\n               tensor[*pos as usize + norm_offset] += 1.0f32;\n           }\n           None =&gt; {\n               unknown_norm_ngrams += 1;\n           }\n       };\n   }\n \n   // Populate the SIG ngrams\n   let mut unknown_sig_ngrams = 0;\n   let sig_offset = norm_offset + NORM_VOCAB.len();\n \n   let res = SIG_REGEX.replace_all(&amp;input, b&quot;#&quot;);\n \n   for s in res.windows(3) {\n       match SIG_VOCAB.get(s) {\n           Some(pos) =&gt; {\n               // adding +1 here as the first position will be the unknown_sig_ngrams\n               tensor[*pos as usize + sig_offset + 1] += 1.0f32;\n           }\n           None =&gt; {\n               unknown_sig_ngrams += 1;\n           }\n       }\n   }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>So essentially the pre-processing function performs a ton of hash map lookups, the volume of which depends on the size of the input string, e.g. 1469 lookups for the given benchmark case <em>avg-body-1000</em>.</p><h3 id=\"optimization-attempt-1-hashmap-%E2%86%92-aho-corasick\">Optimization attempt #1: HashMap → Aho-Corasick</h3><p>Rust hash maps are generally quite fast. However, when that many lookups are being performed, it's not very cache friendly.</p><p>So can we do better than hash maps, and what should we try first? The answer is the <a href=\"https://docs.rs/aho-corasick/latest/aho_corasick/\">Aho-Corasick library</a>.</p><p>This library provides multiple pattern search principally through an implementation of the <a href=\"https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm\">Aho-Corasick algorithm</a>, which builds a fast finite state machine for executing searches in linear time.</p><p>We can also tune Aho-Corasick settings based on this recommendation:</p><blockquote><em>“You might want to use </em><a href=\"https://docs.rs/aho-corasick/1.1.3/aho_corasick/struct.AhoCorasickBuilder.html#method.kind\"><em>AhoCorasickBuilder::kind</em></a><em> to set your searcher to always use </em><a href=\"https://docs.rs/aho-corasick/1.1.3/aho_corasick/enum.AhoCorasickKind.html#variant.DFA\"><em>AhoCorasickKind::DFA</em></a><em> if search speed is critical and memory usage isn’t a concern.”</em></blockquote><!--kg-card-begin: markdown--><pre><code class=\"language-python\">static ref NORM_VOCAB_AC: AhoCorasick = AhoCorasick::builder().kind(Some(AhoCorasickKind::DFA)).build(&amp;[    \n    &quot;abc&quot;,\n    &quot;def&quot;,\n    &quot;wuq&quot;,\n    &quot;ijf&quot;,\n    &quot;iru&quot;,\n    &quot;piw&quot;,\n    &quot;mjw&quot;,\n    &quot;isn&quot;,\n    &quot;od &quot;,\n    &quot;pro&quot;,\n    ...\n]).unwrap();\n</code></pre>\n<!--kg-card-end: markdown--><p>Then we use the constructed AhoCorasick dictionary to lookup ngrams using its <a href=\"https://docs.rs/aho-corasick/latest/aho_corasick/struct.AhoCorasick.html#method.find_overlapping_iter\">find_overlapping_iter</a> method:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">for mat in NORM_VOCAB_AC.find_overlapping_iter(&amp;input) {\n    tensor_input_data[mat.pattern().as_usize() + 1] += 1.0;\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>We ran benchmarks and compared them against the baseline times shown above:</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-gyuw{color:#38761D;font-weight:bold;text-align:right;vertical-align:top}\n.tg .tg-lqy6{text-align:right;vertical-align:top}\n.tg .tg-kxn2{background-color:#EFEFEF;font-weight:bold;text-align:center;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Benchmark case</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Baseline time, μs</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Aho-Corasick time, μs</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Optimization</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/long-body-9482</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">248.46</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">129.59</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-47.84% or 1.64x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-body-1000</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">28.19</span></td>\n    <td class=\"tg-lqy6\">\t<span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">16.47</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-41.56% or 1.71x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-url-44</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">1.45</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">1.01</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-30.38% or 1.44x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-ua-91</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">2.87</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">1.90</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-33.60% or 1.51x</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>That's substantially better – Aho-Corasick DFA does wonders.</p><h3 id=\"optimization-attempt-2-aho-corasick-%E2%86%92-match\">Optimization attempt #2: Aho-Corasick → match</h3><p>One would think optimization with Aho-Corasick DFA is enough and that it seems unlikely that anything else can beat it. Yet, we can throw Aho-Corasick away and simply use the Rust match statement and let the compiler do the optimization for us!</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">#[inline]\nconst fn norm_vocab_lookup(ngram: &amp;[u8; 3]) -&gt; usize {     \n    match ngram {\n        b&quot;abc&quot; =&gt; 1,\n        b&quot;def&quot; =&gt; 2,\n        b&quot;wuq&quot; =&gt; 3,\n        b&quot;ijf&quot; =&gt; 4,\n        b&quot;iru&quot; =&gt; 5,\n        b&quot;piw&quot; =&gt; 6,\n        b&quot;mjw&quot; =&gt; 7,\n        b&quot;isn&quot; =&gt; 8,\n        b&quot;od &quot; =&gt; 9,\n        b&quot;pro&quot; =&gt; 10,\n        ...\n        _ =&gt; 0,\n    }\n}```</code></pre>\n<!--kg-card-end: markdown--><p>Here's how it performs in practice, based on the assembly generated by the <a href=\"https://godbolt.org/z/dqTq5n5Y3\">Godbolt compiler explorer</a>. The corresponding assembly code efficiently implements this lookup by employing a jump table and byte-wise comparisons to determine the return value based on input sequences, optimizing for quick decisions and minimal branching. Although the example only includes ten ngrams, it's important to note that in applications like our WAF Attack Score ML models, we deal with thousands of ngrams. This simple match-based approach outshines both HashMap lookups and the Aho-Corasick method.</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-gyuw{color:#38761D;font-weight:bold;text-align:right;vertical-align:top}\n.tg .tg-lqy6{text-align:right;vertical-align:top}\n.tg .tg-kxn2{background-color:#EFEFEF;font-weight:bold;text-align:center;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Benchmark case</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Baseline time, μs</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Match time, μs</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Optimization</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/long-body-9482</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">248.46</span></td>\n    <td class=\"tg-lqy6\">\t<span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">112.96</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-54.54% or 2.20x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-body-1000</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">28.19</span></td>\n    <td class=\"tg-lqy6\">\t<span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">13.12</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-53.45% or 2.15x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-url-44</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">1.45</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">0.75</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-48.37% or 1.94x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-ua-91</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">2.87</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">1.4076</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-50.91% or 2.04x</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>Switching to match gave us another 7-18% drop in latency, depending on the case.</p><h3 id=\"optimization-attempt-3-regex-%E2%86%92-windowedreplacer\">Optimization attempt #3: Regex → WindowedReplacer</h3><p>So, what exactly is the purpose of <em>Regex::replace_all</em> in pre-processing? Regex is defined and used like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-regex\">pub static SIG_REGEX: Lazy&lt;Regex&gt; =\n    Lazy::new(|| RegexBuilder::new(&quot;[a-z]+&quot;).unicode(false).build().unwrap());\n    ... \n    let res = SIG_REGEX.replace_all(&amp;input, b&quot;#&quot;);\n    for s in res.windows(3) {\n        tensor[sig_vocab_lookup(s.try_into().unwrap())] += 1.0;\n    }\n</code></pre>\n<!--kg-card-end: markdown--><p>Essentially, all we need is to:</p><ol><li>Replace every sequence of lowercase letters in the input with a single byte \"#\".</li><li>Iterate over replaced bytes in a windowed fashion with a step of 3 bytes representing an ngram.</li><li>Look up the ngram index and increment it in the tensor.</li></ol><p>This logic seems simple enough that we could implement it more efficiently with a single pass over the input and without any allocations:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">type Window = [u8; 3];\ntype Iter&lt;'a&gt; = Peekable&lt;std::slice::Iter&lt;'a, u8&gt;&gt;;\n\npub struct WindowedReplacer&lt;'a&gt; {\n    window: Window,\n    input_iter: Iter&lt;'a&gt;,\n}\n\n#[inline]\nfn is_replaceable(byte: u8) -&gt; bool {\n    matches!(byte, b'a'..=b'z')\n}\n\n#[inline]\nfn next_byte(iter: &amp;mut Iter) -&gt; Option&lt;u8&gt; {\n    let byte = iter.next().copied()?;\n    if is_replaceable(byte) {\n        while iter.next_if(|b| is_replaceable(**b)).is_some() {}\n        Some(b'#')\n    } else {\n        Some(byte)\n    }\n}\n\nimpl&lt;'a&gt; WindowedReplacer&lt;'a&gt; {\n    pub fn new(input: &amp;'a [u8]) -&gt; Option&lt;Self&gt; {\n        let mut window: Window = Default::default();\n        let mut iter = input.iter().peekable();\n        for byte in window.iter_mut().skip(1) {\n            *byte = next_byte(&amp;mut iter)?;\n        }\n        Some(WindowedReplacer {\n            window,\n            input_iter: iter,\n        })\n    }\n}\n\nimpl&lt;'a&gt; Iterator for WindowedReplacer&lt;'a&gt; {\n    type Item = Window;\n\n    #[inline]\n    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {\n        for i in 0..2 {\n            self.window[i] = self.window[i + 1];\n        }\n        let byte = next_byte(&amp;mut self.input_iter)?;\n        self.window[2] = byte;\n        Some(self.window)\n    }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>By utilizing the <em>WindowedReplacer</em>, we simplify the replacement logic:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">if let Some(replacer) = WindowedReplacer::new(&amp;input) {                \n    for ngram in replacer.windows(3) {\n        tensor[sig_vocab_lookup(ngram.try_into().unwrap())] += 1.0;\n    }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>This new approach not only eliminates the need for allocating additional buffers to store replaced content, but also leverages Rust's iterator optimizations, which the compiler can more effectively optimize. You can view an example of the assembly output for this new iterator at the provided <a href=\"https://godbolt.org/z/fjaoP7z6Y\">Godbolt link</a>.</p><p>Now let's benchmark this and compare against the original implementation:</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-gyuw{color:#38761D;font-weight:bold;text-align:right;vertical-align:top}\n.tg .tg-lqy6{text-align:right;vertical-align:top}\n.tg .tg-kxn2{background-color:#EFEFEF;font-weight:bold;text-align:center;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Benchmark case</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Baseline time, μs</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Match time, μs</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Optimization</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/long-body-9482</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">248.46</span></td>\n    <td class=\"tg-lqy6\">\t<span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">51.00</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-79.47% or 4.87x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-body-1000</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">28.19</span></td>\n    <td class=\"tg-lqy6\">\t<span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">5.53</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-80.36% or 5.09x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-url-44</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">1.45</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">0.40</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-72.11% or 3.59x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-ua-91</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">2.87</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">0.69</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-76.07% or 4.18x</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>The new letters replacement implementation has doubled the preprocessing speed compared to the previously optimized version using match statements, and it is four to five times faster than the original version!</p><h3 id=\"optimization-attempt-4-going-nuclear-with-branchless-ngram-lookups\">Optimization attempt #4: Going nuclear with branchless ngram lookups</h3><p>At this point, 4-5x improvement might seem like a lot and there is no point pursuing any further optimizations. After all, using an ngram lookup with a match statement has beaten the following methods, with benchmarks omitted for brevity:</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-ekg0{background-color:#EFEFEF;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-zb5k{color:#15C;text-align:left;text-decoration:underline;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-ekg0\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Lookup method</span></th>\n    <th class=\"tg-ekg0\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Description</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-zb5k\"><a href=\"https://doc.rust-lang.org/std/collections/struct.HashMap.html\"><span style=\"font-weight:400;font-style:normal;text-decoration:underline;color:#15C;background-color:transparent\">std::collections::HashMap</span></a></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Uses </span><a href=\"https://github.com/rust-lang/hashbrown\"><span style=\"font-weight:400;font-style:normal;text-decoration:underline;color:#15C;background-color:transparent\">Google’s SwissTable</span></a><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\"> design with SIMD lookups to scan multiple hash entries in parallel. </span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-zb5k\"><a href=\"https://docs.rs/aho-corasick/latest/aho_corasick/#\"><span style=\"font-weight:400;font-style:normal;text-decoration:underline;color:#15C;background-color:transparent\">Aho-Corasick</span></a><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\"> matcher with and without </span><a href=\"https://docs.rs/aho-corasick/latest/aho_corasick/dfa/struct.DFA.html\"><span style=\"font-weight:400;font-style:normal;text-decoration:underline;color:#15C;background-color:transparent\">DFA</span></a></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Also utilizes SIMD instructions in some cases.</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-zb5k\"><a href=\"https://crates.io/crates/phf\"><span style=\"font-weight:400;font-style:normal;text-decoration:underline;color:#15C;background-color:transparent\">phf crate</span></a><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\"> </span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">A library to generate efficient lookup tables at compile time using </span><a href=\"https://en.wikipedia.org/wiki/Perfect_hash_function\"><span style=\"font-weight:400;font-style:normal;text-decoration:underline;color:#15C;background-color:transparent\">perfect hash functions</span></a><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">.</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-zb5k\"><a href=\"https://crates.io/crates/ph\"><span style=\"font-weight:400;font-style:normal;text-decoration:underline;color:#15C;background-color:transparent\">ph crate</span></a></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Another Rust library of data structures based on perfect hashing. </span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-zb5k\"><a href=\"https://crates.io/crates/quickphf\"><span style=\"font-weight:400;font-style:normal;text-decoration:underline;color:#15C;background-color:transparent\">quickphf crate</span></a></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">A Rust crate that allows you to use static compile-time generated hash maps and hash sets using </span><a href=\"https://arxiv.org/abs/2104.10402\"><span style=\"font-weight:400;font-style:normal;text-decoration:underline;color:#15C;background-color:transparent\">PTHash perfect hash functions</span></a><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">.</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>However, if we look again at <a href=\"https://godbolt.org/z/dqTq5n5Y3\">the assembly of the norm_vocab_lookup function</a>, it is clear that the execution flow has to perform a bunch of comparisons using <em>cmp</em> instructions. This creates many branches for the CPU to handle, which can lead to branch mispredictions. Branch mispredictions occur when the CPU incorrectly guesses the path of execution, causing delays as it discards partially completed instructions and fetches the correct ones. By reducing or eliminating these branches, we can avoid these mispredictions and improve the efficiency of the lookup process. How can we get rid of those branches when there is a need to look up thousands of unique ngrams?</p><p>Since there are only 3 bytes in each ngram, we can build two lookup tables of 256 x 256 x 256 size, storing the ngram tensor index. With this naive approach, our memory requirements will be: 256 x 256 x 256 x 2 x 2 = 64 MB, which seems like a lot.</p><p>However, given that we only care about ASCII bytes 0..127, then memory requirements can be lower: 128 x 128 x 128 x 2 x 2 = 8 MB, which is better. However, we will need to check for bytes &gt;= 128, which will introduce a branch again.</p><p>So can we do better? Considering that the actual number of distinct byte values used in the ngrams is significantly less than the total possible 256 values, we can reduce memory requirements further by employing the following technique:</p><p>1. To avoid the branching caused by comparisons, we use precomputed offset lookup tables. This means instead of comparing each byte of the ngram during each lookup, we precompute the positions of each possible byte in a lookup table. This way, we replace the comparison operations with direct memory accesses, which are much faster and do not involve branching. We build an ngram bytes offsets lookup const array, storing each unique ngram byte offset position multiplied by the number of unique ngram bytes:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">const NGRAM_OFFSETS: [[u32; 256]; 3] = [\n    [\n        // offsets of first byte in ngram\n    ],\n    [\n        // offsets of second byte in ngram\n    ],\n    [\n        // offsets of third byte in ngram\n    ],\n];\n</code></pre>\n<!--kg-card-end: markdown--><p>2. Then to obtain the ngram index, we can use this simple const function:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">#[inline]\nconst fn ngram_index(ngram: [u8; 3]) -&gt; usize {\n    (NGRAM_OFFSETS[0][ngram[0] as usize]\n        + NGRAM_OFFSETS[1][ngram[1] as usize]\n        + NGRAM_OFFSETS[2][ngram[2] as usize]) as usize\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>3. To look up the tensor index based on the ngram index, we construct another const array at compile time using a list of all ngrams, where N is the number of unique ngram bytes:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">const NGRAM_TENSOR_IDX: [u16; N * N * N] = {\n    let mut arr = [0; N * N * N];\n    arr[ngram_index(*b&quot;abc&quot;)] = 1;\n    arr[ngram_index(*b&quot;def&quot;)] = 2;\n    arr[ngram_index(*b&quot;wuq&quot;)] = 3;\n    arr[ngram_index(*b&quot;ijf&quot;)] = 4;\n    arr[ngram_index(*b&quot;iru&quot;)] = 5;\n    arr[ngram_index(*b&quot;piw&quot;)] = 6;\n    arr[ngram_index(*b&quot;mjw&quot;)] = 7;\n    arr[ngram_index(*b&quot;isn&quot;)] = 8;\n    arr[ngram_index(*b&quot;od &quot;)] = 9;\n    ...\n    arr\n};\n</code></pre>\n<!--kg-card-end: markdown--><p>4. Finally, to update the tensor based on given ngram, we lookup the ngram index, then the tensor index, and then increment it with help of <a href=\"https://doc.rust-lang.org/std/primitive.slice.html#method.get_unchecked_mut\">get_unchecked_mut</a>, which avoids unnecessary (in this case) boundary checks and eliminates another source of branching:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">#[inline]\nfn update_tensor_with_ngram(tensor: &amp;mut [f32], ngram: [u8; 3]) {\n    let ngram_idx = ngram_index(ngram);\n    debug_assert!(ngram_idx &lt; NGRAM_TENSOR_IDX.len());\n    unsafe {\n        let tensor_idx = *NGRAM_TENSOR_IDX.get_unchecked(ngram_idx) as usize;\n        debug_assert!(tensor_idx &lt; tensor.len());\n        *tensor.get_unchecked_mut(tensor_idx) += 1.0;\n    }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>This logic works effectively, passes correctness tests, and most importantly, it's completely branchless! Moreover, the memory footprint of used lookup arrays is tiny – just ~500 KiB of memory – which easily fits into modern CPU L2/L3 caches, ensuring that expensive cache misses are rare and performance is optimal.</p><p>The last trick we will employ is loop unrolling for ngrams processing. By taking 6 ngrams (corresponding to 8 bytes of the input array) at a time, the compiler can unroll the second loop and auto-vectorize it, leveraging parallel execution to improve performance:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">const CHUNK_SIZE: usize = 6;\n\nlet chunks_max_offset =\n    ((input.len().saturating_sub(2)) / CHUNK_SIZE) * CHUNK_SIZE;\nfor i in (0..chunks_max_offset).step_by(CHUNK_SIZE) {\n    for ngram in input[i..i + CHUNK_SIZE + 2].windows(3) {\n        update_tensor_with_ngram(tensor, ngram.try_into().unwrap());\n    }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Tying up everything together, our final pre-processing benchmarks show the following:</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-gyuw{color:#38761D;font-weight:bold;text-align:right;vertical-align:top}\n.tg .tg-lqy6{text-align:right;vertical-align:top}\n.tg .tg-kxn2{background-color:#EFEFEF;font-weight:bold;text-align:center;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Benchmark case</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Baseline time, μs</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Branchless time, μs</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Optimization</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/long-body-9482</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">248.46</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">21.53</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-91.33% or 11.54x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-body-1000</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">28.19</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">2.33</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-91.73% or 12.09x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-url-44</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">1.45</span></td>\n    <td class=\"tg-lqy6\">\t<span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">0.26</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-82.34% or 5.66x</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">preprocessing/avg-ua-91</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">2.87</span></td>\n    <td class=\"tg-lqy6\">\t<span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">0.43</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-84.92% or 6.63x</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>The longer input is, the higher the latency drop will be due to branchless ngram lookups and loop unrolling, ranging from<strong> six to twelve times faster</strong> than baseline implementation.</p><p>After trying various optimizations, the final version of pre-processing retains optimization attempts 3 and 4, using branchless ngram lookup with offset tables and a single-pass non-allocating replacement iterator.</p><p>There are potentially more CPU cycles left on the table, and techniques like memory pre-fetching and manual SIMD intrinsics could speed this up a bit further. However, let's now switch gears into looking at inference latency a bit closer.</p><h2 id=\"model-inference-optimizations\">Model inference optimizations</h2><h3 id=\"initial-benchmarks-1\">Initial benchmarks</h3><p>Let’s have a look at original performance numbers of the WAF Attack Score ML model, which uses <a href=\"https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0\">TensorFlow Lite 2.6.0</a>: </p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-lqy6{text-align:right;vertical-align:top}\n.tg .tg-kxn2{background-color:#EFEFEF;font-weight:bold;text-align:center;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Benchmark case</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Inference time, μs</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">inference/long-body-9482</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">247.31</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">inference/avg-body-1000</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">246.31</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">inference/avg-url-44</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">246.40</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">inference/avg-ua-91</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">246.88</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>Model inference is actually independent of the original input length, as inputs are transformed into tensors of predetermined size during the pre-processing phase, which we optimized above. From now on, we will refer to a singular inference time when benchmarking our optimizations.</p><p>Digging deeper with profiler, we observed that most of the time is spent on the following operations:</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"http://blog.cloudflare.com/content/images/2024/07/unnamed--6--6.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1600\" height=\"137\"></figure><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-ekg0{background-color:#EFEFEF;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-lqy6{text-align:right;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-ekg0\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Function name</span></th>\n    <th class=\"tg-ekg0\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">% Time spent</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">tflite::tensor_utils::PortableMatrixBatchVectorMultiplyAccumulate</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">42.46%</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">tflite::tensor_utils::PortableAsymmetricQuantizeFloats</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">30.59%</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">tflite::optimized_ops::SoftmaxImpl</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">12.02%</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">tflite::reference_ops::MaximumMinimumBroadcastSlow</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">5.35%</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">tflite::ops::builtin::elementwise::LogEval</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">4.13%</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>The most expensive operation is matrix multiplication, which boils down to iteration within <a href=\"https://github.com/tensorflow/tensorflow/blob/v2.6.0/tensorflow/lite/kernels/internal/reference/portable_tensor_utils.cc#L119-L136\">three nested loops</a>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">void PortableMatrixBatchVectorMultiplyAccumulate(const float* matrix,\n                                                 int m_rows, int m_cols,\n                                                 const float* vector,\n                                                 int n_batch, float* result) {\n  float* result_in_batch = result;\n  for (int b = 0; b &lt; n_batch; b++) {\n    const float* matrix_ptr = matrix;\n    for (int r = 0; r &lt; m_rows; r++) {\n      float dot_prod = 0.0f;\n      const float* vector_in_batch = vector + b * m_cols;\n      for (int c = 0; c &lt; m_cols; c++) {\n        dot_prod += *matrix_ptr++ * *vector_in_batch++;\n      }\n      *result_in_batch += dot_prod;\n     ++result_in_batch;\n    }\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>This doesn’t look very efficient and many <a href=\"https://en.algorithmica.org/hpc/algorithms/matmul/\">blogs</a> and <a href=\"https://www.cs.utexas.edu/~flame/pubs/GotoTOMS_revision.pdf\">research papers</a> have been written on how matrix multiplication can be optimized, which basically boils down to:</p><ul><li><strong>Blocking</strong>: Divide matrices into smaller blocks that fit into the cache, improving cache reuse and reducing memory access latency.</li><li><strong>Vectorization</strong>: Use SIMD instructions to process multiple data points in parallel, enhancing efficiency with vector registers.</li><li><strong>Loop Unrolling</strong>: Reduce loop control overhead and increase parallelism by executing multiple loop iterations simultaneously.</li></ul><p>To gain a better understanding of how these techniques work, we recommend watching this video, which brilliantly depicts the process of matrix multiplication:</p><!--kg-card-begin: html--><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aMvCEEBIBto?si=tbw1XsYwHFezUX98&amp;controls=0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n<p></p><!--kg-card-end: html--><h3 id=\"tensorflow-lite-with-avx2\">Tensorflow Lite with AVX2</h3><p>TensorFlow Lite does, in fact, support SIMD matrix multiplication – we just need to enable it and re-compile the TensorFlow Lite library:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">if [[ &quot;$(uname -m)&quot; == x86_64* ]]; then\n    # On x86_64 target x86-64-v3 CPU to enable AVX2 and FMA.\n    arguments+=(&quot;--copt=-march=x86-64-v3&quot;)\nfi\n</code></pre>\n<!--kg-card-end: markdown--><p>After running profiler again using the SIMD-optimized TensorFlow Lite library:<br></p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"http://blog.cloudflare.com/content/images/2024/07/unnamed--7--5.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1600\" height=\"102\"></figure><p>Top operations as per profiler output:</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-ekg0{background-color:#EFEFEF;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-lqy6{text-align:right;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-ekg0\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Function name</span></th>\n    <th class=\"tg-ekg0\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">% Time spent</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">tflite::tensor_utils::SseMatrixBatchVectorMultiplyAccumulateImpl</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">43.01%</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">tflite::tensor_utils::NeonAsymmetricQuantizeFloats</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">22.46%</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">tflite::reference_ops::MaximumMinimumBroadcastSlow</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">7.82%</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">tflite::optimized_ops::SoftmaxImpl</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">6.61%</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">tflite::ops::builtin::elementwise::LogEval</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">4.63%</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>Matrix multiplication now uses <a href=\"https://github.com/tensorflow/tensorflow/blob/15ec568b5505727c940b651aeb2a9643b504086c/tensorflow/lite/kernels/internal/optimized/sse_tensor_utils.cc#L161-L199\">AVX2 instructions</a>, which uses blocks of 8x8 to multiply and accumulate the multiplication result.</p><p>Proportionally, matrix multiplication and quantization operations take a similar time share when compared to non-SIMD version, however in absolute numbers, it’s almost twice as fast when SIMD optimizations are enabled:</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-gyuw{color:#38761D;font-weight:bold;text-align:right;vertical-align:top}\n.tg .tg-lqy6{text-align:right;vertical-align:top}\n.tg .tg-kxn2{background-color:#EFEFEF;font-weight:bold;text-align:center;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Benchmark case</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Baseline time, μs</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">SIMD time, μs</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Optimization</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">inference/avg-body-1000</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">246.31</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">130.07</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-47.19% or 1.89x</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>Quite a nice performance boost just from a few lines of build config change!</p><h3 id=\"tensorflow-lite-with-xnnpack\">Tensorflow Lite with XNNPACK</h3><p>Tensorflow Lite comes with a useful benchmarking tool called <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark\">benchmark_model</a>, which also has a built-in profiler.</p><p>The tool can be built locally using the command:</p><!--kg-card-begin: markdown--><pre><code>bazel build -j 4 --copt=-march=native -c opt tensorflow/lite/tools/benchmark:benchmark_model\n</code></pre>\n<!--kg-card-end: markdown--><p>After building, benchmarks were run with different settings:</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-ekg0{background-color:#EFEFEF;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-ekg0\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Benchmark run</span></th>\n    <th class=\"tg-ekg0\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Inference time, μs</span></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">benchmark_model --graph=model.tflite --num_runs=100000 --use_xnnpack=false</span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">105.61</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">benchmark_model --graph=model.tflite --num_runs=100000 --use_xnnpack=true --xnnpack_force_fp16=true</span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">111.95</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">benchmark_model --graph=model.tflite --num_runs=100000 --use_xnnpack=true</span></td>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">49.05</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>Tensorflow Lite with XNNPACK enabled emerges as a leader, achieving ~50% latency reduction, when compared to the original Tensorflow Lite implementation.</p><p>More technical details about XNNPACK can be found in these blog posts:</p><ul><li><a href=\"https://blog.tensorflow.org/2022/06/Profiling-XNNPACK-with-TFLite.html\">Profiling XNNPACK with TFLite</a></li><li><a href=\"https://blog.tensorflow.org/2024/04/faster-dynamically-quantized-inference-with-xnnpack.html\">Faster Dynamically Quantized Inference with XNNPack</a></li></ul><p>Re-running benchmarks with XNNPack enabled, we get the following results:</p><!--kg-card-begin: html--><style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-gyuw{color:#38761D;font-weight:bold;text-align:right;vertical-align:top}\n.tg .tg-lqy6{text-align:right;vertical-align:top}\n.tg .tg-kxn2{background-color:#EFEFEF;font-weight:bold;text-align:center;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\" width=\"100%\"><thead>\n  <tr>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Benchmark case</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Baseline time, μs</span><br><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">TFLite 2.6.0</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">SIMD time, μs</span><br><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">TFLite 2.6.0</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">SIMD time, μs</span><br><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">TFLite 2.16.1</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">SIMD + XNNPack time, μs</span><br><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">TFLite 2.16.1</span></th>\n    <th class=\"tg-kxn2\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">Optimization</span></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">inference/avg-body-1000</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">246.31</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">130.07</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">115.17</span></td>\n    <td class=\"tg-lqy6\"><span style=\"font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent\">56.22</span></td>\n    <td class=\"tg-gyuw\"><span style=\"font-weight:700;font-style:normal;text-decoration:none;color:#38761D;background-color:transparent\">-77.17% or 4.38x</span></td>\n  </tr>\n</tbody></table><!--kg-card-end: html--><p>By upgrading TensorFlow Lite from 2.6.0 to 2.16.1 and enabling SIMD optimizations along with the XNNPack, we were able to decrease WAF ML model inference time more than <strong>four-fold</strong>, achieving a <strong>77.17%</strong> reduction.</p><h2 id=\"caching-inference-result\">Caching inference result</h2><p>While making code faster through pre-processing and inference optimizations is great, it's even better when code doesn't need to run at all. This is where caching comes in. <a href=\"https://en.wikipedia.org/wiki/Amdahl%27s_law\">Amdahl's Law</a> suggests that optimizing only parts of a program has diminishing returns. By avoiding redundant executions with caching, we can achieve significant performance gains beyond the limitations of traditional code optimization.</p><p>A simple key-value cache would quickly occupy all available memory on the server due to the high cardinality of URLs, HTTP headers, and HTTP bodies. However, because \"everything on the Internet has an L-shape\" or more specifically, follows a <a href=\"https://en.wikipedia.org/wiki/Zipf%27s_law\">Zipf's law</a> distribution, we can optimize our caching strategy.</p><p><a href=\"https://en.wikipedia.org/wiki/Zipf%27s_law\">Zipf</a>'<a href=\"https://en.wikipedia.org/wiki/Zipf%27s_law\">s law</a> states that in many natural datasets, the frequency of any item is inversely proportional to its rank in the frequency table. In other words, a few items are extremely common, while the majority are rare. By analyzing our request data, we found that URLs, HTTP headers, and even HTTP bodies follow this distribution. For example, here is the user agent header frequency distribution against its rank:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2024/07/unnamed--8--3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1277\" height=\"800\"></figure><p>By caching the top-N most frequently occurring inputs and their corresponding inference results, we can ensure that both pre-processing and inference are skipped for the majority of requests. This is where the <a href=\"https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU\">Least Recently Used (LRU)</a> cache comes in – frequently used items stay hot in the cache, while the least recently used ones are evicted.</p><p>We use <a href=\"https://github.com/openresty/lua-resty-lrucache\">lua-resty-mlcache</a> as our caching solution, allowing us to share cached inference results between different Nginx workers via a shared memory dictionary. The LRU cache effectively exploits the <a href=\"https://en.wikipedia.org/wiki/Space%E2%80%93time_tradeoff\">space-time trade-off</a>, where we trade a small amount of memory for significant CPU time savings.</p><p>This approach enables us to achieve a <strong>~70%</strong> cache hit ratio, significantly reducing latency further, as we will analyze in the final section below.</p><h2 id=\"optimization-results\">Optimization results</h2><p>The optimizations discussed in this post were rolled out in several phases to ensure system correctness and stability.</p><p>First, we enabled SIMD optimizations for TensorFlow Lite, which reduced WAF ML total execution time by approximately <strong>41.80%,</strong> decreasing from <strong>1519 </strong>➔<strong> 884 μs</strong> on average.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2024/07/unnamed--9--3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1600\" height=\"470\"></figure><p>Next, we upgraded TensorFlow Lite from version 2.6.0 to 2.16.1, enabled XNNPack, and implemented pre-processing optimizations. This further reduced WAF ML total execution time by <strong>~40.77%</strong>, bringing it down from <strong>932</strong> ➔ <strong>552 μs</strong> on average. The initial average time of 932 μs was slightly higher than the previous 884 μs due to the increased number of customers using this feature and the months that passed between changes.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2024/07/unnamed--10--3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1600\" height=\"472\"></figure><p>Lastly, we introduced LRU caching, which led to an additional reduction in WAF ML total execution time by <strong>~50.18%</strong>, from <strong>552</strong> ➔ <strong>275 μs</strong> on average.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2024/07/unnamed--11--3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1600\" height=\"482\"></figure><p>Overall, we cut WAF ML execution time by <strong>~81.90%</strong>, decreasing from <strong>1519</strong> ➔ <strong>275 μs</strong>, or <strong>5.5x </strong>faster!</p><p>To illustrate the significance of this: with Cloudflare’s average rate of 9.5 million requests per second passing through WAF ML, saving <strong>1244 microseconds</strong> per request equates to saving ~<strong>32 years</strong> of processing time every single day! That’s in addition to the savings of <strong>523 microseconds</strong> per request or <strong>65 years</strong> of processing time per day demonstrated last year in our <a href=\"http://blog.cloudflare.com/scalable-machine-learning-at-cloudflare\">Every request, every microsecond: scalable machine learning at Cloudflare</a> post about our Bot Management product.</p><h2 id=\"conclusion\">Conclusion</h2><p>We hope you enjoyed reading about how we made our WAF ML models go brrr, just as much as we enjoyed implementing these optimizations to bring scalable WAF ML to more customers on a truly global scale.</p><p>Looking ahead, we are developing even more sophisticated ML security models. These advancements aim to bring our <a href=\"https://www.cloudflare.com/application-services/products/waf/\">WAF</a> and <a href=\"https://www.cloudflare.com/application-services/products/bot-management/\">Bot Management</a> products to the next level, making them even more useful and effective for our customers.</p>",
		"id": "66a0edd358eb3b000a36905a",
		"meta_description": "In this post, we discuss the performance optimizations we've implemented for our WAF ML product. We'll guide you through specific code examples and benchmark numbers, and we'll share the impressive latency reduction numbers observed after the rollout.",
		"meta_title": null,
		"og_description": null,
		"og_image": "http://blog.cloudflare.com/content/images/2024/07/Making-WAF-ML-models-go-brrr--saving-decades-of-processing-time-OG-1.png",
		"og_title": null,
		"primary_author": {
			"id": "5d1644b141acde0011a94fbd",
			"name": "Alex Bocharov",
			"slug": "alex-bocharov",
			"profile_image": "http://blog.cloudflare.com/content/images/2018/02/27b2849.jpg",
			"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-66.png",
			"bio": null,
			"website": null,
			"location": null,
			"facebook": null,
			"twitter": null,
			"meta_title": null,
			"meta_description": null,
			"url": "http://blog.cloudflare.com/author/alex-bocharov/"
		},
		"primary_tag": null,
		"published_at": "2024-07-25T14:00:46.000+01:00",
		"reading_time": 23,
		"slug": "making-waf-ai-models-go-brr",
		"tags": [
			{
				"id": "66a0ee8c58eb3b000a369069",
				"name": "#BLOG-2447",
				"slug": "hash-blog-2447",
				"description": null,
				"feature_image": null,
				"visibility": "internal",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/404/"
			},
			{
				"id": "648ca18c3bb168000add4778",
				"name": "Machine Learning",
				"slug": "machine-learning",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/machine-learning/"
			},
			{
				"id": "5d16450341acde0011a951e6",
				"name": "WAF",
				"slug": "waf",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/waf/"
			},
			{
				"id": "5e7369969a5a0d03f5ec4b7e",
				"name": "Performance",
				"slug": "performance",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/performance/"
			},
			{
				"id": "5d16450341acde0011a95196",
				"name": "Optimization",
				"slug": "optimization",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/optimization/"
			},
			{
				"id": "5d16450341acde0011a9523c",
				"name": "Rust",
				"slug": "rust",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/rust/"
			},
			{
				"id": "65af91a8d5ea6b000a6a6600",
				"name": "AI WAF",
				"slug": "ai-waf",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/ai-waf/"
			},
			{
				"id": "6390de843833d8000a01b629",
				"name": "WAF Attack Score",
				"slug": "waf-attack-score",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/waf-attack-score/"
			}
		],
		"title": "Making WAF ML models go brrr: saving decades of processing time",
		"twitter_description": null,
		"twitter_image": "http://blog.cloudflare.com/content/images/2024/07/Making-WAF-ML-models-go-brrr--saving-decades-of-processing-time-OG.png",
		"twitter_title": null,
		"updated_at": "2024-07-25T13:59:18.000+01:00",
		"url": "http://blog.cloudflare.com/making-waf-ai-models-go-brr/",
		"uuid": "a175b943-dcfb-44f0-aab2-fe8b4c0b15e5",
		"visibility": "public"
	}
}