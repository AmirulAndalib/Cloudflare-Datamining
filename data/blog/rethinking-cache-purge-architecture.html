<div class="mb2 gray5">6 min read</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/3EU9TDz1YS47zrHosO5j0g/c66127cdbe46eb4696cdf7d695c7e7b1/image4-12.png" alt="Part 2: Rethinking cache purge with a new architecture" class="kg-image" width="1999" height="1125" loading="lazy">

	</figure>
	<p>In <a href="https://blog.cloudflare.com/part1-coreless-purge">Part 1: Rethinking Cache Purge, Fast and Scalable Global Cache Invalidation</a>, we outlined the importance of cache invalidation and the difficulties of purging caches, how our existing purge system was designed and performed, and we gave a high level overview of what we wanted our new Cache Purge system to look like.</p>
	<p>It’s been a while since we published the first blog post and it’s time for an update on what we’ve been working on. In this post we’ll be talking about some of the architecture improvements we’ve made so far and what we’re working on now.</p>
	<h2>Cache Purge end to end</h2>
	<p>We touched on the high level design of what we called the “coreless” purge system in part 1, but let’s dive deeper into what that design encompasses by following a purge request from end to end:</p>
	<figure class="kg-card kg-image-card kg-width-wide">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/2wDdgC1O4K0X6npqec7hd9/dc87b95437c93500d75fe0f6967f8a92/image3-13.png" alt="" class="kg-image" width="1999" height="905" loading="lazy">

	</figure>
	<h3>Step 1: Request received locally</h3>
	<p>An API request to Cloudflare is routed to the nearest Cloudflare data center and passed to an <b>API Gateway worker</b>. This worker looks at the request URL to see which service it should be sent to and forwards the request to the appropriate upstream backend. Most endpoints of the Cloudflare API are currently handled by centralized services, so the <b>API Gateway worker</b> is often just proxying requests to the nearest “core” data center which have their own gateway services to handle authentication, authorization, and further routing. But for endpoints which aren’t handled centrally the <b>API Gateway worker</b> must handle <a href="https://www.cloudflare.com/learning/access-management/what-is-authentication">authentication</a> and route authorization, and then proxy to an appropriate upstream. For cache purge requests that upstream is a <b>Purge Ingest worker</b> in the same data center.</p>
	<h3>Step 2: Purges tested locally</h3>
	<p>The <b>Purge Ingest worker</b> evaluates the purge request to make sure it is processible. It scans the URLs in the body of the request to see if they’re valid, then attempts to purge the URLs from the local data center’s cache. This concept of <b>local purging</b> was a new step introduced with the coreless purge system allowing us to capitalize on existing logic already used in every data center.</p>
	<p>By leveraging the same ownership checks our data centers use to serve a zone’s normal traffic on the URLs being purged, we can determine if those URLs are even cacheable by the zone. Currently <b>more than 50%</b> of the URLs we’re asked to purge can’t be cached by the requesting zones, either because they don’t own the URLs (e.g. a customer asking us to purge <a href="https://cloudflare.com">https://cloudflare.com</a>) or because the zone’s settings for the URL prevent caching (e.g. the zone has a “bypass” cache rule that matches the URL). All such purges are superfluous and shouldn’t be processed further, so we filter them out and avoid broadcasting them to other data centers freeing up resources to process more legitimate purges.</p>
	<p>On top of that, generating the cache key for a file isn’t free; we need to load zone configuration options that might affect the cache key, apply various transformations, et cetera. The cache key for a given file is the same in every data center though, so when we purge the file locally we now return the generated cache key to the <b>Purge Ingest worker</b> and broadcast that key to other data centers instead of making each data center generate it themselves.</p>
	<h3>Step 3: Purges queued for broadcasting</h3>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/KE3gyiFfEntTtr8vMYODq/dab2f42c24a74169cb3efe901521d667/image2-15.png" alt="purge request to small colo, ingest worker sends to queue worker in T1" class="kg-image" width="1999" height="1370" loading="lazy">

	</figure>
	<p>Once the local purge is done the <b>Purge Ingest worker</b> forwards the purge request with the cache key obtained from the local cache to a <b>Purge Queue worker</b>. The queue worker is a <a href="https://developers.cloudflare.com/workers/learning/using-durable-objects">Durable Object</a> worker using its persistent state to hold a queue of purges it receives and pointers to how far along in the queue each data center in our network is in processing purges.</p>
	<p>The queue is important because it allows us to automatically recover from a number of scenarios such as connectivity issues or data centers coming back online after maintenance. Having a record of all purges since an issue arose lets us replay those purges to a data center and “catch up”.</p>
	<p>But Durable Objects are globally unique, so having one manage all global purges would have just moved our centrality problem from a core data center to wherever that Durable Object was provisioned. Instead we have dozens of Durable Objects in each region, and the <b>Purge Ingest worker</b> looks at the load balancing pool of Durable Objects for its region and picks one (often in the same data center) to forward the request to. The Durable Object will write the purge request to its queue and immediately loop through all the data center pointers and attempt to push any outstanding purges to each.</p>
	<p>While benchmarking our performance we found our particular workload exhibited a “goldilocks zone” of throughput to a given Durable Object. On script startup we have to load all sorts of data like network topology and data center health–then refresh it continuously in the background–and as long as the Durable Object sees steady traffic it stays active and we amortize those startup costs. But if you ask a single Durable Object to do too much at once like send or receive too many requests, the single-threaded runtime won’t keep up. Regional purge traffic fluctuates a lot depending on local time of day, so there wasn’t a static quantity of Durable Objects per region that would let us stay within the goldilocks zone of enough requests to each to keep them active but not too many to keep them efficient. So we built load monitoring into our Durable Objects, and a <b>Regional Autoscaler worker</b> to aggregate that data and adjust load balancing pools when we start approaching the upper or lower edges of our efficiency goldilocks zone.</p>
	<h3>Step 4: Purges broadcast globally</h3>
	<figure class="kg-card kg-image-card ">

		<img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/6wb88kH1WkS4aZjc8wWsk2/ac39791bf51c55f6a823b1edcade7763/image1-22.png" alt="multiple regions, durable object sends purges to fanouts in other regions, fanout sends to small colos in their region" class="kg-image" width="1999" height="1155" loading="lazy">

	</figure>
	<p>Once a purge request is queued by a <b>Purge Queue worker</b> it needs to be broadcast to the rest of Cloudflare’s data centers to be carried out by their caches. The Durable Objects will broadcast purges directly to all data centers in their region, but when broadcasting to other regions they pick a <b>Purge Fanout worker</b> per region to take care of their region’s distribution. The fanout workers manage queues of their own as well as pointers for all of their region’s data centers, and in fact they share a lot of the same logic as the <b>Purge Queue workers</b> in order to do so. One key difference is fanout workers aren’t Durable Objects; they’re normal worker scripts, and their queues are purely in memory as opposed to being backed by Durable Object state. This means not all queue worker Durable Objects are talking to the same fanout worker in each region. Fanout workers can be dropped and spun up again quickly by any metal in the data center because they aren’t canonical sources of state. They maintain queues and pointers for their region but all of that info is also sent back downstream to the Durable Objects who persist that data themselves, reliably.</p>
	<p>But what does the fanout worker get us? Cloudflare has hundreds of <a href="https://www.cloudflare.com/learning/cdn/glossary/data-center">data centers</a> all over the world, and as we mentioned above we benefit from keeping the number of incoming and outgoing requests for a Durable Object fairly low. Sending purges to a fanout worker per region means each Durable Object only has to make a fraction of the requests it would if it were broadcasting to every data center directly, which means it can process purges faster.</p>
	<p>On top of that, occasionally a request will fail to get where it was going and require retransmission. When this happens between data centers in the same region it’s largely unnoticeable, but when a Durable Object in Canada has to retry a request to a data center in rural South Africa the cost of traversing that whole distance again is steep. The data centers elected to host fanout workers have the most reliable connections in their regions to the rest of our network. This minimizes the chance of inter-regional retries and limits the <a href="https://www.cloudflare.com/learning/performance/glossary/what-is-latency">latency</a> imposed by retries to regional timescales.</p>
	<p>The introduction of the <b>Purge Fanout worker</b> was a massive improvement to our distribution system, reducing our end-to-end purge latency by 50% on its own and increasing our throughput threefold.</p>
	<h2>Current status of coreless purge</h2>
	<p>We are proud to say our new purge system has been in production serving <a href="https://api.cloudflare.com/#zone-purge-files-by-url">purge by URL requests</a> since July 2022, and the results in terms of latency improvements are dramatic. In addition, <a href="https://api.cloudflare.com/#zone-purge-files-by-cache-tags,-host,-or-prefix">flexible purge requests</a> (purge by tag/prefix/host and purge everything) share and benefit from the new coreless purge system’s entrypoint workers before heading to a core data center for fulfillment.</p>
	<p>The reason flexible purge isn’t also fully coreless yet is because it’s a more complex task than “purge this object”; flexible purge requests can end up purging multiple objects–or even entire zones–from cache. They do this through an entirely different process that isn’t coreless compatible, so to make flexible purge fully coreless we would have needed to come up with an entirely new multi-purge mechanism on top of redesigning distribution. We chose instead to start with just purge by URL so we could focus purely on the most impactful improvements, revamping distribution, without reworking the logic a data center uses to actually remove an object from cache.</p>
	<p>This is not to say that the flexible purges haven’t benefited from the coreless purge project. Our cache purge API lets users bundle single file and flexible purges in one request, so the <b>API Gateway worker</b> and <b>Purge Ingest worker</b> handle authorization, authentication and payload validation for flexible purges too. Those flexible purges get forwarded directly to our services in core data centers pre-authorized and validated which reduces load on those core data center auth services. As an added benefit, because authorization and validity checks all happen at the edge for all purge types users get much faster feedback when their requests are malformed.</p>
	<h2>Next steps</h2>
	<p>While coreless cache purge has come a long way since the part 1 blog post, we’re not done. We continue to work on reducing end-to-end latency even more for purge by URL because we can do better. Alongside improvements to our new distribution system, we’ve also been working on the redesign of flexible purge to make it fully coreless, and we’re really excited to share the results we’re seeing soon. Flexible cache purge is an incredibly popular API and we’re giving its refresh the care and attention it deserves.</p>
</div>