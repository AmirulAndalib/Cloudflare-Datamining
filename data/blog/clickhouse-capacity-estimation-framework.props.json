{
	"locale": "en-us",
	"post": {
		"id": "5fa4000430cdba01bcf670ed",
		"uuid": "e2bfcbfa-e557-4f1b-bb12-a7485e4f76df",
		"title": "ClickHouse Capacity Estimation Framework",
		"slug": "clickhouse-capacity-estimation-framework",
		"html": "<p>We use ClickHouse widely at Cloudflare. It helps us with our internal analytics workload, bot management, customer dashboards, and many other systems. For instance, before <a href=\"http://blog.cloudflare.com/cloudflare-bot-management-machine-learning-and-more/\">Bot Management</a> can analyze and classify our traffic, we need to collect logs. The <a href=\"http://blog.cloudflare.com/updates-to-firewall-analytics/\">Firewall Analytics tool</a> needs to store and query data somewhere too. The same goes for our new <a href=\"http://blog.cloudflare.com/introducing-cloudflare-radar/\">Cloudflare Radar project</a>. We are using ClickHouse for this purpose. It is a big database that can store <a href=\"https://www.cloudflare.com/learning/ai/big-data/\">huge amounts of data</a> and return it on demand. This is not the first time we have talked about ClickHouse, there is a <a href=\"http://blog.cloudflare.com/http-analytics-for-6m-requests-per-second-using-clickhouse/\">dedicated blogpost</a> on how we introduced ClickHouse for HTTP analytics.</p><p>Our biggest cluster has more than 100 nodes, another one about half that number. Besides that, we have over 20 clusters that have at least three nodes and the replication factor of three. Our current insertion rate is about 90M rows per second.</p><p>We use the standard approach in ClickHouse schema design. At the top level we have clusters, which hold shards, a group of nodes, and a node is a physical machine. You can find technical characteristics of the nodes <a href=\"http://blog.cloudflare.com/http-analytics-for-6m-requests-per-second-using-clickhouse/\">here</a>. Stored data is replicated between clusters. Different shards hold different parts of the data, but inside of each shard replicas are equal.</p><p>Schema of one cluster:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/11/image1-1.jpg\" class=\"kg-image\" alt loading=\"lazy\"></figure><h3 id=\"capacity-planning\">Capacity planning</h3><p>As engineers, we periodically face the question of how many additional nodes we have to order to support the growing demand for the next X months, with disk space as our prime concern.</p><p>ClickHouse stores extensive information in system tables about the operating processes, which is helpful. From the early days of using ClickHouse we added <a href=\"https://github.com/f1yegor/clickhouse_exporter\">clickhouse_exporter</a> as part of our monitoring stack. One of the metrics we are interested in is exposed from the <a href=\"https://clickhouse.tech/docs/en/operations/system-tables/parts/\">system.parts</a> table. Roughly speaking, clickhouse_exporter runs <a href=\"https://github.com/f1yegor/clickhouse_exporter/blob/master/exporter/exporter.go#L53\">SQL queries</a> asking how many bytes are used by each table. After that, these metrics are sent from Prometheus to <a href=\"https://thanos.io/\">Thanos</a> and stored for at least a year.</p><p>Every time we wanted to make a forecast of disk usage we queried Thanos for historical data using this expression:</p><p><code>sum by (table) (clickhouse_table_parts_bytes{cluster=\"{cluster}\"})</code><br><br>After that, we uploaded data in <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\">dataframes</a> to a <a href=\"https://jupyter.org/\">Jupyter notebook</a>.</p><p>There were a few problems with this approach. Only a few people knew where the notebooks were and how to get them running. It wasn't trivial to download historical data. And most importantly, it was hard to look at past predictions and assess whether or not they were correct since results were not stored anywhere except internal blog posts. Also, as the number and size of clusters and products grew it became impossible for a single team to work on capacity planning and we needed to get engineers building products involved as they have the most context on how the growth will change in the future.</p><p>We wanted to automate this process and made calculations more transparent for our colleagues, including those who use ClickHouse for their services. Honestly, at the beginning we weren’t sure if it was even possible and what we would get out of it.</p><h3 id=\"finding-the-right-metrics\">Finding the right metrics</h3><p>The crucial moment of adding new nodes for us is a disk space, so this was a place to start. We decided to use <em>system.parts, </em>as we used it before with the manual approach.</p><p>Luckily, we started doing it for the cluster that had recently changed its topology. That cluster had <em>two shards with four and five nodes in every shard</em>. After the topology change, it was replaced <em>with three shards and three nodes in every shard</em>, but the number of machines and unreplicated data on the disks remained the same. However, it had an impact on our metrics: we previously had four replicated nodes in one shard and five replicated in another, we took one node off from the first shard and two nodes from the second and created a new one based on these three nodes. The new shard was empty, so we just added it, but the total amount of data in the first and the second shards was less as the count of the remaining nodes.</p><p>You can see on the graph below in April we had this sharp decrease caused by topology changes. We got ~550T instead of ~850T among all shards and replicas.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/11/image3-1.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>When we tried to train our model based on the real data due to the April drop it thought we had a downward trend. Which was incorrect as we only dropped replicated data. The trend for unreplicated data hadn’t changed. So we decided to take into account only unreplicated data. It saved us from the topology change and node replacement in case of problems with hardware.</p><p>The rule that we use for metrics now is:</p><!--kg-card-begin: markdown--><pre><code>sum by(cluster) (\nmax by (cluster, shardgroup) (\nnode_clickhouse_shardgroupinfo{} *\non (instance) group_right (cluster, shardgroup) sum(table_parts_bytes{cluster=&quot;%s&quot;}) by (instance)\n))\n</code></pre>\n<!--kg-card-end: markdown--><p>We continue using <em>system.parts </em>from clickhouse_exporter, but instead of counting the whole amount of data we use the maximum of unreplicated data from every shard.</p><p>In the image below there is the same cluster as in the image above but instead of counting the whole amount of data we look at unreplicated data from all shards. You can clearly see that we continued to grow and didn’t have any drop in data.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/11/image2-1.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Another problem we faced was that we migrated some tables from one cluster to another because we were running out of space and it required immediate action. However, our model didn’t know that part of the tables didn’t live there anymore, and we didn’t want them to be a part of the prediction. To solve this problem we queried Prometheus to get the list of the tables that existed at the prediction time, then filtered historical data to include only these tables and used them as the input for training a model.</p><h3 id=\"load-of-metrics\">Load of metrics</h3><p>After determining the correct metrics, we needed to obtain them for our forecasting procedure. Our long-term metrics solution, Thanos, stores billions of data points. Querying it for a cluster with over one hundred nodes even for one day takes a huge amount of time, and we needed these data points for a year.</p><p>As we planned to use Python we wrote a small client using <a href=\"https://docs.aiohttp.org/\">aiohttp</a> that concurrently sends HTTP requests to Thanos. The requests are sent in chunks, and every request has start/end dates with a difference of one hour. We needed to get the data for the whole year once and then append new ones day by day. We got csv files: one file for one cluster. The client became a part of the project, and it runs once a day, queries Thanos for new metrics (previous day) and appends data to the files.</p><h3 id=\"forecasting-procedure\">Forecasting procedure</h3><p>At this point, we have collected metrics in files, now it’s time to make a forecast. We needed something for time-series metrics, so we chose <a href=\"https://facebook.github.io/prophet/\">Prophet</a> from Facebook. It’s very simple to use, you can follow the documentation and get good results even with the default parameters.</p><p>One challenge we faced using Prophet was the need to feed it one data point for a day. In the metric files we have thousands of those for every day. It looks logical to take the point at the end of every day, but it’s not really true. All tables have a retention period, the time for how long we store data in ClickHouse. We don’t know when the data is cleared, it happens gradually throughout the day. So, we decided to take the maximum number for a day.</p><h3 id=\"drawing-graphs\">Drawing Graphs</h3><p>We chose <a href=\"https://grafana.com/\">Grafana</a> to present results, though we needed to store predicted data points somewhere. The first thought was to use Prometheus, but because of high cardinality, we had about 300,000 points in summary for clusters and tables, so we passed. We decided to use ClickHouse itself. We wanted to have both graphs, real and predicted, on the same dashboard. We had real data points in Prometheus and with <a href=\"https://grafana.com/docs/grafana/latest/features/datasources/\">mixed data source</a> could do this. However, the problem was the same as the loading of metrics into files, for some clusters it’s impossible to obtain metrics for a long period of time. We added functionality to upload real metrics in ClickHouse as well, now both real and predicted metrics are displayed in Grafana, taken from ClickHouse.</p><h3 id=\"summary\">Summary</h3><p>This is what we have in Grafana:</p><ul><li>The yellow line shows the real data;</li><li>The green line was created based on Prophet output;</li><li>The red line - the maximum disk capacity. We already increased it twice.</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2020/11/image4-1.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>We have a service running in Kubernetes that does all the toil, and we created an environment for other metrics. We have the place where we collect metrics from Thanos and expose them in the required format to Grafana. If we find the right metrics for accounting other resources like IO, CPU or other systems like Kafka we can easily add them to our framework. We can easily replace Prophet with another algorithm, and we can go back months and evaluate how close we were in our prediction according to the real data.</p><p>With this automation we were able to spot we were going out of disk space for a couple of clusters which we didn’t expect. We have over 20 clusters and have updates for all of them every day. This dashboard is used not only by our colleagues who are direct customers of ClickHouse but by the team who makes a plan for buying servers. It is easy to read and costs none of developers time.</p><p>This project was carried out by the Core SRE team to improve our daily work. If you are interested in this project, check out our <a href=\"https://www.cloudflare.com/careers/jobs/\">job openings</a>.</p><p>We didn’t know what we would get at the end, we discussed, looked for solutions and tried different approaches. Huge thanks for this to Nicolae Vartolomei, Alex Semiglazov and John Skopis.</p>",
		"comment_id": "5fa4000430cdba01bcf670ed",
		"feature_image": "http://blog.cloudflare.com/content/images/2020/11/image4-2.png",
		"featured": false,
		"visibility": "public",
		"created_at": "2020-11-05T13:37:08.000+00:00",
		"updated_at": "2023-12-13T16:04:40.000+00:00",
		"published_at": "2020-11-05T14:12:48.000+00:00",
		"custom_excerpt": "We use ClickHouse widely at Cloudflare. It helps us with our internal analytics workload, bot management, customer dashboards, and many other systems. ",
		"codeinjection_head": null,
		"codeinjection_foot": null,
		"custom_template": null,
		"canonical_url": null,
		"tags": [
			{
				"id": "5d16450341acde0011a95155",
				"name": "Analytics",
				"slug": "analytics",
				"description": "Analytics (English)",
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/analytics/"
			},
			{
				"id": "5eb280586fa3db01bc6a31b2",
				"name": "Bot Management",
				"slug": "bot-management",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/bot-management/"
			},
			{
				"id": "5d16450341acde0011a951a3",
				"name": "Dashboard",
				"slug": "dashboard-tag",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/dashboard-tag/"
			},
			{
				"id": "5fa43c0c30cdba01bcf6714c",
				"name": "ClickHouse",
				"slug": "clickhouse",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": "ClickHouse",
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/clickhouse/"
			}
		],
		"authors": [
			{
				"id": "5f3141d797a44803f37fbf58",
				"name": "Oxana Kharitonova",
				"slug": "oxana",
				"profile_image": "http://blog.cloudflare.com/content/images/2023/05/Oxana-Kharitonova.png",
				"cover_image": null,
				"bio": null,
				"website": null,
				"location": "London",
				"facebook": null,
				"twitter": null,
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/oxana/"
			}
		],
		"primary_author": {
			"id": "5f3141d797a44803f37fbf58",
			"name": "Oxana Kharitonova",
			"slug": "oxana",
			"profile_image": "http://blog.cloudflare.com/content/images/2023/05/Oxana-Kharitonova.png",
			"cover_image": null,
			"bio": null,
			"website": null,
			"location": "London",
			"facebook": null,
			"twitter": null,
			"meta_title": null,
			"meta_description": null,
			"url": "http://blog.cloudflare.com/author/oxana/"
		},
		"primary_tag": {
			"id": "5d16450341acde0011a95155",
			"name": "Analytics",
			"slug": "analytics",
			"description": "Analytics (English)",
			"feature_image": null,
			"visibility": "public",
			"meta_title": null,
			"meta_description": null,
			"og_image": null,
			"og_title": null,
			"og_description": null,
			"twitter_image": null,
			"twitter_title": null,
			"twitter_description": null,
			"codeinjection_head": null,
			"codeinjection_foot": null,
			"canonical_url": null,
			"accent_color": null,
			"url": "http://blog.cloudflare.com/tag/analytics/"
		},
		"url": "http://blog.cloudflare.com/clickhouse-capacity-estimation-framework/",
		"excerpt": "We use ClickHouse widely at Cloudflare. It helps us with our internal analytics workload, bot management, customer dashboards, and many other systems. ",
		"reading_time": 7,
		"access": true,
		"comments": false,
		"og_image": "http://blog.cloudflare.com/content/images/2020/11/Copy-of-Open-Graph-Images-Template--1200x627----Q42020-2.png",
		"og_title": null,
		"og_description": null,
		"twitter_image": "http://blog.cloudflare.com/content/images/2020/11/Copy-of-Open-Graph-Images-Template--1200x627----Q42020-1.png",
		"twitter_title": null,
		"twitter_description": null,
		"meta_title": null,
		"meta_description": "We use ClickHouse widely at Cloudflare. It helps us with our internal analytics workload, bot management, customer dashboards, and many other systems. ",
		"email_subject": null,
		"frontmatter": null,
		"feature_image_alt": null,
		"feature_image_caption": null
	}
}