<div class="mb2 gray5">3 min read</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2024/07/image1-19.png" class="kg-image" alt="Meta Llama 3.1 now available on Workers AI" loading="lazy" width="1999" height="1125"></figure>
	<p>At Cloudflare, we’re big supporters of the open-source community – and that extends to our approach for <a href="https://developers.cloudflare.com/workers-ai">Workers AI</a> models as well. Our strategy for our Cloudflare AI products is to provide a top-notch developer experience and toolkit that can help people build applications with open-source models.</p>
	<p>We’re excited to be one of Meta’s launch partners to make their newest <a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md">Llama 3.1 8B model</a> available to all Workers AI users on Day 1. You can run their latest model by simply swapping out your model ID to <code>@cf/meta/llama-3.1-8b-instruct</code> or test out the model on our <a href="https://playground.ai.cloudflare.com">Workers AI Playground</a>. Llama 3.1 8B is free to use on Workers AI until the model graduates out of beta.</p>
	<p>Meta’s Llama collection of models have consistently shown high-quality performance in areas like general knowledge, steerability, math, tool use, and multilingual translation. Workers AI is excited to continue to distribute and serve the Llama collection of models on our serverless inference platform, powered by our globally distributed GPUs.</p>
	<p>The Llama 3.1 model is particularly exciting, as it is released in a higher precision (bfloat16), incorporates function calling, and adds support across 8 languages. Having multilingual support built-in means that you can use Llama 3.1 to write prompts and receive responses directly in languages like English, French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Expanding model understanding to more languages means that your applications have a bigger reach across the world, and it’s all possible with just one model.</p><!--kg-card-begin: markdown-->
	<pre><code class="language-JavaScript">const answer = await env.AI.run('@cf/meta/llama-3.1-8b-instruct', {
    stream: true,
    messages: [{
        "role": "user",
        "content": "Qu'est-ce que ç'est verlan en français?"
    }],
});
</code></pre>
	<!--kg-card-end: markdown-->
	<p>Llama 3.1 also introduces native function calling (also known as tool calls) which allows LLMs to generate structured JSON outputs which can then be fed into different APIs. This means that function calling is supported out-of-the-box, without the need for a fine-tuned variant of Llama that specializes in tool use. Having this capability built-in means that you can use one model across various tasks. </p>
	<p>Workers AI recently announced <a href="https://blog.cloudflare.com/embedded-function-calling">embedded function calling</a>, which is now usable with Meta Llama 3.1 as well. Our embedded function calling gives developers a way to run their inference tasks far more efficiently than traditional architectures, leveraging Cloudflare Workers to reduce the number of requests that need to be made manually. It also makes use of our open-source <a href="https://www.npmjs.com/package/@cloudflare/ai-utils">ai-utils</a> package, which helps you orchestrate the back-and-forth requests for function calling along with other helper methods that can automatically generate tool schemas. Below is an example function call to Llama 3.1 with embedded function calling that then stores key-values in Workers KV.</p><!--kg-card-begin: markdown-->
	<pre><code class="language-JavaScript">const response = await runWithTools(env.AI, "@cf/meta/llama-3.1-8b-instruct", {
    messages: [{ role: "user", content: "Greet the user and ask them a question" }],
    tools: [{
        name: "Store in memory",
        description: "Store everything that the user talks about in memory as a key-value pair.",
        parameters: {
            type: "object",
            properties: {
                key: {
                    type: "string",
                    description: "The key to store the value under.",
                },
                value: {
                    type: "string",
                    description: "The value to store.",
                },
            },
            required: ["key", "value"],
        },
        function: async ({ key, value }) =&gt; {
                await env.KV.put(key, value);

                return JSON.stringify({
                    success: true,
                });
         }
    }]
})
</code></pre>
	<!--kg-card-end: markdown-->
	<p>We’re excited to see what you build with these new capabilities. As always, use of the new model should be conducted with Meta’s <a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/USE_POLICY.md">Acceptable Use Policy</a> and <a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE">License</a> in mind. Take a look at our <a href="https://developers.cloudflare.com/workers-ai/models/llama-3.1-8b-instruct">developer documentation</a> to get started!</p>
</div>