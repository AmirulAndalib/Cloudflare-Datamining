{
	"post": {
		"id": "5d16453b41acde0011a955b5",
		"uuid": "b5fc4567-c756-47c8-ab4a-24b8e24aaaab",
		"title": "This is strictly a violation of the TCP specification",
		"slug": "this-is-strictly-a-violation-of-the-tcp-specification",
		"html": "<!--kg-card-begin: markdown--><p>I was asked to debug another weird issue on our network. Apparently every now and then a connection going through CloudFlare would time out with 522 HTTP error.</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2016/08/16132759228_7eed8f32d1_z.jpg\" alt=\"\" loading=\"lazy\"><br>\n<small><a href=\"https://creativecommons.org/lclosicenses/by/2.0/\">CC BY 2.0</a> <a href=\"https://www.flickr.com/photos/cosmicherb70/16132759228/\">image</a> by Chris Combe</small></p>\n<p><a href=\"https://support.cloudflare.com/hc/en-us/articles/200171906-Error-522-Connection-timed-out\">522 error on CloudFlare</a> indicates a connection issue between our edge server and the origin server. Most often the blame is on the origin server side - the origin server is slow, offline or encountering high packet loss. Less often the problem is on our side.</p>\n<p>In the case I was debugging it was neither. The internet connectivity between CloudFlare and origin was perfect. No packet loss, flat latency. So why did we see a 522 error?</p>\n<p>The root cause of this issue was pretty complex. After a lot of debugging we identified an important symptom: sometimes, once in thousands of runs, our test program failed to establish a connection between two daemons on the same machine. To be precise, an NGINX instance was trying to establish a TCP connection to our internal acceleration service on localhost. This failed with a timeout error.</p>\n<p>Once we knew what to look for we were able to reproduce this with good old <code>netcat</code>. After a couple of dozen of runs this is what we saw:</p>\n<pre><code>$ nc 127.0.0.1 5000  -v\nnc: connect to 127.0.0.1 port 5000 (tcp) failed: Connection timed out\n</code></pre>\n<p>The view from <code>strace</code>:</p>\n<pre><code>socket(PF_INET, SOCK_STREAM, IPPROTO_TCP) = 3\nconnect(3, {sa_family=AF_INET, sin_port=htons(5000), sin_addr=inet_addr(&quot;127.0.0.1&quot;)}, 16) = -110 ETIMEDOUT\n</code></pre>\n<p><code>netcat</code> calls <code>connect()</code> to establish a connection to localhost. This takes a long time and eventually fails with <code>ETIMEDOUT</code> error. Tcpdump confirms that <code>connect()</code> did send SYN packets over loopback but never received any SYN+ACKs:</p>\n<pre><code>$ sudo tcpdump -ni lo port 5000 -ttttt -S\n00:00:02.405887 IP 127.0.0.12.59220 &gt; 127.0.0.1.5000: Flags [S], seq 220451580, win 43690, options [mss 65495,sackOK,TS val 15971607 ecr 0,nop,wscale 7], length 0\n00:00:03.406625 IP 127.0.0.12.59220 &gt; 127.0.0.1.5000: Flags [S], seq 220451580, win 43690, options [mss 65495,sackOK,TS val 15971857 ecr 0,nop,wscale 7], length 0\n... 5 more ...\n</code></pre>\n<p>Hold on. What just happened here?</p>\n<p>Well, we called <code>connect()</code> to localhost and it timed out. The SYN packets went off over loopback to localhost but were never answered.</p>\n<h3 id=\"loopbackcongestion\">Loopback congestion</h3>\n<p><img src=\"http://blog.cloudflare.com/content/images/2016/08/26449341072_009ae28070_z.jpg\" alt=\"\" loading=\"lazy\"><br>\n<small><a href=\"https://creativecommons.org/licenses/by/2.0/\">CC BY 2.0</a> <a href=\"https://www.flickr.com/photos/akj1706/26449341072\">image</a> by akj1706</small></p>\n<p>The first thought is about Internet stability. Maybe the SYN packets were lost? A little known fact is that it's not possible to have any packet loss or congestion on the loopback interface. The <a href=\"http://lxr.free-electrons.com/source/drivers/net/loopback.c\">loopback works magically</a>: when an application sends packets to it, it immediately, still within the <code>send</code> syscall handling, gets delivered to the appropriate target. There is no buffering over loopback. Calling <code>send</code> over loopback triggers iptables, network stack delivery mechanisms and <em>delivers</em> the packet to the appropriate queue of the target application. Assuming the target application has some space in its buffers, packet loss over loopback is not possible.</p>\n<h3 id=\"maybethelisteningapplicationmisbehaved\">Maybe the listening application misbehaved?</h3>\n<p>Under normal circumstances connections to localhost are not supposed to time out. There is one corner case when this may happen though - when the listening application does not call <code>accept()</code> fast enough.</p>\n<p>When that happens, the default behavior is to drop the new SYN packets. If the listening socket has a full accept queue, then new SYN packets will be dropped. The intention is to cause push-back, to slow down the rate of incoming connections. The peers should eventually re-send SYN packets, and hopefully by that time the accept queue will be freed. This behavior is controlled by the <code>tcp_abort_on_overflow</code> <a href=\"https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt\">sysctl</a>.</p>\n<p>But this accept queue overflow did not happen in our case. Our listening application had an empty accept queue. We checked this with the <code>ss</code> command:</p>\n<pre><code>$ ss -n4lt 'sport = :5000'\nState      Recv-Q Send-Q  Local Address:Port    Peer Address:Port\nLISTEN     0      128                 *:5000               *:*\n</code></pre>\n<p>The <code>Send-Q</code> column <a href=\"https://github.com/torvalds/linux/blob/c1e64e298b8cad309091b95d8436a0255c84f54a/net/ipv4/tcp_diag.c#L26\">shows the backlog / accept queue size</a> given to <code>listen()</code> syscall - 128 in our case. The <code>Recv-Q</code> reports on the number of outstanding connections in the accept queue - zero.</p>\n<h3 id=\"theproblem\">The problem</h3>\n<p>To recap: we are establishing connections to localhost. Most of them work fine but sometimes the <code>connect()</code> syscall times out. The SYN packets are being sent over loopback. Because it's loopback they <em>are</em> being delivered to the listening socket. The listening socket accept queue is empty, but we see no SYN+ACKs.</p>\n<p>Further investigation revealed something peculiar. We noticed hundreds of CLOSE_WAIT sockets:</p>\n<pre><code>$ ss -n4t | head\nState      Recv-Q Send-Q  Local Address:Port    Peer Address:Port\nCLOSE-WAIT 1      0           127.0.0.1:5000       127.0.0.1:36599\nCLOSE-WAIT 1      0           127.0.0.1:5000       127.0.0.1:36467\nCLOSE-WAIT 1      0           127.0.0.1:5000       127.0.0.1:36154\nCLOSE-WAIT 1      0           127.0.0.1:5000       127.0.0.1:36412\nCLOSE-WAIT 1      0           127.0.0.1:5000       127.0.0.1:36536\n...\n</code></pre>\n<h3 id=\"whatisclose_waitanyway\">What is CLOSE_WAIT anyway?</h3>\n<p><img src=\"http://blog.cloudflare.com/content/images/2016/08/20147524535_8c6ac1c853_z.jpg\" alt=\"\" loading=\"lazy\"><br>\n<small><a href=\"https://creativecommons.org/licenses/by/2.0/\">CC BY 2.0</a> <a href=\"https://www.flickr.com/photos/sidelong/20147524535\">image</a> by DaveBleasdale</small></p>\n<p>Citing the <a href=\"https://access.redhat.com/solutions/437133\">Red Hat docs</a>:</p>\n<p><em>CLOSE_WAIT - Indicates that the server has received the first FIN signal from the client and the connection is in the process of being closed. This means the socket is waiting for the application to execute <code>close()</code>. A socket can be in CLOSE_WAIT state indefinitely until the application closes it. Faulty scenarios would be like a file descriptor leak: server not executing <code>close()</code> on sockets leading to pile up of CLOSE_WAIT sockets.</em></p>\n<p>This makes sense. Indeed, we were able to confirm the listening application leaks sockets. Hurray, good progress!</p>\n<p>The leaking sockets don't explain everything though.</p>\n<p>Usually a Linux process can open up to 1,024 file descriptors. If our application did run out of file descriptors the <code>accept</code> syscall would return the EMFILE error. If the application further mishandled this error case, this could result in losing incoming SYN packets. Failed <code>accept</code> calls will <a href=\"https://github.com/torvalds/linux/blob/c1e64e298b8cad309091b95d8436a0255c84f54a/net/socket.c#L1438\">not dequeue a socket from accept queue</a>, causing the accept queue to grow. The accept queue will not be drained and will eventually overflow. An overflowing accept queue could result in dropped SYN packets and failing connection attempts.</p>\n<p>But this is not what happened here. Our application hasn't run out of file descriptors yet. This can be verified by counting file descriptors in <code>/proc/&lt;pid&gt;/fd</code> directory:</p>\n<pre><code>$ ls /proc/` pidof listener `/fd | wc -l\n517\n</code></pre>\n<p>517 file descriptors are comfortably far from the 1,024 file descriptor limit. Also, we earlier showed with <code>ss</code> that the accept queue is empty. So why did our connections time out?</p>\n<h3 id=\"whatreallyhappens\">What really happens</h3>\n<p>The root cause of the problem is definitely our application leaking sockets. The symptoms though, the connection timing out, are still unexplained.</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2016/08/Screen-Shot-2016-08-11-at-23-59-05-1.png\" alt=\"\" loading=\"lazy\"></p>\n<p>Time to raise the curtain of doubt. Here is what happens.</p>\n<p>The listening application leaks sockets, they are stuck in CLOSE_WAIT TCP state forever. These sockets look like (127.0.0.1:5000, 127.0.0.1:some-port). The client socket at the other end of the connection is (127.0.0.1:some-port, 127.0.0.1:5000), and is properly closed and cleaned up.</p>\n<p>When the client application quits, the (127.0.0.1:some-port, 127.0.0.1:5000) socket enters the FIN_WAIT_1 state and then quickly transitions to FIN_WAIT_2. The FIN_WAIT_2 state should move on to TIME_WAIT if the client received FIN packet, but this never happens. The FIN_WAIT_2 eventually times out. On Linux this is 60 seconds, controlled by <code>net.ipv4.tcp_fin_timeout</code> sysctl.</p>\n<p>This is where the problem starts. The (127.0.0.1:5000, 127.0.0.1:some-port) socket is still in CLOSE_WAIT state, while (127.0.0.1:some-port, 127.0.0.1:5000) has been cleaned up and is ready to be reused. When this happens the result is a total mess. One part of the socket won't be able to advance from the SYN_SENT state, while the other part is stuck in CLOSE_WAIT. The SYN_SENT socket will eventually give up failing with ETIMEDOUT.</p>\n<h3 id=\"howtoreproduce\">How to reproduce</h3>\n<p>It all starts with a listening application that leaks sockets and forgets to call <code>close()</code>. This kind of bug does happen in complex applications. An example <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2016-08-time-out/listener.go\">buggy code is available here</a>. When you run it nothing will happen initially. <code>ss</code> will show a usual listening socket:</p>\n<pre><code>$ go build listener.go &amp;&amp; ./listener &amp;\n$ ss -n4tpl 'sport = :5000'\nState      Recv-Q Send-Q  Local Address:Port    Peer Address:Port\nLISTEN     0      128                 *:5000               *:*      users:((&quot;listener&quot;,81425,3))\n\n</code></pre>\n<p>Then we have a client application. The client behaves correctly - it establishes a connection and after a while it closes it. We can demonstrate this with <code>nc</code>:</p>\n<pre><code>$ nc -4 localhost 5000 &amp;\n$ ss -n4tp '( dport = :5000 or sport = :5000 )'\nState      Recv-Q Send-Q  Local Address:Port    Peer Address:Port\nESTAB      0      0           127.0.0.1:5000       127.0.0.1:36613  users:((&quot;listener&quot;,81425,5))\nESTAB      0      0           127.0.0.1:36613      127.0.0.1:5000   users:((&quot;nc&quot;,81456,3))\n</code></pre>\n<p>As you see above <code>ss</code> shows two TCP sockets, representing the two ends of the TCP connection. The client one is (127.0.0.1:36613, 127.0.0.1:5000), the server one (127.0.0.1:5000, 127.0.0.1:36613).</p>\n<p>The next step is to gracefully close the client connection:</p>\n<pre><code>$ kill `pidof nc`\n</code></pre>\n<p>Now the connections enter TCP cleanup stages: FIN_WAIT_2 for the client connection, and CLOSE_WAIT for the server one (if you want to read more about these TCP states <a href=\"https://benohead.com/tcp-about-fin_wait_2-time_wait-and-close_wait/\">here's a recommended read</a>):</p>\n<pre><code>$ ss -n4tp\nState      Recv-Q Send-Q  Local Address:Port    Peer Address:Port\nCLOSE-WAIT 1      0           127.0.0.1:5000       127.0.0.1:36613  users:((&quot;listener&quot;,81425,5))\nFIN-WAIT-2 0      0           127.0.0.1:36613      127.0.0.1:5000\n</code></pre>\n<p>After a while FIN_WAIT_2 will expire:</p>\n<pre><code>$ ss -n4tp\nState      Recv-Q Send-Q  Local Address:Port    Peer Address:Port\nCLOSE-WAIT 1      0           127.0.0.1:5000       127.0.0.1:36613  users:((&quot;listener&quot;,81425,5))\n</code></pre>\n<p>But the CLOSE_WAIT socket stays in! Since we have a leaked file descriptor in the <code>listener</code> program, the kernel is not allowed to move it to FIN_WAIT state. It is stuck in CLOSE_WAIT indefinitely. This stray CLOSE_WAIT would not be a problem if only the same port pair was never reused.  Unfortunately, it happens and causes the problem.</p>\n<p>To see this we need to launch hundreds of <code>nc</code> instances and hope the kernel will assign the colliding port number to one of them. The affected <code>nc</code> will be stuck in <code>connect()</code> for a while:</p>\n<pre><code>$ nc -v -4 localhost 5000 -w0\n...\n</code></pre>\n<p>We can use the <code>ss</code> to confirm that the ports indeed collide:</p>\n<pre><code>SYN-SENT   0  1   127.0.0.1:36613      127.0.0.1:5000   users:((&quot;nc&quot;,89908,3))\nCLOSE-WAIT 1  0   127.0.0.1:5000       127.0.0.1:36613  users:((&quot;listener&quot;,81425,5))\n</code></pre>\n<p>In our example the kernel allocated source address (127.0.0.1:36613) to the <code>nc</code> process. This TCP flow is okay to be used for a connection going <em>to</em> the listener application. But the listener will not be able to allocate a flow in reverse direction since (127.0.0.1:5000, 127.0.0.1:36613) from previous connections is still being used and remains with CLOSE_WAIT state.</p>\n<p>The kernel gets confused. It retries the SYN packets, but will never respond since the other TCP socket is stick in the CLOSE_WAIT state. Eventually our affected <code>netcat</code> will die with unhappy ETIMEDOUT error message:</p>\n<pre><code>...\nnc: connect to localhost port 5000 (tcp) failed: Connection timed out\n</code></pre>\n<p>If you want to reproduce this weird scenario consider running this script. It will greatly increase the probability of netcat hitting the conflicted socket:</p>\n<pre><code>$ for i in `seq 500`; do nc -v -4 -s 127.0.0.1 localhost 5000 -w0; done\n</code></pre>\n<p>A little known fact is that the source port automatically assigned by the kernel is incremental, unless you <a href=\"https://idea.popcount.org/2014-04-03-bind-before-connect/\">select the source IP manually</a>. In such case the source port is random. This bash script will create a minefield of CLOSE_WAIT sockets randomly distributed across the ephemeral port range.</p>\n<h3 id=\"finalwords\">Final words</h3>\n<p>If there's a moral from the story it's to watch out for CLOSE_WAIT sockets. Their presence indicate leaking sockets, and with leaking sockets some incoming connections may time out. Presence of many FIN_WAIT_2 sockets says the problem is not on current machine but on the remote end of the connection.</p>\n<p>Furthermore, this bug shows that it is possible for the states of the two ends of a TCP connection to be at odds, even if the connection is over the loopback interface.</p>\n<p>It seems that the design decisions made by the BSD Socket API have unexpected long lasting consequences. If you think about it - why exactly the socket can automatically expire the FIN_WAIT state, but can't move off from CLOSE_WAIT after some grace time. This is very confusing... And it should be! The original TCP specification does not allow automatic state transition after FIN_WAIT_2 state! According to the spec FIN_WAIT_2 is supposed to stay running until the application on the other side cleans up.</p>\n<p>Let me leave you with the <a href=\"http://man7.org/linux/man-pages/man7/tcp.7.html\"><code>tcp(7)</code> manpage</a> describing the <code>tcp_fin_timeout</code> setting:</p>\n<pre><code>tcp_fin_timeout (integer; default: 60)\n      This specifies how many seconds to wait for a final FIN packet\n      before the socket is forcibly closed.  This is strictly a\n      violation of the TCP specification, but required to prevent\n      denial-of-service attacks.\n</code></pre>\n<p>I think now we understand why automatically closing FIN_WAIT_2 is strictly speaking a violation of the TCP specification.</p>\n<p><em>Do you enjoy playing with low level networking bits? Are you interested in dealing with some of the largest DDoS attacks ever seen?</em></p>\n<p><em>If so you should definitely have a look at the <a href=\"https://www.cloudflare.com/join-our-team/\">open positions</a> in our London, San Francisco, Singapore, Champaign (IL) and Austin (TX) offices!</em></p>\n<!--kg-card-end: markdown-->",
		"comment_id": "4830",
		"feature_image": "http://blog.cloudflare.com/content/images/2018/08/16132759228_7eed8f32d1_z.jpg",
		"featured": false,
		"visibility": "public",
		"created_at": "2016-08-10T20:09:54.000+01:00",
		"updated_at": "2024-02-20T17:23:17.000+00:00",
		"published_at": "2016-08-12T14:03:26.000+01:00",
		"custom_excerpt": "I was asked to debug another weird issue on our network. Apparently every now and then a connection going through CloudFlare would time out with 522 HTTP error.",
		"codeinjection_head": null,
		"codeinjection_foot": null,
		"custom_template": null,
		"canonical_url": null,
		"authors": [
			{
				"id": "5d1644b141acde0011a94f30",
				"name": "Marek Majkowski",
				"slug": "marek-majkowski",
				"profile_image": "http://blog.cloudflare.com/content/images/2017/03/b5967d6c687939594adb6992723d0529.jpeg",
				"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-101.png",
				"bio": null,
				"website": null,
				"location": null,
				"facebook": null,
				"twitter": "@majek04",
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/marek-majkowski/"
			}
		],
		"tags": [
			{
				"id": "5d16450341acde0011a95163",
				"name": "TCP",
				"slug": "tcp",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/tcp/"
			},
			{
				"id": "5d16450341acde0011a95221",
				"name": "Best Practices",
				"slug": "best-practices",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/best-practices/"
			}
		],
		"primary_author": {
			"id": "5d1644b141acde0011a94f30",
			"name": "Marek Majkowski",
			"slug": "marek-majkowski",
			"profile_image": "http://blog.cloudflare.com/content/images/2017/03/b5967d6c687939594adb6992723d0529.jpeg",
			"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-101.png",
			"bio": null,
			"website": null,
			"location": null,
			"facebook": null,
			"twitter": "@majek04",
			"meta_title": null,
			"meta_description": null,
			"url": "http://blog.cloudflare.com/author/marek-majkowski/"
		},
		"primary_tag": {
			"id": "5d16450341acde0011a95163",
			"name": "TCP",
			"slug": "tcp",
			"description": null,
			"feature_image": null,
			"visibility": "public",
			"meta_title": null,
			"meta_description": null,
			"og_image": null,
			"og_title": null,
			"og_description": null,
			"twitter_image": null,
			"twitter_title": null,
			"twitter_description": null,
			"codeinjection_head": null,
			"codeinjection_foot": null,
			"canonical_url": null,
			"accent_color": null,
			"url": "http://blog.cloudflare.com/tag/tcp/"
		},
		"url": "http://blog.cloudflare.com/this-is-strictly-a-violation-of-the-tcp-specification/",
		"excerpt": "I was asked to debug another weird issue on our network. Apparently every now and then a connection going through CloudFlare would time out with 522 HTTP error.",
		"reading_time": 9,
		"access": true,
		"comments": false,
		"og_image": null,
		"og_title": null,
		"og_description": null,
		"twitter_image": null,
		"twitter_title": null,
		"twitter_description": null,
		"meta_title": null,
		"meta_description": null,
		"email_subject": null,
		"frontmatter": null,
		"feature_image_alt": null,
		"feature_image_caption": null
	},
	"locale": "en-us"
}