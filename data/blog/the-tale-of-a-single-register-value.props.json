{
	"browserLocale": "en-us",
	"locale": "en-us",
	"post": {
		"access": true,
		"authors": [
			{
				"id": "5d1644b141acde0011a9501b",
				"name": "Jakub Sitnicki",
				"slug": "jakub",
				"profile_image": "http://blog.cloudflare.com/content/images/2019/05/4_300698554542850065.jpg",
				"cover_image": "http://blog.cloudflare.com/content/images/2019/05/general@2x-2.png",
				"bio": null,
				"website": null,
				"location": null,
				"facebook": null,
				"twitter": null,
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/jakub/"
			}
		],
		"canonical_url": null,
		"codeinjection_foot": null,
		"codeinjection_head": null,
		"comment_id": "618270043d6e9c02a9439a2a",
		"comments": false,
		"created_at": "2021-11-03T11:18:28.000+00:00",
		"custom_excerpt": "It’s not every day that you get to debug what may well be a packet of death. It was certainly the first time for me.\nWhat do I mean by “a packet of death”? A software bug where the network stack crashes in reaction to a single received network packet, taking down the whole operating system with it. ",
		"custom_template": null,
		"email_subject": null,
		"excerpt": "It’s not every day that you get to debug what may well be a packet of death. It was certainly the first time for me.\nWhat do I mean by “a packet of death”? A software bug where the network stack crashes in reaction to a single received network packet, taking down the whole operating system with it. ",
		"feature_image": "http://blog.cloudflare.com/content/images/2021/11/The-tale-of-a-single-register-value-header.png",
		"feature_image_alt": null,
		"feature_image_caption": null,
		"featured": false,
		"frontmatter": null,
		"html": "<figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/The-tale-of-a-single-register-value-header-2.png\" class=\"kg-image\" alt=\"The tale of a single register value\" loading=\"lazy\"></figure><blockquote>“Once you eliminate the impossible, whatever remains, no matter how improbable, must be the truth.” — Sherlock Holmes</blockquote><h2 id=\"intro\">Intro</h2><p>It’s not every day that you get to debug what may well be a packet of death. It was certainly the first time for me.</p><p>What do I mean by “a packet of death”? A software bug where the network stack crashes in reaction to a single received network packet, taking down the whole operating system with it. Like in the well known case of Windows <a href=\"https://en.wikipedia.org/wiki/Ping_of_death\">ping of death</a>.</p><p><em>Challenge accepted.</em></p><h2 id=\"it-starts-with-an-oops\">It starts with an oops</h2><p>Around a year ago <a href=\"https://lore.kernel.org/netdev/CAKxSbF01cLpZem2GFaUaifh0S-5WYViZemTicAg7FCHOnh6kug@mail.gmail.com/T/#u\">we started seeing kernel crashes</a> in the Linux ipv4 stack. Servers were crashing sporadically, but we learned the hard way to never ignore cases like that — when possible we always trace crashes. We also couldn’t tie it to a particular kernel version, which could indicate a regression which hopefully could be tracked down to a single faulty change in the Linux kernel.</p><p>The crashed servers were leaving behind only a crash report, affectionately known as a “kernel oops”. Let’s take a look at it and go over what information we have there.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/oops-before-decode-white-bg.png\" class=\"kg-image\" alt=\"kernel oops before decoding\" loading=\"lazy\"></figure><p>Parts of the oops, like offsets into functions, need to be decoded in order to be human-readable. Fortunately Linux comes with the <code>decode_stacktrace.sh</code> script that did the work for us.</p><p>All we need is to install a kernel debug and source packages before running the script. We will use the latest version of the script as it has been significantly improved since Linux v5.4 came out.</p><!--kg-card-begin: markdown--><pre><code>$ RELEASE=`uname -r`\n$ apt install linux-image-$RELEASE-dbg linux-source-$RELEASE\n$ curl -sLO https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/plain/scripts/decode_stacktrace.sh\n$ curl -sLO https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/plain/scripts/decodecode\n$ chmod +x decode_stacktrace.sh decodecode\n$ ./decode_stacktrace.sh -r 5.4.14-cloudflare-2020.1.11 &lt; oops.txt &gt; oops-decoded.txt\n</code></pre>\n<!--kg-card-end: markdown--><p>When decoded, the oops report is even longer than before! But that is a good thing. There is new information there that can help us.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/oops-after-decode-white-bg.png\" class=\"kg-image\" alt=\"kernel oops after decoding\" loading=\"lazy\"></figure><h2 id=\"what-has-happened\">What has happened?</h2><p>With this much input we can start sketching a picture of what could have happened. First thing to check is where exactly did we crash?</p><p>The report points at line 5160 in the <code>skb_gso_transport_seglen()</code> function. If we take a look at the <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/net/core/skbuff.c?h=v5.4.14#n5160\">source code</a>, we can get a rough idea of what happens there. We are processing a <a href=\"https://www.kernel.org/doc/html/latest/networking/segmentation-offloads.html#generic-segmentation-offload\">Generic Segmentation Offload</a> (GSO) packet carrying an encapsulated TCP packet. What is a GSO packet? In this context it's a batch of consecutive TCP segments, travelling through the network stack together to amortize the processing cost. We will look more at the GSO later.</p><!--kg-card-begin: markdown--><pre><code>net/core/skbuff.c:\n5150) static unsigned int skb_gso_transport_seglen(const struct sk_buff *skb)\n5151) {\n          …\n5155)     if (skb-&gt;encapsulation) {\n                  …\n5159)             if (likely(shinfo-&gt;gso_type &amp; (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)))\n5160)                     thlen += inner_tcp_hdrlen(skb); ?\n5161)     } else if (…) {\n          …\n5172)     return thlen + shinfo-&gt;gso_size;\n5173) }\n</code></pre>\n<!--kg-card-end: markdown--><p>The exact line where we crashed belongs to an if-branch that handles tunnel traffic. It calculates the length of the TCP header of the inner packet, that is the encapsulated one. We do that to compute the length of the outer L4 segment, which accounts for the inner packet length:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/image3-9.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>To understand how the length of the inner TCP header is computed we have to peel off a few layers of inlined function calls:</p><!--kg-card-begin: markdown--><pre><code>inner_tcp_hdrlen(skb)\n⇓\ninner_tcp_hdr(skb)-&gt;doff * 4\n⇓\n((struct tcphdr *)skb_inner_transport_header(skb))-&gt;doff * 4\n⇓\n((struct tcphdr *)(skb-&gt;head + skb-&gt;inner_transport_header))-&gt;doff * 4\n</code></pre>\n<!--kg-card-end: markdown--><p>Now it is clear that <code>inner_tcp_hdrlen(skb)</code> simply reads the <a href=\"https://datatracker.ietf.org/doc/html/rfc793#section-3.1\">Data Offset field</a> (<code>doff</code>) inside the inner TCP header. Because Data Offset carries the number of 32-bit words in the TCP header, we multiply it by 4 to get the TCP header length in bytes.</p><p>From the memory access point of view, to read the Data Offset value we need to:</p><ol><li>load <code>skb-&gt;head</code> value from address <code>skb + offsetof(struct sk_buff, head)</code></li><li>load <code>skb-&gt;inner_transport_header</code> value from address <code>skb + offsetof(struct sk_buff, inner_transport_header)</code>,</li><li>load the TCP Data Offset from <code>skb-&gt;head + skb-&gt;inner_transport_header + offsetof(struct tcphdr, doff)</code></li></ol><p>Potentially, any of these loads could trigger a page fault. But it’s unlikely that <code>skb</code> contains an invalid address since we accessed the <code>skb-&gt;encapsulation</code> field without crashing just a few lines earlier. Our main suspect is the last load.</p><p>The invalid memory address we attempt to load from should be in one of the CPU registers at the time of the exception. And we have the CPU register snapshot in the oops report. Which register holds the address? That has been decided by the compiler. We will need to take a look at the instruction stream to discover that.</p><p>Remember the disassembly in the decoded kernel oops? Now is the time to go back to it. Hint, it’s in <a href=\"https://en.wikipedia.org/wiki/X86_assembly_language#Syntax\">AT&amp;T syntax</a>. But to give everyone a fair chance to follow along, here’s the same disassembly but in Intel syntax. (Alright, alright. You caught me. I just can’t read the AT&amp;T syntax.)</p><!--kg-card-begin: markdown--><pre><code>All code\n========\n   0:   c0 41 83 e0             rol    BYTE PTR [rcx-0x7d],0xe0\n   4:   11 f6                   adc    esi,esi\n   6:   87 81 00 00 00 20       xchg   DWORD PTR [rcx+0x20000000],eax\n   c:   74 30                   je     0x3e\n   e:   0f b7 87 aa 00 00 00    movzx  eax,WORD PTR [rdi+0xaa]\n  15:   0f b7 b7 b2 00 00 00    movzx  esi,WORD PTR [rdi+0xb2]\n  1c:   48 01 c1                add    rcx,rax\n  1f:   48 29 f0                sub    rax,rsi\n  22:   45 85 c0                test   r8d,r8d\n  25:   48 89 c6                mov    rsi,rax\n  28:   74 0d                   je     0x37\n  2a:*  0f b6 41 0c             movzx  eax,BYTE PTR [rcx+0xc]           &lt;-- trapping instruction\n  2e:   c0 e8 04                shr    al,0x4\n  31:   0f b6 c0                movzx  eax,al\n  34:   8d 04 86                lea    eax,[rsi+rax*4]\n  37:   0f b7 52 04             movzx  edx,WORD PTR [rdx+0x4]\n  3b:   01 d0                   add    eax,edx\n  3d:   c3                      ret\n  3e:   45                      rex.RB\n  3f:   85                      .byte 0x85\n\nCode starting with the faulting instruction\n===========================================\n   0:   0f b6 41 0c             movzx  eax,BYTE PTR [rcx+0xc]\n   4:   c0 e8 04                shr    al,0x4\n   7:   0f b6 c0                movzx  eax,al\n   a:   8d 04 86                lea    eax,[rsi+rax*4]\n   d:   0f b7 52 04             movzx  edx,WORD PTR [rdx+0x4]\n  11:   01 d0                   add    eax,edx\n  13:   c3                      ret\n  14:   45                      rex.RB\n  15:   85                      .byte 0x85\n</code></pre>\n<!--kg-card-end: markdown--><p>When the trapped page fault happened, we tried to load from address <code>%rcx + 0xc</code>, or 12 bytes from whatever memory location <code>%rcx</code> held. Which is hardly a coincidence since the Data Offset field is 12 bytes into the TCP header.</p><p>This means that <code>%rcx</code> holds the computed <code>skb-&gt;head + skb-&gt;inner_transport_header</code> address. Let’s take a look at it:</p><!--kg-card-begin: markdown--><pre><code>RSP: 0018:ffffa4740d344ba0 EFLAGS: 00010202\nRAX: 000000000000feda RBX: ffff9d982becc900 RCX: ffff9d9624bbaffc\nRDX: ffff9d9624babec0 RSI: 000000000000feda RDI: ffff9d982becc900\n…\n</code></pre>\n<!--kg-card-end: markdown--><p>The RCX value doesn’t look particularly suspicious. We can say that:</p><ol><li>it’s in a kernel virtual address space because it is greater than <code>0xffff000000000000</code> - expected, and</li><li>it is very close to the 4 KiB page boundary (<code>0xffff9d9624bbb000</code> - 4),</li></ol><p>... and not much more.</p><p>We must go back further in the instruction stream. Where did the value in <code>%rcx</code> come from? What I like to do is try to correlate the machine code leading up to the crash with pseudo source code:</p><!--kg-card-begin: markdown--><pre><code>&lt;function entry&gt;                # %rdi = skb\n…\nmovzx  eax,WORD PTR [rdi+0xaa]  # %eax = skb-&gt;inner_transport_header\nmovzx  esi,WORD PTR [rdi+0xb2]  # %esi = skb-&gt;transport_header\nadd    rcx,rax                  # %rcx = skb-&gt;head + skb-&gt;inner_transport_header\nsub    rax,rsi                  # %rax = skb-&gt;inner_transport_header - skb-&gt;transport_header\ntest   r8d,r8d\nmov    rsi,rax                  # %rsi = skb-&gt;inner_transport_header - skb-&gt;transport_header\nje     0x37\nmovzx  eax,BYTE PTR [rcx+0xc]   # %eax = *(skb-&gt;head + skb-&gt;inner_transport_header + offsetof(struct tcphdr, doff))\n</code></pre>\n<!--kg-card-end: markdown--><p>How did I decode that assembly snippet? We know that the <code>skb</code> address was passed to our function in the <code>%rdi</code> register because the <a href=\"https://en.wikipedia.org/wiki/X86_calling_conventions#System_V_AMD64_ABI\">System V AMD64 ABI calling convention</a> dictates that. If the <code>%rdi</code> register hasn’t been clobbered by any function calls, or reused because the compiler decided so, then maybe, just maybe, it still holds the <code>skb</code> address.</p><p>If <code>0xaa</code> and <code>0xb2</code> are offsets into an <code>sk_buff</code> structure, then <code>pahole</code> tool can tell us which fields they correspond to:</p><!--kg-card-begin: markdown--><pre><code>$ pahole --hex -C sk_buff /usr/lib/debug/vmlinux-5.4.14-cloudflare-2020.1.11 | grep '\\(head\\|inner_transport_header\\|transport_header\\);'\n        __u16                      inner_transport_header; /*  0xaa   0x2 */\n        __u16                      transport_header;     /*  0xb2   0x2 */\n        unsigned char *            head;                 /*  0xc0   0x8 */\n</code></pre>\n<!--kg-card-end: markdown--><p>To confirm our guesswork, we can <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2021-10-gso-encap-crash/listings/gdb-disassemble-skb_gso_transport_seglen.txt\">disassemble the whole function</a> in <code>gdb</code>.</p><p>It would be great to find out the value of the <code>inner_transport_header</code> and <code>transport_header</code> offsets. But the registers that were holding them, <code>%rax</code> and <code>%rsi</code>, respectively, were reused after the offset values were loaded.</p><p>However, we can still examine the difference between <code>inner_transport_header</code> and <code>transport_header</code> that both <code>%rax</code> and <code>%rsi</code> hold. Let’s take a look.</p><h2 id=\"the-suspicious-offset\">The suspicious offset</h2><p>Here are the register values from the oops as a reminder:</p><!--kg-card-begin: markdown--><pre><code>RAX: 000000000000feda RBX: ffff9d982becc900 RCX: ffff9d9624bbaffc\nRDX: ffff9d9624babec0 RSI: 000000000000feda RDI: ffff9d982becc900\n</code></pre>\n<!--kg-card-end: markdown--><p>From the register snapshot we can tell that:</p><p><code>%rax = %rsi = skb-&gt;inner_transport_header - skb-&gt;transport_header = 0xfeda = 65242</code></p><p>That is clearly suspicious. We expect that <code>skb-&gt;transport_header &lt; skb-&gt;inner_transport_header</code>, so either</p><ol><li><code>skb-&gt;inner_transport_header &gt; 0xfeda</code>, which would mean that between outer and inner L4 packets there is 65k+ bytes worth of headers - unlikely, or</li><li><code>0xfeda</code> is a garbage value, perhaps an effect of an underflow if <code>skb-&gt;inner_transport_header &lt; skb-&gt;transport_header</code>.</li></ol><p>Let’s entertain the theory that an underflow has occurred.</p><p>Any other scenario, be it an out-of-bounds write or a use-after-free that corrupted the memory, is a scary prospect where we don’t stand much chance of debugging it without help from tools like <a href=\"https://www.kernel.org/doc/html/latest/dev-tools/kasan.html\">KASAN report</a>.</p><p>But if we assume for a moment that it’s an underflow, then the task is simple ?. We “just” need to audit all places where <code>skb-&gt;inner_transport_header</code> or <code>skb-&gt;transport_header</code> offsets could have been updated while the <code>skb</code> buffer travelled through the network stack.</p><p>That raises the question — what path did the packet take through the network stack before it brought the machine down?</p><h2 id=\"packet-path\">Packet path</h2><p>It is time to take a look at the call trace in the oops report. If we walk through it, it is apparent that a veth device received a packet. The packet then got routed and forwarded to some other network device. The kernel crashed before the egress device transmitted the packet out.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/image4-2.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>What immediately draws our attention is the <code>veth_poll()</code> function in the call trace. Polling inside a virtual device that acts as a simple pipe joining two network namespaces together? Puzzling!</p><p>The regular operation mode of a <code>veth</code> device is that transmission of a packet from one side of a <code>veth-pair</code> results in immediate, in-line, on the same CPU, reception of the packet by the other side of the pair. There shouldn't be any polling, context switches or such.</p><p>However, in Linux v4.19 veth driver <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=60afdf066a35317efd5d1d7ae7c7f4ef2b32601f\">gained support</a> for native mode <a href=\"https://docs.cilium.io/en/latest/bpf/#xdp\">eXpress Data Path (XDP)</a>. XDP relies on <a href=\"https://lwn.net/Articles/244640/\">NAPI</a>, an interface between the network drivers and the Linux network stack. NAPI requires that drivers <a href=\"https://www.kernel.org/doc/html/latest/networking/kapi.html?highlight=netif_napi_add#c.netif_napi_add\">register a <code>poll()</code> callback</a> for fetching received packets.</p><p>The NAPI receive path in the <code>veth</code> driver is taken only when there is an XDP program attached. The fork occurs in <a href=\"https://elixir.bootlin.com/linux/v5.4.14/source/drivers/net/veth.c#L228\"><code>veth_forward_skb</code></a>, where the TX path ends and a RX path on the other side begins.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/image9-4.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>This is an important observation because only on the NAPI/XDP path in the <code>veth</code> driver, received packets might get aggregated by the <a href=\"https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#generic-receive-offloading-gro\">Generic Receive Offload</a>.</p><h2 id=\"super-packets\">Super-packets</h2><p>Early on we’ve noted that the crash happens when processing a GSO packet. I’ve promised we will get back to it and now is the time.</p><p>Generic Segmentation Offload (GSO) is all about delaying the L4 segmentation process until the very last moment. So called super-packets, that exceed the egress route MTU in size, travel all the way through the network stack, only to be cut into MTU-sized segments just before handing the data over to the network driver for transmission. This way we process just one big packet on the transmit path, instead of a few smaller ones and save on CPU cycles in all the IP-level stack functions like routing, nftables, traffic control</p><p>Where do these super-packets come from? They can be a result of large write to a socket, or as is our case, they can be received from one network and forwarded to another network.</p><p>The latter case, that is forwarding a super-packet, happens when Generic Receive Offload (GRO) kicks in during receive. GRO is the opposite process of GSO. Smaller, MTU-sized packets get merged to form a super-packet early on the receive path. The goal is the same — process less by pushing just one packet through the network stack layers.</p><p>Not just any packets can be fused together by GRO. <a href=\"https://lwn.net/Articles/358910/\">Loosely speaking</a>, any two packets to be merged must form a logical sequence in the network flow, and carry the same metadata in protocol headers. It is critical that no information is lost in the aggregation process. Otherwise, GSO won’t be able to reconstruct the segment stream when serializing packets in the network card transmission code.</p><p>To this end, each network protocol that supports GRO provides a callback which signals whether the above conditions hold true. GRO implementation (<a href=\"https://elixir.bootlin.com/linux/v5.4.14/source/net/core/dev.c#L5496\"><code>dev_gro_receive()</code></a>) then walks through the packet headers, the outer as well as the inner ones, and delegates the pre-merge check to the right protocol callback. If all stars align, the packets get spliced at the end of the callback chain (<a href=\"https://elixir.bootlin.com/linux/v5.4.14/source/net/core/skbuff.c#L3986\"><code>skb_gro_receive()</code></a>).</p><!--kg-card-begin: markdown--><p>I will be frank. The code that performs GRO is pretty complex, and I spent a significant amount of time staring into it. Hat tip to its authors. However, for our little investigation it will be enough to understand that a TCP stream encapsulated with <a href=\"https://en.wikipedia.org/wiki/Generic_Routing_Encapsulation\">GRE</a><sup>1</sup> would trigger callback chain like so:</p>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/image11-3.png\" class=\"kg-image\" alt=\"GRO walks the headers to check if packets can be merged.\" loading=\"lazy\"></figure><p>Armed with basic GRO/GSO understanding we are ready to take a shot at reproducing the crash.</p><h2 id=\"the-reproducer\">The reproducer</h2><p>Let’s recap what we know:</p><ol><li>a super-packet was received from a <code>veth</code> device,</li><li>the <code>veth</code> device had an <code>XDP</code> program attached,</li><li>the packet was forwarded to another device,</li><li>the egress device was transmitting a GSO super-packet,</li><li>the packet was encapsulated,</li><li>the super-packet must have been produced by GRO on ingress.</li></ol><p>This paints a pretty clear picture on what the setup should look like:</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/image6-4.png\" class=\"kg-image\" alt=\"We have crashed on egress when transmitting forwarded GSO packet.\" loading=\"lazy\"></figure><p>We can work with that. A <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2021-10-gso-encap-crash/setup.sh\">simple shell script</a> will be our setup machinery.</p><p>We will be sending traffic from <code>10.1.1.1</code> to <code>10.2.2.2</code>. Our traffic pattern will be a TCP stream consisting of two consecutive segments so that GRO can merge something. A <a href=\"https://scapy.net/\">Scapy</a> script will be great for that. Let’s call it <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2021-10-gso-encap-crash/send-a-pair.py\"><code>send-a-pair.py</code></a> and give it a run:</p><!--kg-card-begin: markdown--><pre><code>$ { sleep 5; sudo ip netns exec A ./send-a-pair.py; } &amp;\n[1] 1603\n$ sudo ip netns exec B tcpdump -i BA -n -nn -ttt 'ip and not arp'\n…\n 00:00:00.020506 IP 10.1.1.1 &gt; 10.2.2.2: GREv0, length 1480: IP 192.168.1.1.12345 &gt; 192.168.2.2.443: Flags [.], seq 0:1436, ack 1, win 8192, length 1436\n 00:00:00.000082 IP 10.1.1.1 &gt; 10.2.2.2: GREv0, length 1480: IP 192.168.1.1.12345 &gt; 192.168.2.2.443: Flags [.], seq 1436:2872, ack 1, win 8192, length 1436\n</code></pre>\n<!--kg-card-end: markdown--><p>Where is our super-packet? Look at the packet sizes, the GRO didn’t merge anything.</p><p>Turns out NAPI is just too fast at fetching the packets from the Rx ring. We need a little buffering on transmit to increase our chances of GRO batching:</p><!--kg-card-begin: markdown--><pre><code># Help GRO\nip netns exec A tc qdisc add dev AB root netem delay 200us slot 5ms 10ms packets 2 bytes 64k\n</code></pre>\n<!--kg-card-end: markdown--><p>With the delay in place, things look better:</p><!--kg-card-begin: markdown--><pre><code> 00:00:00.016972 IP 10.1.1.1 &gt; 10.2.2.2: GREv0, length 2916: IP 192.168.1.1.12345 &gt; 192.168.2.2.443: Flags [.], seq 0:2872, ack 1, win 8192, length 2872\n</code></pre>\n<!--kg-card-end: markdown--><p>2,872 bytes shown by <code>tcpdump</code> clearly indicate GRO in action. And we are even hitting the crash point:</p><!--kg-card-begin: markdown--><pre><code>$ sudo bpftrace -e 'kprobe:skb_gso_transport_seglen { print(kstack()); }' -c '/usr/bin/ip netns exec A ./send-a-pair.py'\nAttaching 1 probe...\n\n        skb_gso_transport_seglen+1\n        skb_gso_validate_network_len+17\n        __ip_finish_output+293\n        ip_output+113\n        ip_forward+876\n        ip_rcv+188\n        __netif_receive_skb_one_core+128\n        netif_receive_skb_internal+47\n        napi_gro_flush+151\n        napi_complete_done+183\n        veth_poll+1697\n        net_rx_action+314\n        …\n\n^C\n</code></pre>\n<!--kg-card-end: markdown--><p>…but we are not crashing. We will need to dig deeper.</p><p>We know what packet metadata <code>skb_gso_transport_seglen()</code> looks at — the header offsets, then <code>encapsulation</code> flag, and GSO info. Let’s <a href=\"https://github.com/cloudflare/cloudflare-blog/blob/master/2021-10-gso-encap-crash/why-no-crash.bt\">dump</a> all of it:</p><!--kg-card-begin: markdown--><pre><code>$ sudo bpftrace ./why-no-crash.bt -c '/usr/bin/ip netns exec A ./send-a-pair.py'\nAttaching 2 probes...\nDEV  LEN  NH  TH  ENC INH ITH GSO SIZE SEGS TYPE FUNC\nsink 2936 270 290 1   294 254  |  1436 2    0x41 skb_gso_transport_seglen\n</code></pre>\n<!--kg-card-end: markdown--><p>Since the <code>skb-&gt;encapsulation</code> flag (<code>ENC</code>) is set, both outer and inner header offsets should be valid. Are they?</p><p>The outer network / L3 header (<code>NH</code>) looks sane. When XDP is enabled, it reserves 256 bytes of headroom before the headers. 14 byte long Ethernet header follows the headroom. The IPv4 header should then start at 270 bytes into the packet buffer.</p><p>The outer transport / L4 header offset is as expected as well. The IPv4 header takes 20 bytes, and the GRE header follows it.</p><p>The inner network header (<code>INH</code>) begins at the offset of 294 bytes. This makes sense because the GRE header in its most basic form is 4 bytes long.</p><p>The surprise comes last. The inner transport header offset points somewhere near the end of headroom which XDP reserves. Instead, it should start at 314, following the inner IPv4 header.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/image10-6.png\" class=\"kg-image\" alt=\"Header offsets\" loading=\"lazy\"></figure><p>Is this the smoking gun we were looking for?</p><h2 id=\"the-bug\">The bug</h2><p><code>skb_gso_transport_seglen()</code> calculates the length of the outer L4 segment when given a GSO packet. If the <code>inner_transport_header</code> offset is off, then the result of the calculation might be off as well. Worth checking.</p><p>We know that our segments are 1500 bytes long. That makes the L4 part 1480 bytes long. What does <code>skb_gso_transport_seglen()</code> say though?</p><!--kg-card-begin: markdown--><pre><code>$ sudo bpftrace -e 'kretprobe:skb_gso_transport_seglen { print(retval); }' -c …\nAttaching 1 probe...\n1460\n</code></pre>\n<!--kg-card-end: markdown--><p>Seems that we don’t agree. But if <code>skb_gso_transport_seglen()</code> is getting garbage on input we can’t really <a href=\"https://en.wikipedia.org/wiki/Garbage_in%2C_garbage_out\">blame it</a>.</p><p>If <code>inner_transport_header</code> is not correct, that TCP Data Offset read that we know happens inside the function cannot end well.</p><p>If we map it out, it looks like we are loading part of the source MAC address (upper 4 bits of the 5th byte, to be precise) and interpreting it as TCP Data Offset.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/image5-4.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Are we? There is an easy way to check.</p><p>If we ask nicely, <code>tcpdump</code> will tell us what the MAC addresses are:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/Screenshot-2021-11-03-at-14-57-51-The-tale-of-a-single-register-value---Blog-Drafts-documentation.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Plugging that into the calculations that <code>skb_gso_transport_seglen()</code> …</p><!--kg-card-begin: markdown--><pre><code>thlen = inner_transport_header(skb) - transport_header(skb) = 254 - 290 = -36\nthlen += inner_transport_header(skb)-&gt;doff * 4 = -36 + (0xf * 4) = -36 + 60 = 24\nretval = gso_size + thlen = 1436 + 24 = 1460\n</code></pre>\n<!--kg-card-end: markdown--><p>…checks out!</p><p>Does this mean that I can control the return value by setting source MAC address?!</p><!--kg-card-begin: markdown--><pre><code>                                               ?\n$ sudo ip -n A link set AB address be:d6:07:5e:05:11 # Change the MAC address \n$ sudo bpftrace -e 'kretprobe:skb_gso_transport_seglen { print(retval); }' -c …\nAttaching 1 probe...\n1400\n</code></pre>\n<!--kg-card-end: markdown--><p>Yes! <code>1436 + (-36) + (0 * 4) = 1400</code>. This is it.</p><p>However, how does all this tie it to the original crash? The badly calculated L4 segment length will make GSO emit shorter segments on egress. But that’s all.</p><p>Remember the suspicious offset from the crash report?</p><p><code>%rax = %rsi = skb-&gt;inner_transport_header - skb-&gt;transport_header = 0xfeda = 65242</code></p><p>We now know that <code>skb-&gt;transport_header</code> should be 290. That makes <code>skb-&gt;inner_transport_header = 65242 + 290 = 65532 = 0xfffc</code>.</p><p>Which means that when we triggered the page fault we were trying to load memory from a location at</p><p><code>skb-&gt;head + skb-&gt;inner_transport_header + offsetof(tcphdr, doff) = skb-&gt;head + 0xfffc + 12 = 0xffff9d9624bbb008</code></p><p>Solving it for <code>skb-&gt;head yields 0xffff9d9624bbb008 - 0xfffc - 12 = 0xffff9d9624bab000</code>.</p><p>And this makes sense. The <a href=\"http://vger.kernel.org/~davem/skb_data.html\"><code>skb-&gt;head buffer</code></a> is page-aligned, meaning it’s a multiple of 4 KiB on x86-64 platforms — the 12 least significant bits the address are 0.</p><p>However, the address we were trying to read was <code>(0xfffc+12)/4096 ~= 16 pages</code> (or 64 KiB) past the <code>skb-&gt;head</code> page boundary <code>(0xffff9d9624babfff)</code>.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://blog.cloudflare.com/content/images/2021/11/image7-5.png\" class=\"kg-image\" alt=\"Faulty read from an address beyond the packet data\" loading=\"lazy\"></figure><p>Who knows if there was memory mapped to this address?! Looks like from time to time there wasn’t anything mapped there and the kernel page fault handling code went “oops!”.</p><h2 id=\"the-fix\">The fix</h2><p>It is finally time to understand who sets the header offsets in a super-packet.</p><p>Once GRO is done merging segments, it flushes the super-packet down the pipe by kicking off a chain of <code>gro_complete</code> callbacks:</p><p><code>napi_gro_complete → inet_gro_complete → gre_gro_complete → inet_gro_complete → tcp4_gro_complete → tcp_gro_complete</code></p><p>These callbacks are responsible for updating the header offsets and populating the GSO-related fields in <code>skb_shared_info</code> struct. Later on the transmit side will consume this data.</p><!--kg-card-begin: markdown--><p>Let’s see how the packet metadata changes as it travels through the <code>gro_complete callbacks</code><sup>2</sup> by adding a few more tracepoint to our bpftrace script:</p>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code>$ sudo bpftrace ./why-no-crash.bt -c '/usr/bin/ip netns exec A ./send-a-pair.py'\nAttaching 7 probes...\nDEV  LEN  NH  TH  ENC INH ITH GSO SIZE SEGS TYPE FUNC\nBA   2936 294 314 0   254 254  |  1436 0    0x00 napi_gro_complete\nBA   2936 294 314 0   254 254  |  1436 0    0x00 inet_gro_complete\nBA   2936 294 314 0   254 254  |  1436 0    0x00 gre_gro_complete\nBA   2936 294 314 1   254 254  |  1436 0    0x40 inet_gro_complete\nBA   2936 294 314 1   294 254  |  1436 0    0x40 tcp4_gro_complete\nBA   2936 294 314 1   294 254  |  1436 0    0x41 tcp_gro_complete\nsink 2936 270 290 1   294 254  |  1436 2    0x41 skb_gso_transport_seglen\n</code></pre>\n<!--kg-card-end: markdown--><p>As the packet travels through the <code>gro_complete</code> callbacks, the inner network header (<code>INH</code>) offset <a href=\"https://elixir.bootlin.com/linux/v5.4.14/source/net/ipv4/af_inet.c#L1591\">gets updated</a> after we have processed the inner IPv4 header.</p><p>However, the same thing did not happen to the inner transport header (ITH) that is causing trouble. We need to fix that.</p><!--kg-card-begin: markdown--><pre><code>--- a/net/ipv4/tcp_offload.c\n+++ b/net/ipv4/tcp_offload.c\n@@ -298,6 +298,9 @@ int tcp_gro_complete(struct sk_buff *skb)\n        if (th-&gt;cwr)\n                skb_shinfo(skb)-&gt;gso_type |= SKB_GSO_TCP_ECN;\n\n+       if (skb-&gt;encapsulation)\n+               skb-&gt;inner_transport_header = skb-&gt;transport_header;\n+\n        return 0;\n }\n EXPORT_SYMBOL(tcp_gro_complete);\n</code></pre>\n<!--kg-card-end: markdown--><p>With the patch in place, the header offsets are finally all sane and <code>skb_gso_transport_seglen()</code> return value is as expected:</p><!--kg-card-begin: markdown--><pre><code>$ sudo bpftrace ./why-no-crash.bt -c '/usr/bin/ip netns exec A ./send-a-pair.py'\nAttaching 2 probes...\nDEV  LEN  NH  TH  ENC INH ITH GSO SIZE SEGS TYPE FUNC\nsink 2936 270 290 1   294 314  |  1436 2    0x41 skb_gso_transport_seglen\n\n$ sudo bpftrace -e 'kretprobe:skb_gso_transport_seglen { print(retval); }' -c …\nAttaching 1 probe...\n1480\n</code></pre>\n<!--kg-card-end: markdown--><p>Don’t worry, though. The fix is already likely in your kernel long time ago. Patch <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=d51c5907e9809a803b276883d203f45849abd4d6\">d51c5907e980 (“net, gro: Set inner transport header offset in tcp/udp GRO hook”)</a> has been merged into Linux v5.14, and backported to v5.10.58 and v5.4.140 LTS kernels. The Linux kernel community has got you covered. But please, keep on updating your production kernels.</p><h2 id=\"outro\">Outro</h2><p>What a journey! We have learned a ton and fixed a real bug in the Linux kernel. In the end it was not a Packet of Death. Maybe next time we can find one ;-)</p><p>Enjoyed the read? Why not join Cloudflare and help us fix the remaining bugs in the Linux kernel? We are hiring in <a href=\"https://boards.greenhouse.io/cloudflare/jobs/3547091?gh_jid=3547091\">Lisbon</a>, <a href=\"https://boards.greenhouse.io/cloudflare/jobs/3550616?gh_jid=3550616\">London</a>, and <a href=\"https://boards.greenhouse.io/cloudflare/jobs/3550623?gh_jid=3550623\">Austin</a>.</p><p>And if you would like to see more kernel blog posts, please let us know!</p><!--kg-card-begin: markdown--><p>...<br>\n<sup>1</sup>Why GRE and not some other type of encapsulation? If you follow our blog closely, you might already know that <a href=\"http://blog.cloudflare.com/magic-transit-network-functions/\">Cloudflare Magic Transit</a> uses veth pairs to route traffic into and out of network namespaces. It also happens to use GRE encapsulation. If you are curious why we chose network namespaces linked with veth pairs, be sure to watch the <a href=\"https://linuxplumbersconf.org/event/7/contributions/682/\">How we built Magic Transit</a>  talk.<br>\n<sup>2</sup>Just turn off GRO on all other network devices in use to get a clean output (sudo ethtool -K enp0s5 gro off).</p>\n<!--kg-card-end: markdown-->",
		"id": "618270043d6e9c02a9439a2a",
		"meta_description": "It’s not every day that you get to debug what may well be a packet of death. It was certainly the first time for me.\nWhat do I mean by “a packet of death”? A software bug where the network stack crashes in reaction to a single received network packet, taking down the whole operating system with it. ",
		"meta_title": null,
		"og_description": null,
		"og_image": "http://blog.cloudflare.com/content/images/2021/11/The-tale-of-a-single-register-value-OG-1.png",
		"og_title": null,
		"primary_author": {
			"id": "5d1644b141acde0011a9501b",
			"name": "Jakub Sitnicki",
			"slug": "jakub",
			"profile_image": "http://blog.cloudflare.com/content/images/2019/05/4_300698554542850065.jpg",
			"cover_image": "http://blog.cloudflare.com/content/images/2019/05/general@2x-2.png",
			"bio": null,
			"website": null,
			"location": null,
			"facebook": null,
			"twitter": null,
			"meta_title": null,
			"meta_description": null,
			"url": "http://blog.cloudflare.com/author/jakub/"
		},
		"primary_tag": null,
		"published_at": "2021-11-03T14:37:10.000+00:00",
		"reading_time": 16,
		"slug": "the-tale-of-a-single-register-value",
		"tags": [
			{
				"id": "618272313d6e9c02a9439a2f",
				"name": "#BLOG-762",
				"slug": "hash-blog-762",
				"description": null,
				"feature_image": null,
				"visibility": "internal",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/404/"
			},
			{
				"id": "5d16450341acde0011a9531b",
				"name": "Deep Dive",
				"slug": "deep-dive",
				"description": null,
				"feature_image": "http://blog.cloudflare.com/content/images/2023/08/CloudflareBlog-DeepDive.png",
				"visibility": "public",
				"meta_title": "Cloudflare Blog: Deep Dive",
				"meta_description": "Collection of Cloudflare blog posts tagged 'Deep Dive'.",
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/deep-dive/"
			}
		],
		"title": "The tale of a single register value",
		"twitter_description": null,
		"twitter_image": "http://blog.cloudflare.com/content/images/2021/11/The-tale-of-a-single-register-value-OG.png",
		"twitter_title": null,
		"updated_at": "2021-12-01T09:45:19.000+00:00",
		"url": "http://blog.cloudflare.com/the-tale-of-a-single-register-value/",
		"uuid": "7117ab33-1fe2-4ece-92f5-6a985440c0b0",
		"visibility": "public"
	}
}