<div class="post-content lh-copy gray1">
	<!--kg-card-begin: markdown-->
	<p><small>This post is also available in <a href="https://blog.cloudflare.com/fr-fr/zero-trust-ai-security-fr-fr">Français</a>, <a href="https://blog.cloudflare.com/es-es/zero-trust-ai-security-es-es">Español</a>, <a href="https://blog.cloudflare.com/de-de/zero-trust-ai-security-de-de">Deutsch</a>, <a href="https://blog.cloudflare.com/ja-jp/zero-trust-ai-security-ja-jp">日本語</a>, <a href="https://blog.cloudflare.com/pt-br/zero-trust-ai-security-pt-br/">Português</a> and <a href="https://blog.cloudflare.com/zh-cn/zero-trust-ai-security-zh-cn">简体中文</a>.</small></p>
	<!--kg-card-end: markdown-->
	<p><strong>A collection of tools from Cloudflare One to help your teams use AI services safely</strong></p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/05/image1-18.png" class="kg-image" alt="A complete suite of Zero Trust security tools to get the most from AI" loading="lazy" width="1801" height="1013"></figure>
	<p>Cloudflare One gives teams of any size the ability to safely use the best tools on the Internet without management headaches or performance challenges. We’re excited to announce Cloudflare One for AI, a new collection of features that help your team build with the latest AI services while still maintaining a Zero Trust security posture.</p>
	<h3 id="large-language-models-larger-security-challenges">Large Language Models, Larger Security Challenges</h3>
	<p>A Large Language Model (LLM), like OpenAI’s GPT or Google’s Bard, consists of a neural network trained against a set of data to predict and generate text based on a prompt. Users can ask questions, solicit feedback, and lean on the service to create output from poetry to <a href="https://blog.samrhea.com/posts/2022/five-minute-ai-site?ref=blog.cloudflare.com" target="_blank">Cloudflare Workers applications</a>.</p>
	<p>The tools also bear an uncanny resemblance to a real human. As in some real-life personal conversations, oversharing can become a <a href="https://mashable.com/article/samsung-chatgpt-leak-details?ref=blog.cloudflare.com" target="_blank">serious problem</a> with these AI services. This risk multiplies due to the types of use cases where LLM models thrive. These tools can help developers solve difficult coding challenges or information workers create succinct reports from a mess of notes. While helpful, every input fed into a prompt becomes a piece of data leaving your organization’s control.</p>
	<p>Some responses to tools like ChatGPT have been to try and ban the service outright; either at a corporate level or across an <a href="https://www.reuters.com/technology/germany-principle-could-block-chat-gpt-if-needed-data-protection-chief-2023-04-03/?ref=blog.cloudflare.com" target="_blank">entire nation</a>. We don’t think you should have to do that. Cloudflare One’s goal is to allow you to safely use the tools you need, wherever they live, without compromising performance. These features will feel familiar to any existing use of the Zero Trust products in Cloudflare One, but we’re excited to walk through cases where you can use the tools available right now to allow your team to take advantage of the latest LLM features.</p>
	<h3 id="measure-usage">Measure usage</h3>
	<p>SaaS applications make it easy for any user to sign up and start testing. That convenience also makes these tools a liability for IT budgets and security policies. Teams refer to this problem as “<a href="https://blog.cloudflare.com/introducing-shadow-it-discovery/">Shadow IT</a>” - the adoption of applications and services outside the approved channels in an organization.</p>
	<p>In terms of budget, we have heard from early adopter customers who know that their team members are beginning to experiment with LLMs, but they are not sure how to approach making a commercial licensing decision. What services and features do their users need and how many seats should they purchase?</p>
	<p>On the security side, the AIs can be revolutionary for getting work done but terrifying for data control policies. Team members treat these AIs like sounding boards for painful problems. The services invite users to come with their questions or challenges. Sometimes the context inside those prompts can contain sensitive information that should never leave an organization. Even if teams select and approve a single vendor, members of your organization might prefer another AI and continue to use it in their workflow.</p>
	<p>Cloudflare One customers on any plan <a href="https://developers.cloudflare.com/cloudflare-one/analytics/access/?ref=blog.cloudflare.com" target="_blank">can now review</a> the usage of AIs. Your IT department can deploy Cloudflare Gateway and passively observe how many users are selecting which services as a way to start scoping out enterprise licensing plans.</p>
	<figure class="kg-card kg-image-card kg-width-wide"><img src="https://blog.cloudflare.com/content/images/2023/05/image6-5.png" class="kg-image" alt="" loading="lazy" width="1879" height="682"></figure>
	<p>Administrators can also block the use of these services with a single click, but that is not our goal today. You might want to use this feature if you select ChatGPT as your approved model, and you want to make sure team members don’t continue to use alternatives, but we hope you don’t block all of these services outright. Cloudflare’s priority is to give you the ability to use these tools safely.</p>
	<h3 id="control-api-access">Control API access</h3>
	<p>When our teams began experimenting with OpenAI’s ChatGPT service, we were astonished by what it already knew about Cloudflare. We asked ChatGPT to create applications with <a href="https://workers.cloudflare.com/?ref=blog.cloudflare.com" target="_blank">Cloudflare Workers</a> or guide us through how to configure a <a href="https://www.cloudflare.com/products/zero-trust/access/?ref=blog.cloudflare.com" target="_blank">Cloudflare Access</a> policy and, in most cases, the results were accurate and helpful.</p>
	<p>In some cases the results missed the mark. The AIs were using outdated information, or we were asking questions about features that had only launched recently. Thankfully, these AIs can learn and we can help. We can train these models with scoped inputs and <a href="https://openai.com/blog/chatgpt-plugins?ref=blog.cloudflare.com" target="_blank">connect plug-ins</a> to provide our customers with better AI-guided experiences when using Cloudflare services.</p>
	<p>We heard from customers who want to do the same thing and, like us, they need to securely share training data and grant plug-in access for an AI service. Cloudflare One’s security suite extends beyond human users and can give teams the ability to securely share Zero Trust access to sensitive data over APIs.</p>
	<p>First, teams can create <a href="https://developers.cloudflare.com/cloudflare-one/identity/service-tokens/?ref=blog.cloudflare.com" target="_blank">service tokens</a> that external services must present to reach data made available through Cloudflare One. Administrators can provide these tokens to systems making API requests and log every single request. As needed, teams can revoke these tokens with a single click.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/05/download-7.png" class="kg-image" alt="" loading="lazy" width="1600" height="1317"></figure>
	<p>After creating and issuing service tokens, administrators can create policies to allow specific services access to their training data. These policies will verify the service token and can be extended to verify country, IP address or an mTLS certificate. Policies can also be created to require human users to authenticate with an identity provider and complete an MFA prompt before accessing sensitive training data or services.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/05/download--1--4.png" class="kg-image" alt="" loading="lazy" width="1600" height="1100"></figure>
	<p>When teams are ready to allow an AI service to connect to their infrastructure, they can do so without poking holes in their firewalls by using <a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/?ref=blog.cloudflare.com" target="_blank">Cloudflare Tunnel</a>. Cloudflare Tunnel will create an encrypted, outbound-only connection to Cloudflare’s network where every request will be checked against the access rules configured for one or more services protected by Cloudflare One.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/05/download--2--3.png" class="kg-image" alt="" loading="lazy" width="1600" height="1100"></figure>
	<p>Cloudflare’s Zero Trust access control gives you the ability to enforce authentication on each and every request made to the data your organization decides to provide to these tools. That still leaves a gap in the data your team members might overshare on their own.</p>
	<h3 id="restrict-data-uploads">Restrict data uploads</h3>
	<p>Administrators can select an AI service, block Shadow IT alternatives, and carefully gate access to their training material, but humans are still involved in these AI experiments. Any one of us can accidentally cause a security incident by oversharing information in the process of using an AI service - even an approved service.</p>
	<p>We expect AI playgrounds to continue to evolve to feature more data management capabilities, but we don’t think you should have to wait for that to begin adopting these services as part of your workflow. <a href="https://developers.cloudflare.com/cloudflare-one/policies/data-loss-prevention/dlp-policies/?ref=blog.cloudflare.com" target="_blank">Cloudflare’s Data Loss Prevention</a> (DLP) service can provide a safeguard to stop oversharing before it becomes an incident for your security team.</p>
	<p>First, tell us what data you care about. We provide simple, preconfigured options that give you the ability to check for things that look like social security numbers or credit card numbers. Cloudflare DLP can also scan for patterns based on regular expressions configured by your team.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/05/download--3--3.png" class="kg-image" alt="" loading="lazy" width="1600" height="1100"></figure>
	<p>Once you have defined the data that should never leave your organization, you can build granular rules about how it can and cannot be shared with AI services. Maybe some users are approved to experiment with projects that contain sensitive data, in which case you can build a rule that only allows an Active Directory or Okta group to upload that kind of information while everyone else is blocked.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2023/05/download--4--3.png" class="kg-image" alt="" loading="lazy" width="1600" height="1100"></figure>
	<h3 id="control-use-without-a-proxy">Control use without a proxy</h3>
	<p>The tools in today’s blog post focus on features that apply to data-in-motion. We also want to make sure that misconfigurations in the applications don’t lead to security violations. For example, the new plug-in feature in ChatGPT brings the knowledge and workflows of external services into the AI interaction flow. However, that can also lead to the services behind plug-ins having more access than you want to.</p>
	<p>Cloudflare’s <a href="https://www.cloudflare.com/products/zero-trust/casb/?ref=blog.cloudflare.com" target="_blank">Cloud Access Security Broker</a> (CASB) scans your SaaS applications for potential issues that can occur when users make changes. Whether alerting you to files that someone accidentally just made public on the Internet to checking that your <a href="https://developers.cloudflare.com/cloudflare-one/applications/scan-apps/casb-integrations/github/?ref=blog.cloudflare.com#security-findings" target="_blank">GitHub repositories have the right membership controls</a>, Cloudflare’s CASB removes the manual effort required to check each and every setting for potential issues in your SaaS applications.</p>
	<p>Available soon, we are working on new integrations with popular AI services to check for misconfigurations. Like most users of these services, we’re still learning more about where potential accidents can occur, and we are excited to provide administrators who use our CASB with our first wave of controls for AI services.</p>
	<h3 id="what%E2%80%99s-next">What’s next?</h3>
	<p>The usefulness of these tools will only accelerate. The ability of AI services to coach and generate output will continue to make it easier for builders from any background to create the next big thing.</p>
	<p>We share a similar goal. The Cloudflare products focused on helping users build applications and services, our <a href="https://workers.cloudflare.com/?ref=blog.cloudflare.com" target="_blank">Workers platform</a>, remove hassles like worrying about where to deploy your application or how to scale your services. Cloudflare solves those headaches so that users can focus on creating. Combined with the AI services, we expect to see thousands of new builders launch the next wave of products built on Cloudflare and inspired by AI coaching and generation.</p>
	<p>We have already seen dozens of projects flourish that were built on Cloudflare Workers using guidance from tools like ChatGPT. We plan to launch new integrations with these models to make this even more seamless, bringing better Cloudflare-specific guidance to the chat experience.</p>
	<p>We also know that the security risk of these tools will grow. We will continue to bring functionality into Cloudflare One that aims to stay one step ahead of the risks as they evolve with these services. Ready to get started? Sign up here to begin using Cloudflare One at no cost for teams of up to 50 users.</p>
</div>