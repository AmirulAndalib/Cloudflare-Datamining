{
	"locale": "en-us",
	"post": {
		"access": true,
		"authors": [
			{
				"id": "5d1644b141acde0011a94f96",
				"name": "Guest Author",
				"slug": "guest-author",
				"profile_image": "https://blog-cloudflare-com-assets.storage.googleapis.com/2019/10/Screen-Shot-2019-10-09-at-11.21.59-AM.png",
				"cover_image": "http://blog.cloudflare.com/content/images/2019/05/general@2x-9.png",
				"bio": null,
				"website": null,
				"location": null,
				"facebook": null,
				"twitter": null,
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/guest-author/"
			}
		],
		"canonical_url": null,
		"codeinjection_foot": null,
		"codeinjection_head": null,
		"comment_id": "5f80308308dda903f3414a81",
		"comments": false,
		"created_at": "2020-10-09T10:42:27.000+01:00",
		"custom_excerpt": "Edge networks present a significant opportunity for Artificial Intelligence (AI) performance and applicability. AI technologies already make it possible to run compelling applications like object and voice recognition, navigation, and recommendations.",
		"custom_template": null,
		"email_subject": null,
		"excerpt": "Edge networks present a significant opportunity for Artificial Intelligence (AI) performance and applicability. AI technologies already make it possible to run compelling applications like object and voice recognition, navigation, and recommendations.",
		"feature_image": "http://blog.cloudflare.com/content/images/2020/10/image3-4.png",
		"feature_image_alt": null,
		"feature_image_caption": null,
		"featured": false,
		"frontmatter": null,
		"html": "<p><em>This is a guest post by Videet Parekh, Abelardo Lopez-Lagunas, Sek Chai at <a href=\"https://www.latentai.com\">Latent AI</a>.</em></p><p>Edge networks present a significant opportunity for Artificial Intelligence (AI) performance and applicability. AI technologies already make it possible to run compelling applications like object and voice recognition, navigation, and recommendations.</p><p><a href=\"https://latentai.com/scaling-edge-ai-in-a-data-driven-world-part-2/\">AI at the edge</a> presents a host of benefits. One is scalability—it is simply impractical to send all data to a centralized cloud. In fact, <a href=\"https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf\">one study</a> has predicted a global scope of 90 zettabytes generated by billions of IoT devices by 2025. Another is privacy—many users are reluctant to move their personal data to the cloud, whereas data processed at the edge are more ephemeral.</p><p>When AI services are distributed away from centralized data centers and closer to the service edge, it becomes possible to enhance the overall application speed without moving data unnecessarily.  However, there are still challenges to make AI from the deep-cloud run efficiently on edge hardware. Here, we use the term deep-cloud to refer to highly centralized, massively-sized data centers. Deploying edge AI services can be hard because AI is both computational and memory bandwidth intensive. We need to tune the AI models so the computational latency and bandwidth can be radically reduced for the edge.</p><h3 id=\"the-case-for-distributed-ai-services\">The Case for Distributed AI Services</h3><p>Edge network infrastructure for distributed AI is already widely available. Edge networks like Cloudflare serve a significant proportion of today’s Internet traffic, and can serve as the bridge between devices and the centralized cloud. Highly-performant AI services are possible because of the distributed processing that has excellent spatial proximity to the edge data.</p><p>We at Latent AI are exploring ways to deploy AI at the edge, with technology that transforms and compresses AI models for the edge. The size of our edge AI model is many orders of magnitudes smaller than the sensor data (e.g., kilobytes or megabytes for the edge AI model, compared to petabytes of edge data). We are exploring using WebAssembly (WASM) within the Cloudflare Workers environment. We want to identify possible operating points for the distributed AI services by exploring achievable performance on the available edge infrastructure.</p><h3 id=\"architectural-approach-for-exploration\">Architectural Approach for Exploration</h3><p><a href=\"https://webassembly.org/\">WebAssembly</a> (WASM) is a new open-standard format for programs that run on the Web. It is a popular way to enable high-performance web-based applications. WASM is closer to machine code, and thus <a href=\"https://hacks.mozilla.org/2017/02/what-makes-webassembly-fast/\">faster</a> than JavaScript (JS) or JIT. Compiler optimizations, already done ahead of time, reduce the overhead in fetching and parsing application code. Today, WASM offers the flexibility and portability of JS at the near-optimum performance of compiled machine code.</p><p>AI models have notoriously large memory usage demands because configuring them requires high parameter counts. Cloudflare already extends support for WASM using their <a href=\"https://developers.cloudflare.com/workers/quickstart\">Wrangler CLI</a>, and we chose to use it for our exploration. <a href=\"http://blog.cloudflare.com/improving-the-wrangler-startup-experience/\">Wrangler</a> is the open-source CLI tool used to manage Workers, and is designed to enable a smooth developer experience.</p><h3 id=\"how-latent-ai-accelerates-distributed-ai-services\">How Latent AI Accelerates Distributed AI Services</h3><p><a href=\"https://latentai.com/\">Latent AI</a>’s mission is to enable ambient computing, regardless of any resource constraints. We develop developer tools that greatly reduce the computing resources needed to process AI on the edge while being completely hardware-agnostic.</p><p>Latent AI’s tools significantly compress AI models to reduce their memory size. We have shown up to 10x compression for state-of-the-art models. This capability addresses the load time latencies challenging many edge network deployments. We also offer an optimized runtime that executes a neural network natively. Results are a 2-3x speedup on runtime without any hardware-specific accelerators. This dramatic performance boost offers fast and efficient inferences for the edge.</p><p>Our compression uses quantization algorithms to convert parameters for the AI model from 32-bit floating-point toward 16-bit or 8-bit models, with minimal loss of accuracy. The key benefit of moving to lower bit-precision is the higher power efficiency with less storage needed.  Now AI inference can be processed using more efficient parallel processor hardware on the continuum of platforms at the distributed edge. </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://blog.cloudflare.com/content/images/2020/10/image1-8.png\" class=\"kg-image\" alt loading=\"lazy\"><figcaption>Optimized AI services can process data closest to the source and perform inferences at the distributed edge.</figcaption></figure><h3 id=\"selecting-real-world-wasm-neural-network-examples\">Selecting Real-World WASM Neural Network Examples</h3><p>For our exploration, we use state-of-the-art deep neural networks called MobileNet. MobileNets are designed specifically for embedded platforms such as smartphones, and can achieve high recognition accuracy in visual object detection. We compress MobileNets AI models to be small fast, in order to represent the variety of use cases that can be deployed as distributed AI services. Please see this <a href=\"https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c\">blog</a> for more details on the AI model architecture. </p><p>We use the MobileNetV2 model variant for our exploration. The models are trained with different visual objects that can be detected: (1) a larger sized model with 10 objects derived from <a href=\"https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/\">ImageNet</a> dataset, and (2) a smaller version with just two classes derived from the <a href=\"https://cocodataset.org/#home\">COCO</a> dataset. The COCO dataset are public open-source databases of images that are used as benchmarks for AI models. Images are labeled with detected objects such as persons, vehicles, bicycles, traffic lights, etc. Using Latent AI’s <a href=\"https://latentai.com/how-latent-ais-leip-sdk-improves-deep-learning-inference-efficiency/\">compression tool</a>, we were able to compress and compile the MobileNetV2 models into WASM programs. In the WASM form, we can achieve fast and efficient processing of the AI model with a small storage footprint.</p><p>We want WASM neural networks to be as fast and efficient as possible. We spun up a Workers app to accept an image from a client, convert and preprocess the image into a cleaned data array, run it through the model and then return a class for that image. For both the large and small MobileNetv2 models, we create three variants with different bit-precision (32-bit floating point, 16-bit integer, and 8-bit integer).  The average memory and inference times for the large AI model are 110ms and 189ms, respectively; And for the smaller AI model, they are 159ms and 15ms, respectively.</p><p>Our analysis suggests that overall processing can be improved by reducing the overhead for memory operations. For the large model, lowering bit precision to 8-bits reduces memory operations from 48% to 26%. For the small model, the memory load times dominate over the inference computation with over 90% of the latency in memory operations.</p><p>It is important to note that our results are based on our initial exploration, which is focused more on functionality rather than optimization. We make sure the results are consistent by averaging our measurements over 50-100 iterations. We do acknowledge that there are still network and system related latencies that can be further optimized, but we believe that the early results described here show promise with respect to AI model inferences on the distributed edge.  </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://blog.cloudflare.com/content/images/2020/10/image3-5.png\" class=\"kg-image\" alt loading=\"lazy\"><figcaption>Comparison of memory and inference processing times for large and small DNNs.</figcaption></figure><h3 id=\"learning-from-real-world-wasm-neural-network-example\">Learning from Real-World WASM Neural Network Example</h3><p>What lessons can we draw from our example use case?</p><p>First of all, we recommend a minimal compute and memory footprint for AI models deployed to the network edge. A small footprint allows for better line up of data types for WASM AI models to reduce memory load overhead. WASM practitioners know that WASM speed-ups come from the tighter coupling of the API between JavaScript API and native machine code. Because WASM code does not need to speculate on data types, parallelizing compilation for WASM can achieve better optimization.</p><p>Furthermore, we encourage the use of running AI models at 8-bit precision to reduce the overall size. These 8-bit AI models are readily compressed and compiled for the target hardware to greatly reduce the overhead in hosting the models for inference. Furthermore, for video imagery, there is no overhead to convert digitized raw data (e.g. image files digitized and stored as integers) to floating-point values for use with floating point AI models.</p><p>Finally, we suggest the use of a smart cache for AI models so that Workers can essentially reduce memory load times and focus solely on neural network inferences at runtime. Again, 8-bit models allow more AI models to be hosted and ready for inference. Referring to our exploratory results, hosted small AI models can be served at approximately 15ms inference time, offering a very compelling user experience with low latency and local processing. The WASM API provides a significant performance increase over pure-JS toolchains like Tensorflow.js. For example, for inference time for the large AI model of 189ms on WASM, we have observed a range of 1500ms on Tensorflow.js workflow, which is approximately an 8X difference in compute latency.</p><h3 id=\"unlocking-the-future-of-the-distributed-edge\">Unlocking the Future of the Distributed Edge</h3><p>With exceedingly optimized WASM neural networks, distributed edge networks can move the inference closer to users, offering new edge AI services closer to the source of the data. With Latent AI technology to compress and compile WASM neural networks, the distributed edge networks can (1) host more models, (2) offer lower latency responses, and (3) potentially lower power utilization with more efficient computing. </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://blog.cloudflare.com/content/images/2020/10/image2-4.png\" class=\"kg-image\" alt loading=\"lazy\"><figcaption>Example person detected using a small AI model, 10x compressed to 150KB.</figcaption></figure><p>Imagine for example that the small AI model described earlier can distinguish if a person is in a video feed. Digital systems, e.g. door bell and doorway entry cameras, can hook up to Cloudflare Workers to verify if a person is present in the camera field of view. Similarly, other AI services could conduct sound analyses to check for broken windows  and water leaks. With these distributed AI services, applications can run without access to deep cloud services. Furthermore, the sensor platform can be made with ultra low cost, low power hardware, in very compact form factors.</p><p>Application developers can now offer AI services with neural networks trained, compressed, and compiled natively as a WASM neural network. Latent AI developer tools can compress WASM neural networks and provide WASM runtimes offering blazingly fast inferences for the device and infrastructure edge.  With scale and speed baked in, developers can easily create high-performance experiences for their users, wherever they are, at any scale. More importantly, we can scale enterprise applications on the edge, while offering the desired return on investments using edge networks.</p><h3 id=\"about-latent-ai\">About Latent AI</h3><p>Latent AI is an early-stage venture spinout of SRI International. Our mission is to enable developers and change the way we think about building AI for the edge. We develop software tools designed to help companies add AI to edge devices and to empower users with new smart IoT applications. For more information about the availability of LEIP SDK, please feel free to contact us at <a href=\"mailto:info@latentai.com\">info@latentai.com</a> or check out our <a href=\"https://latentai.com/\">website</a>.</p>",
		"id": "5f80308308dda903f3414a81",
		"meta_description": "Latent AI developer tools can compress WASM neural networks and provide WASM runtimes offering blazingly fast inferences for the device and infrastructure edge. ",
		"meta_title": null,
		"og_description": null,
		"og_image": "http://blog.cloudflare.com/content/images/2020/10/facebook-link-image-7.png",
		"og_title": null,
		"primary_author": {
			"id": "5d1644b141acde0011a94f96",
			"name": "Guest Author",
			"slug": "guest-author",
			"profile_image": "https://blog-cloudflare-com-assets.storage.googleapis.com/2019/10/Screen-Shot-2019-10-09-at-11.21.59-AM.png",
			"cover_image": "http://blog.cloudflare.com/content/images/2019/05/general@2x-9.png",
			"bio": null,
			"website": null,
			"location": null,
			"facebook": null,
			"twitter": null,
			"meta_title": null,
			"meta_description": null,
			"url": "http://blog.cloudflare.com/author/guest-author/"
		},
		"primary_tag": {
			"id": "5d16450341acde0011a95253",
			"name": "Cloudflare Workers",
			"slug": "workers",
			"description": null,
			"feature_image": null,
			"visibility": "public",
			"meta_title": null,
			"meta_description": null,
			"og_image": null,
			"og_title": null,
			"og_description": null,
			"twitter_image": null,
			"twitter_title": null,
			"twitter_description": null,
			"codeinjection_head": null,
			"codeinjection_foot": null,
			"canonical_url": null,
			"accent_color": null,
			"url": "http://blog.cloudflare.com/tag/workers/"
		},
		"published_at": "2020-10-09T12:00:00.000+01:00",
		"reading_time": 7,
		"slug": "exploring-webassembly-ai-services-on-cloudflare-workers",
		"tags": [
			{
				"id": "5d16450341acde0011a95253",
				"name": "Cloudflare Workers",
				"slug": "workers",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/workers/"
			},
			{
				"id": "5d16450341acde0011a95252",
				"name": "Serverless",
				"slug": "serverless",
				"description": null,
				"feature_image": "http://blog.cloudflare.com/content/images/2020/10/Serverless.png",
				"visibility": "public",
				"meta_title": "Cloudflare Blog: Serverless",
				"meta_description": "Cloudflare blog posts tagged 'serverless'.",
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/serverless/"
			}
		],
		"title": "Exploring WebAssembly AI Services on Cloudflare Workers",
		"twitter_description": null,
		"twitter_image": "http://blog.cloudflare.com/content/images/2020/10/twitter-shared-link-6.png",
		"twitter_title": null,
		"updated_at": "2020-10-09T16:23:53.000+01:00",
		"url": "http://blog.cloudflare.com/exploring-webassembly-ai-services-on-cloudflare-workers/",
		"uuid": "371dd3ea-2462-4b16-90c9-a2c82c6cb75f",
		"visibility": "public"
	}
}