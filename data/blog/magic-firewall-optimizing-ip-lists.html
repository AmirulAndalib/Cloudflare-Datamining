<div class="mb2 gray5">7 min read</div>
<div class="post-content lh-copy gray1">
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/03/image6-10.png" class="kg-image" alt="Optimizing Magic Firewallâ€™s IP Lists" loading="lazy" title="Optimizing Magic Firewallâ€™s IP Lists"></figure>
	<p>Magic Firewall is Cloudflareâ€™s replacement for network-level firewall hardware. It evaluates gigabits of traffic every second against user-defined rules that can include millions of IP addresses. Writing a firewall rule for each IP address is cumbersome and verbose, so we have been building out support for various <a href="https://developers.cloudflare.com/magic-firewall/about/list-types">IP lists</a> in Magic Firewallâ€”essentially named groups that make the rules easier to read and write. Some users want to reject packets based on our growing <a href="https://blog.cloudflare.com/magic-firewall-gets-smarter">threat intelligence</a> of bad actors, while others know the exact set of IPs they want to match, which Magic Firewall supports via the same <a href="https://api.cloudflare.com/#rules-lists-create-list">API</a> as Cloudflareâ€™s WAF.</p>
	<p>With all those IPs, the system was using more of our memory budget than weâ€™d like. To understand why, we need to first peek behind the curtain of our magic.</p>
	<h3 id="life-inside-a-network-namespace">Life inside a network namespace</h3>
	<p><a href="https://developers.cloudflare.com/magic-transit">Magic Transit</a> and <a href="https://blog.cloudflare.com/magic-wan-firewall">Magic WAN</a> enable Cloudflare to route layer 3 traffic, and they are the front door for Magic Firewall. We have <a href="https://blog.cloudflare.com/magic-transit-network-functions">previously written</a> about how Magic Transit uses network namespaces to route packets and isolate customer configuration. Magic Firewall operates inside these namespaces, using <a href="https://wiki.nftables.org">nftables</a> as the primary implementation of packet filtering.</p>
	<figure class="kg-card kg-image-card"><img src="https://blog.cloudflare.com/content/images/2022/03/image1-111.png" class="kg-image" alt="Optimizing Magic Firewallâ€™s IP Lists" loading="lazy" title="Magic Firewall Architecture"></figure>
	<p>When a user makes an API request to configure their firewall, a daemon running on every server detects the change and makes the corresponding changes to nftables. Because nftables configuration is stored within the namespace, each userâ€™s firewall rules are isolated from the next. Every Cloudflare server routes traffic for all of our customers, so itâ€™s important that the firewall only applies a customerâ€™s rules to their own traffic.</p>
	<p>Using our existing technologies, we built IP lists using <a href="https://wiki.nftables.org/wiki-nftables/index.php/Sets">nftables sets</a>. In practice, this means that the <a href="https://developers.cloudflare.com/ruleset-engine/rules-language">wirefilter</a> expression</p><!--kg-card-begin: markdown-->
	<pre><code>ip.geoip.country == â€œUSâ€
</code></pre>
	<!--kg-card-end: markdown-->
	<p>is implemented as</p><!--kg-card-begin: markdown-->
	<pre><code>table ip mfw {
	set geoip.US {
		typeof ip saddr
		elements = { 192.0.2.0, â€¦ }
	}
	chain input {
		ip saddr @geoip.US block
	}
}
</code></pre>
	<!--kg-card-end: markdown-->
	<p>This design for the feature made it very easy to extend our wirefilter syntax so that <a href="https://developers.cloudflare.com/magic-firewall/how-to/use-rules-list">users could write rules</a> using all the different IP lists we support.</p>
	<h3 id="scaling-to-many-customers">Scaling to many customers</h3>
	<p>We chose this design since sets fit easily into our existing use of nftables. We shipped quickly with just a few, relatively small lists to a handful of customers. This approach worked well at first, but we eventually started to observe a dramatic increase in our systemâ€™s memory use. Itâ€™s common practice for our team to enable a new feature for just a few customers at first. Hereâ€™s what we observed as those customers started using IP lists:</p>
	<figure class="kg-card kg-image-card kg-width-wide"><img src="https://blog.cloudflare.com/content/images/2022/03/image3-45.png" class="kg-image" alt="Kmalloc64 slab memory Q4" loading="lazy" title="Graph of Kmalloc64 Slab Memory"></figure>
	<p>It turns out you canâ€™t store millions of IPs in the kernel without someone noticing, but the real problem is that we pay that cost for each customer that uses them. Each network namespace gets a complete copy of all the lists that its rules reference. Just to make it perfectly clear, if 10 users have a rule that uses the anonymizer list, then a server has 10 copies of that list stored in the kernel. Yikes!</p>
	<p>And it isnâ€™t just the storage of these IPs that caused alarm; the sheer size of the nftables configuration causes nft (the command-line program for configuring nftables) to use quite a bit of compute power.</p>
	<figure class="kg-card kg-image-card kg-width-wide"><img src="https://blog.cloudflare.com/content/images/2022/03/image5-25.png" class="kg-image" alt="CPU time during nftables updates" loading="lazy" title="Graph of CPU Time"></figure>
	<p>Can you guess when the nftables updates take place? Â&nbsp;Now the CPU usage wasnâ€™t particularly problematic, as the service is already quite lean. However, those spikes also create competition with other services that heavily use netlink sockets. At one point, a monitoring alert for another service started firing for high CPU, and a fellow development team went through the incident process only to realize our system was the primary contributor to the problem. They made use of the <code>-terse</code> flag to prevent nft from wasting precious time rendering huge lists of IPs, but degrading another teamâ€™s service was a harsh way to learn that lesson.</p>
	<p>We put quite a bit of effort into working around this problem. We tried doing incremental updates of the sets, only adding or deleting the elements that had changed since the last update. It was helpful sometimes, but the complexity ended up not being worth the small savings. Through a lot of profiling, we found that splitting an update across multiple statements improved the situation some. This means we changed</p><!--kg-card-begin: markdown-->
	<pre><code>add element ip mfw set0 { 192.0.2.0, 192.0.2.2, â€¦, 198.51.100.0, 198.51.100.2, â€¦ }
</code></pre>
	<!--kg-card-end: markdown-->
	<p>to</p><!--kg-card-begin: markdown-->
	<pre><code>add element ip mfw set0 { 192.0.2.0, â€¦ }
add element ip mfw set0 { 198.51.100.0, â€¦ }
</code></pre>
	<!--kg-card-end: markdown-->
	<p>just to squeeze out a second or two of reduced time that nft took to process our configuration. We were spending a fair amount of development cycles looking for optimizations, and we needed to find a way to turn the tides.</p>
	<h3 id="one-more-step-with-ebpf">One more step with eBPF</h3>
	<p>As we mentioned <a href="https://blog.cloudflare.com/programmable-packet-filtering-with-magic-firewall">on our blog</a> recently, Magic Firewall leverages eBPF for some advanced packet-matching. And it may not be a big surprise that eBPF can match an IP in a list, but it wasnâ€™t a tool we thought to reach for a year ago. What makes eBPF relevant to this problem is that eBPF maps exist regardless of a network namespace. Thatâ€™s right; eBPF gives us a way to share this data across all the network namespaces we create for our customers.</p>
	<p>Since several of our IP lists contain ranges, we canâ€™t use a simple <code>BPF_MAP_TYPE_HASH</code> or <code>BPF_MAP_TYPE_ARRAY</code> to perform a lookup. Fortunately, Linux already has a data structure that we can use to accommodate ranges: the <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/kernel/bpf/lpm_trie.c"><code>BPF_MAP_TYPE_LPM_TRIE</code></a>.</p>
	<p>Given an IP address, the trieâ€™s implementation of <code>bpf_map_lookup_elem()</code> will return a non-null result if the IP falls within any of the ranges already inserted into the map. And using the map is about as simple as it gets for eBPF programs. Hereâ€™s an example:</p><!--kg-card-begin: markdown-->
	<pre><code>typedef struct {
	__u32 prefix_len;
	__u8 ip[4];
} trie_key_t;

SEC("maps")
struct bpf_map_def ip_list = {
    .type = BPF_MAP_TYPE_LPM_TRIE,
    .key_size = sizeof(trie_key_t),
    .value_size = 1,
    .max_entries = 1,
    .map_flags = BPF_F_NO_PREALLOC,
};

SEC("socket/saddr_in_list")
int saddr_in_list(struct __sk_buff *skb) {
	struct iphdr iph;
	trie_key_t trie_key;

	bpf_skb_load_bytes(skb, 0, &amp;iph, sizeof(iph));

	trie_key.prefix_len = 32;
	trie_key.ip[0] = iph.saddr &amp; 0xff;
	trie_key.ip[1] = (iph.saddr &gt;&gt; 8) &amp; 0xff;
	trie_key.ip[2] = (iph.saddr &gt;&gt; 16) &amp; 0xff;
	trie_key.ip[3] = (iph.saddr &gt;&gt; 24) &amp; 0xff;

	void *val = bpf_map_lookup_elem(&amp;ip_list, &amp;trie_key);
	return val == NULL ? BPF_OK : BPF_DROP;
}

</code></pre>
	<!--kg-card-end: markdown-->
	<h3 id="nftables-befriends-ebpf">nftables befriends eBPF</h3>
	<p>One of the limitations of our prior work incorporating eBPF into the firewall involves how the program is loaded into the nftables ruleset. Because the nft command-line program does not support calling a BPF program from a rule, we formatted the Netlink messages ourselves. While this allowed us to insert a rule that executes an eBPF program, it came at the cost of losing out on <a href="https://wiki.nftables.org/wiki-nftables/index.php/Atomic_rule_replacement">atomic rule replacements</a>.</p>
	<p>So any native nftables rules could be applied in one transaction, but any eBPF-based rules would have to be inserted afterwards. This introduces some headaches for handling failuresâ€”if inserting an eBPF rule fails, should we simply retry, or perhaps roll back the nftables ruleset as well? And what if those follow-up operations fail?</p>
	<p>In other words, we went from a relatively simple operationâ€”the new firewall configuration either fully applied or didnâ€™tâ€”to a much more complex one. We didnâ€™t want to keep using more and more eBPF without solving this problem. After hacking on the nftables repo for a few days, we came out with a patch that adds a BPF keyword to the nftables configuration syntax.</p>
	<p>I wonâ€™t cover all the changes, which we hope to upstream soon, but there were two key parts. The first is implementing <a href="https://git.netfilter.org/nftables/tree/include/expression.h?v1.0.1=#n158">struct expr_ops</a> and some methods required, which is how nftâ€™s parser knows what to do with each keyword in its configuration language (like the â€œbpfâ€ keyword we introduced). The second is just a different approach for what we solved previously. Instead of directly formatting <a href="https://git.netfilter.org/iptables/tree/include/linux/netfilter/xt_bpf.h?h=v1.8.7#n27">struct xt_bpf_info_v1</a> into a byte array as iptables does, nftables uses the nftnl library to create the netlink messages.</p>
	<p>Once we had our footing working in a foreign codebase, that code turned out fairly simple.</p><!--kg-card-begin: markdown-->
	<pre><code>static void netlink_gen_bpf(struct netlink_linearize_ctx *ctx,
                            const struct expr *expr,
                            enum nft_registers dreg)
{
       struct nftnl_expr *nle;

       nle = alloc_nft_expr("match");
       nftnl_expr_set_str(nle, NFTNL_EXPR_MT_NAME, "bpf");
       nftnl_expr_set_u8(nle, NFTNL_EXPR_MT_REV, 1);
       nftnl_expr_set(nle, NFTNL_EXPR_MT_INFO, expr-&gt;bpf.info, sizeof(struct xt_bpf_info_v1));
       nft_rule_add_expr(ctx, nle, &amp;expr-&gt;location);
}
</code></pre>
	<!--kg-card-end: markdown-->
	<p>With the patch finally built, it became possible for us to write configuration like this where we can provide a path to any pinned eBPF program.</p><!--kg-card-begin: markdown-->
	<pre><code># nft -f - &lt;&lt;EOF
table ip mfw {
  chain input {
    bpf pinned "/sys/fs/bpf/filter" drop
  }
}
EOF
</code></pre>
	<!--kg-card-end: markdown-->
	<p>And now we can mix native nftables matches with eBPF programs without giving up the atomic nature of nft commands. This unlocks the opportunity for us to build even more advanced packet inspection and protocol detection in eBPF.</p>
	<h3 id="results">Results</h3>
	<p>This plot shows kernel memory during the time of the deployment - the change was rolled out to each namespace over a few hours.</p>
	<figure class="kg-card kg-image-card kg-width-wide"><img src="https://blog.cloudflare.com/content/images/2022/03/image2-97.png" class="kg-image" alt="Kmalloc64 slab memory during release" loading="lazy" title="Graph of Kmalloc64 Slab Memory"></figure>
	<p>Though at this point, weâ€™ve merely moved memory out of this slab. Fortunately, the eBPF map memory now appears in the cgroup for our Golang process, so itâ€™s relatively easy to report. Hereâ€™s the point at which we first started populating the maps:</p>
	<figure class="kg-card kg-image-card kg-width-wide"><img src="https://blog.cloudflare.com/content/images/2022/03/image4-29.png" class="kg-image" alt="Cgroup memory during release" loading="lazy" title="Graph of Cgroup Memory"></figure>
	<p>This change was pushed a couple of weeks before the maps were actually used in the firewall (i.e. the graph just above). And it has remained constant ever sinceâ€”unlike our original design, the number of customers is no longer a predominant factor in this featureâ€™s resource usage. Â&nbsp;The new model makes us more confident about how the service will behave as our product continues to grow.</p>
	<p>Other than being better positioned to use eBPF more in the future, we made a significant improvement on the efficiency of the product today. In any project that grows rapidly for a while, an engineer can often see a few pesky pitfalls looming on the horizon. It is very satisfying to dive deep into these technologies and come out with a creative, novel solution before experiencing too much pain. We love solving hard problems and improving our systems to handle rapid growth in addition to pushing lots of new features.</p>
	<p>Does that sound like an environment you want to work in? Consider <a href="https://www.cloudflare.com/careers">applying</a>!</p>
</div>