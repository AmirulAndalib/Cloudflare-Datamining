{
	"browserLocale": "en-us",
	"locale": "en-us",
	"post": {
		"access": true,
		"authors": [
			{
				"id": "5d1644b141acde0011a94f30",
				"name": "Marek Majkowski",
				"slug": "marek-majkowski",
				"profile_image": "http://blog.cloudflare.com/content/images/2017/03/b5967d6c687939594adb6992723d0529.jpeg",
				"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-101.png",
				"bio": null,
				"website": null,
				"location": null,
				"facebook": null,
				"twitter": "@majek04",
				"meta_title": null,
				"meta_description": null,
				"url": "http://blog.cloudflare.com/author/marek-majkowski/"
			}
		],
		"canonical_url": null,
		"codeinjection_foot": null,
		"codeinjection_head": null,
		"comment_id": "4885",
		"comments": false,
		"created_at": "2016-12-19T14:38:43.000+00:00",
		"custom_excerpt": "We extensively monitor our network and use multiple systems that give us visibility including external monitoring and internal alerts when things go wrong.",
		"custom_template": null,
		"email_subject": null,
		"excerpt": "We extensively monitor our network and use multiple systems that give us visibility including external monitoring and internal alerts when things go wrong.",
		"feature_image": "http://blog.cloudflare.com/content/images/2018/08/hp2-1.png",
		"feature_image_alt": null,
		"feature_image_caption": null,
		"featured": false,
		"frontmatter": null,
		"html": "<!--kg-card-begin: markdown--><p>We extensively monitor our network and use multiple systems that give us visibility including external monitoring and internal alerts when things go wrong. One of the most useful systems is <a href=\"http://grafana.org/\">Grafana</a> that allows us to quickly create arbitrary dashboards. And a heavy user of Grafana we are: at last count we had 645 different Grafana dashboards configured in our system!</p>\n<pre><code>grafana=&gt; select count(1) from dashboard;\n count\n-------\n   645\n(1 row)\n</code></pre>\n<p>This post is not about our Grafana systems though. It's about something we noticed a few days ago, while looking at one of those dashboards. We noticed this:</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2017/01/hedgehog.png\" alt=\"\" loading=\"lazy\"></p>\n<p>This chart shows the number of HTTP requests per second handled by our systems globally. You can clearly see multiple spikes, and this chart most definitely should not look like a porcupine! The spikes were large in scale - <em>500k to 1M HTTP requests per second</em>. Something very strange was going on.</p>\n<h3 id=\"tracingthespikes\">Tracing the spikes<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup></h3>\n<p>Our intuition indicated an attack - but our attack mitigation systems didn't confirm it. We'd seen no major HTTP attacks at those times.</p>\n<p>It would be <em>bad</em> if we were under such heavy HTTP attack and our mitigation systems didn't notice it. Without more ideas, we went back to one of our favorite debugging tools - <code>tcpdump</code>.</p>\n<p>The spikes happened every 80 minutes and lasted about 10 minutes. We waited, and tried to catch the offending traffic. Here is what the HTTP traffic looked like on the wire:</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2016/12/hwire.png\" alt=\"\" loading=\"lazy\"></p>\n<p>The client had sent some binary junk to our HTTP server on port 80; they weren't even sending a fake GET or POST line!</p>\n<p>Our server politely responded with HTTP 400 error. This explains why it wasn't caught by our attack mitigation systems. Invalid HTTP requests don't trigger our HTTP DDoS mitigations - it makes no sense to mitigate traffic which is never accepted by NGINX in the first place!</p>\n<h3 id=\"thepayload\">The payload</h3>\n<p>At first glance the payload sent to HTTP servers seems random. A colleague of mine, <a href=\"https://twitter.com/chrisbranch\">Chris Branch</a>, investigated and proved me wrong. The payload has patterns.</p>\n<p>Let me show what's happening. Here are the first 24 bytes of the mentioned payload:</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2016/12/hp1.png\" alt=\"\" loading=\"lazy\"></p>\n<p>If you look closely, the pattern will start to emerge. Let's add some colors and draw it in not eight, but seven bytes per row:</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2016/12/hp2-1.png\" alt=\"\" loading=\"lazy\"></p>\n<p>This checkerboard-like pattern, is exhibited in most of the requests with payload sizes below 512 bytes.</p>\n<p>Another <a href=\"https://twitter.com/jgrahamc\">engineer</a> pointed out there appear to actually be two separate sequences generated in the same fashion. Starting with the <code>a6</code> and the <code>cb</code> take alternating bytes</p>\n<pre><code>a6 ef 39 82 cb 15 5e a7 f0 3a 83 cc 16 5f\ncb 15 5e a7 f0 3a 83 cc 16 5f a8 f1 3b\n</code></pre>\n<p>Aligning that differently shows that the second sequence is essentially the same as the first:</p>\n<pre><code>a6 ef 39 82 cb 15 5e a7 f0 3a 83 cc 16 5f\n            cb 15 5e a7 f0 3a 83 cc 16 5f a8 f1 3b\n</code></pre>\n<p>Thinking of that as one sequence gets</p>\n<pre><code>a6 ef 39 82 cb 15 5e a7 f0 3a 83 cc 16 5f a8 f1 3b\n</code></pre>\n<p>Which is generated by starting at <code>ef</code> and adding the following repeating sequence.</p>\n<pre><code>4a 49 49 4a 49 49 49\n</code></pre>\n<p>The 'random' binary junk is actually generated by some simple code.</p>\n<p>The length distribution of the requests is also interesting. Here's the histogram showing the popularity of particular lengths of payloads.</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2016/12/hlengths.png\" alt=\"\" loading=\"lazy\"></p>\n<p>About 80% of the junk requests we received had length of up to 511 bytes, uniformly distributed.</p>\n<p>The remaining 20% had length uniformly distributed between 512 and 2047 bytes, with a few interesting spikes. For some reason lengths of 979, 1383 and 1428 bytes stand out. The rest of the distribution looks uniform.</p>\n<h3 id=\"thescale\">The scale</h3>\n<p>The spikes were large. It takes a lot of firepower to generate a spike in our global HTTP statistics! On the first day the spikes reached about 600k junk requests per second. On second day the score went up to 1M rps. In total we recorded 37 spikes.</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2017/01/hattacks.png\" alt=\"\" loading=\"lazy\"></p>\n<h3 id=\"geography\">Geography</h3>\n<p>Unlike L3 attacks, L7 attacks require TCP/IP connections to be fully established. That means the source IP addresses are not spoofed and can be used to investigate the geographic distribution of attacking hosts.</p>\n<p>The spikes were generated by IP addresses from all around the world. We recorded IP numbers from 4,912 distinct Autonomous Systems. Here are top ASN numbers by number of unique attacking IP addresses:</p>\n<pre><code class=\"language-.txt\">Percent of unique IP addresses seen:\n21.51% AS36947  # AS de Algerie Telecom, Algeria\n 5.34% AS18881  # Telefonica Brasil S.A, Brasil\n 3.60% AS7738   # Telemar Norte Leste S.A., Brasil\n 3.48% AS27699  # Telefonica Brasil S.A, Brasil\n 3.37% AS28573  # CLARO S.A., Brasil\n 3.20% AS8167   # Brasil Telecom S/A, Brasil\n 2.44% AS2609   # Tunisia BackBone, Tunisia\n 2.22% AS6849   # PJSC &quot;Ukrtelecom&quot;, Ukraine\n 1.77% AS3320   # Deutsche Telekom AG, Germany\n 1.73% AS12322  # Free SAS, France\n 1.73% AS8452   # TE-AS, Egypt\n 1.35% AS12880  # Information Technology Company, Iran\n 1.30% AS37705  # TOPNET, Tunisia\n 1.26% AS53006  # Algar Telecom S/A, Brasil\n 1.22% AS36903  # ASN du reseaux MPLs de Maroc Telecom, Morocco\n ... 4897 AS numbers below 1% of IP addresses.\n</code></pre>\n<p>You get the picture - the traffic was sourced all over the place, with bias towards South America and North Africa. Here is the country distribution of attacking IPs:</p>\n<pre><code class=\"language-.txt\">Percent of unique IP addresses seen:\n31.76% BR\n21.76% DZ\n 7.49% UA\n 5.73% TN\n 4.89% IR\n 3.96% FR\n 3.76% DE\n 2.09% EG\n 1.78% SK\n 1.36% MA\n 1.15% GB\n 1.05% ES\n ... 109 countries below 1% of IP addresses\n</code></pre>\n<p>The traffic was truly global and launched with IPs from 121 countries. This kind of globally distributed attack is where <a href=\"http://blog.cloudflare.com/how-cloudflares-architecture-allows-us-to-scale-to-stop-the-largest-attacks/\">Cloudflare's Anycast</a> network shines. During these spikes the load was nicely distributed across dozens of datacenters. <a href=\"http://blog.cloudflare.com/parabens-brasil-cloudflares-27th-data-center-now-live/\">Our datacenter in SÃ£o Paulo</a> absorbed the most traffic, roughly 4 times more traffic than the second in line - Paris. This chart shows how the traffic was distributed across many datacenters:</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2017/01/hcolos.png\" alt=\"\" loading=\"lazy\"></p>\n<h3 id=\"uniqueips\">Unique IPs</h3>\n<p>During each of the spikes our systems recorded 200k unique source IP addresses sending us junk requests.</p>\n<p>Normally we would conclude that whoever generated the attack controlled roughly 200k bots, and that's it. But these spikes were different. It seems the bots rotated IPs aggressively. Here is an example: during these 16 spikes we recorded a total count of a whopping 1.2M unique IP addresses attacking us.</p>\n<p><img src=\"http://blog.cloudflare.com/content/images/2017/01/hmillion.png\" alt=\"\" loading=\"lazy\"></p>\n<p>This can be explained by bots churning through IP addresses. We believe that out of the estimated 200k bots, between 50k and 100k bots changed their IP addresses during the 80 minutes between attacks. This resulted in 1.2M unique IP addresses during the 16 spikes happening over 24 hours.</p>\n<h3 id=\"abotnet\">A botnet?</h3>\n<p>These spikes were unusual for a number of reasons.</p>\n<ul>\n<li>They were generated by a large number of IP addresses. We estimate 200k concurrent bots.</li>\n<li>The bots were rotating IP addresses aggressively.</li>\n<li>The bots were from around the world with an emphasis on South America and North Africa.</li>\n<li>The traffic generated was enormous, reaching 1M junk connections per second.</li>\n<li>The spikes happened exactly every 80 minutes and lasted for 10 minutes.</li>\n<li>The payload of the traffic was junk, not a usual HTTP request attack.</li>\n<li>The payload had uniformly distributed payload sizes.</li>\n</ul>\n<p>It's hard to draw conclusions, but we can imagine two possible scenarios. It is possible these spikes were an attack intended to break our HTTP servers.</p>\n<p>A second possibility is that these spikes were legitimate connection attempts by some weird, obfuscated protocol. For some reason the clients were connecting to port 80/TCP and retried precisely every 80 minutes.</p>\n<p>We are continuing our investigation. In the meantime we are looking for clues. Please do let us know if you have encountered this kind of TCP/IP payload. We are puzzled by these large spikes.</p>\n<p>If you'd like to work on this type of problem <a href=\"https://www.cloudflare.com/join-our-team/\">we're hiring</a> in London, San Francisco, Austin, Champaign and Singapore.</p>\n<p><em>Update</em> A <a href=\"https://twitter.com/GGreg/status/818462582998704128\">Twitter user</a> pointed out that the sequence <code>a6 ef 39 82 cb 15 5e a7 f0 3a 83 cc 16 5f a8 f1 3b</code> appears in this set of <a href=\"https://github.com/gvanas/KeccakCodePackage/blob/master/TestVectors/KetjeJr.txt\">test vectors</a> so we contacted the <a href=\"http://gva.noekeon.org/\">author</a> who was kind enough to reply and point us to the <a href=\"https://github.com/gvanas/KeccakCodePackage/blob/master/Tests/testKetje.c#L46\">code</a> that generated those vectors.</p>\n<pre><code>static void generateSimpleRawMaterial(unsigned char* data, unsigned int length, unsigned char seed1, unsigned int seed2)\n{\n    unsigned int i;\n\n    for( i=0; i&lt;length; i++) {\n        unsigned int iRolled = i*seed1;\n        unsigned char byte = (iRolled+length+seed2)%0xFF;\n        data[i] = byte;\n    }\n}\n</code></pre>\n<p>Since we identified above that the difference between two bytes seemed to be 0x49 or 0x4a it's worth looking at the difference between bytes in this algorithm. Simplifying, bytes are generated from:</p>\n<pre><code>((i * seed1) + length + seed2)%0xFF\n</code></pre>\n<p>Ignoring the <code>% 0xff</code> for the moment that's <code>(i * seed1) + length + seed</code>. Taking the difference between two adjacent bytes (for <code>i</code> and <code>i+1</code>) gives a difference of just <code>seed1</code>.</p>\n<p>Thus in our case it's likely that <code>seed1</code> is 0x49. It's fairly easy to end up with the following code to generate the sequence:</p>\n<pre><code>seed = 0x49\nbyte = 0xa6\n\ndo\n   byte = (seed + byte) % 0xff\ndone\n</code></pre>\n<p>One big mystery remaining is 'what's the 0x75 at the start of the junk data?'.</p>\n<p>Â </p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Yes, we're aware that porcupines have spines/quills not spikes. <a href=\"#fnref1\" class=\"footnote-backref\">â©ï¸</a></p>\n</li>\n</ol>\n</section>\n<!--kg-card-end: markdown-->",
		"id": "5d16453b41acde0011a955e2",
		"meta_description": null,
		"meta_title": null,
		"og_description": null,
		"og_image": null,
		"og_title": null,
		"primary_author": {
			"id": "5d1644b141acde0011a94f30",
			"name": "Marek Majkowski",
			"slug": "marek-majkowski",
			"profile_image": "http://blog.cloudflare.com/content/images/2017/03/b5967d6c687939594adb6992723d0529.jpeg",
			"cover_image": "http://blog.cloudflare.com/content/images/2018/08/general@2x-101.png",
			"bio": null,
			"website": null,
			"location": null,
			"facebook": null,
			"twitter": "@majek04",
			"meta_title": null,
			"meta_description": null,
			"url": "http://blog.cloudflare.com/author/marek-majkowski/"
		},
		"primary_tag": {
			"id": "5d16450341acde0011a95169",
			"name": "Attacks",
			"slug": "attacks",
			"description": null,
			"feature_image": null,
			"visibility": "public",
			"meta_title": null,
			"meta_description": null,
			"og_image": null,
			"og_title": null,
			"og_description": null,
			"twitter_image": null,
			"twitter_title": null,
			"twitter_description": null,
			"codeinjection_head": null,
			"codeinjection_foot": null,
			"canonical_url": null,
			"accent_color": null,
			"url": "http://blog.cloudflare.com/tag/attacks/"
		},
		"published_at": "2017-01-09T14:08:58.000+00:00",
		"reading_time": 7,
		"slug": "the-porcupine-attack-investigating-millions-of-junk-requests",
		"tags": [
			{
				"id": "5d16450341acde0011a95169",
				"name": "Attacks",
				"slug": "attacks",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/attacks/"
			},
			{
				"id": "5d16450341acde0011a951ef",
				"name": "Grafana",
				"slug": "grafana",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/grafana/"
			},
			{
				"id": "5d16450341acde0011a951ce",
				"name": "Reliability",
				"slug": "reliability",
				"description": null,
				"feature_image": null,
				"visibility": "public",
				"meta_title": null,
				"meta_description": null,
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/reliability/"
			},
			{
				"id": "5d16450341acde0011a95265",
				"name": "Security",
				"slug": "security",
				"description": null,
				"feature_image": "http://blog.cloudflare.com/content/images/2020/10/Security.png",
				"visibility": "public",
				"meta_title": "Cloudflare Blog: Security",
				"meta_description": "Collection of Cloudflare blog posts tagged 'Security'.",
				"og_image": null,
				"og_title": null,
				"og_description": null,
				"twitter_image": null,
				"twitter_title": null,
				"twitter_description": null,
				"codeinjection_head": null,
				"codeinjection_foot": null,
				"canonical_url": null,
				"accent_color": null,
				"url": "http://blog.cloudflare.com/tag/security/"
			}
		],
		"title": "The Porcupine Attack: investigating millions of junk requests",
		"twitter_description": null,
		"twitter_image": null,
		"twitter_title": null,
		"updated_at": "2018-08-28T22:25:09.000+01:00",
		"url": "http://blog.cloudflare.com/the-porcupine-attack-investigating-millions-of-junk-requests/",
		"uuid": "56dd38cd-f218-496e-a202-ef3a6c754a08",
		"visibility": "public"
	}
}