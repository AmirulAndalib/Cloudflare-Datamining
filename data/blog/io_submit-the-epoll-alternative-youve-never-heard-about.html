<div class="post-content lh-copy gray1">
	<!--kg-card-begin: markdown-->
	<p>My curiosity was piqued by an LWN article about <a href="https://lwn.net/Articles/743714/" target="_blank">IOCB_CMD_POLL - A new kernel polling interface</a>. It discusses an addition of a new polling mechanism to Linux AIO API, which was merged in 4.18 kernel. The whole idea is rather intriguing. The author of the patch is proposing to use the Linux AIO API with things like network sockets.</p>
	<p>Hold on. The Linux AIO is designed for, well, <strong>A</strong>synchronous disk <strong>IO</strong>! Disk files are not the same thing as network sockets! Is it even possible to use the Linux AIO API with network sockets in the first place?</p>
	<p>The answer turns out to be a strong YES! In this article I'll explain how to use the strengths of Linux AIO API to write better and faster network servers.</p>
	<p>But before we start, what is Linux AIO anyway?</p>
	<!--kg-card-end: markdown-->
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cloudflare.com/content/images/2019/01/6891085910_3390ebe29f_k.jpg" class="kg-image" alt="" loading="lazy">
		<figcaption><a href="https://www.flickr.com/photos/schill/6891085910/" target="_blank">Photo </a>by <a href="https://www.flickr.com/photos/schill" target="_blank">Scott Schiller </a>CC/BY/2.0</figcaption>
	</figure>
	<p></p><!--kg-card-begin: markdown-->
	<h2 id="introductiontolinuxaio">Introduction to Linux AIO</h2>
	<p>Linux AIO exposes asynchronous disk IO to userspace software.</p>
	<p>Historically on Linux, all disk operations were blocking. Whether you did <code>open()</code>, <code>read()</code>, <code>write()</code> or <code>fsync()</code>, you could be sure your thread would stall if the needed data and meta-data was not ready in disk cache. This usually isn't a problem. If you do small amount of IO or have plenty of memory, the disk syscalls would gradually fill the cache and on average be rather fast.</p>
	<p>The IO operation performance drops for IO-heavy workloads, like databases or caching web proxies. In such applications it would be tragic if a whole server stalled, just because some odd <code>read()</code> syscall had to wait for disk.</p>
	<p>To work around this problem, applications use one of the three approaches:</p>
	<p>(1) Use thread pools and offload blocking syscalls to worker threads. This is what glibc POSIX AIO (not to be confused with Linux AIO) wrapper does. (See: <a href="https://www.ibm.com/developerworks/linux/library/l-async/" target="_blank">IBM's documentation</a>). This is also what we ended up doing in our application at Cloudflare - <a href="https://blog.cloudflare.com/how-we-scaled-nginx-and-saved-the-world-54-years-every-day/">we offloaded read() and open() calls to a thread pool</a>.</p>
	<p>(2) Pre-warm the disk cache with <code>posix_fadvise(2)</code> and hope for the best.</p>
	<p>(3) Use Linux AIO with XFS filesystem, <a href="https://lwn.net/Articles/671649/" target="_blank">file opened with O_DIRECT</a>, and <a href="https://www.scylladb.com/2016/02/09/qualifying-filesystems/" target="_blank">avoid the undocumented pitfalls</a>.</p>
	<p>None of these methods is perfect. Even the Linux AIO if used carelessly, could still block in the <code>io_submit()</code> call. This was recently mentioned <a href="https://lwn.net/Articles/724198/" target="_blank">in another LWN article</a>:</p>
	<blockquote>
		<p>The Linux asynchronous I/O (AIO) layer tends to have many critics and few defenders, but most people at least expect it to actually be asynchronous. In truth, an AIO operation can block in the kernel for a number of reasons, making AIO difficult to use in situations where the calling thread truly cannot afford to block.</p>
	</blockquote>
	<p>Now that we know what Linux AIO API doesn't do well, let's see where it shines.</p>
	<!--kg-card-end: markdown-->
	<p></p><!--kg-card-begin: markdown-->
	<h2 id="simplestlinuxaioprogram">Simplest Linux AIO program</h2>
	<p>To use Linux AIO you first need <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2019-01-io-submit/aio_passwd.c" target="_blank">to define the 5 needed syscalls</a> - glibc doesn't provide wrapper functions. To use Linux AIO we need:</p>
	<p>(1) First call <code>io_setup()</code> to setup the <code>aio_context</code> data structure. Kernel will hand us an opaque pointer.</p>
	<p>(2) Then we can call <code>io_submit()</code> to submit a vector of "I/O control blocks" <code>struct iocb</code> for processing.</p>
	<p>(3) Finally, we can call <code>io_getevents()</code> to block and wait for a vector of <code>struct io_event</code> - completion notification of the iocb's.</p>
	<p>There are 8 commands that can be submitted in an iocb. Two read, two write, two fsync variants and a POLL command introduced in 4.18 Kernel:</p>
	<pre><code>IOCB_CMD_PREAD = 0,
IOCB_CMD_PWRITE = 1,
IOCB_CMD_FSYNC = 2,
IOCB_CMD_FDSYNC = 3,
IOCB_CMD_POLL = 5,   /* from 4.18 */
IOCB_CMD_NOOP = 6,
IOCB_CMD_PREADV = 7,
IOCB_CMD_PWRITEV = 8,
</code></pre>
	<p>The <a href="https://github.com/torvalds/linux/blob/f346b0becb1bc62e45495f9cdbae3eef35d0b635/include/uapi/linux/aio_abi.h#L73-L107" target="_blank"><code>struct iocb</code></a> passed to <code>io_submit</code> is large, and tuned for disk IO. Here's a simplified version:</p>
	<pre><code class="language-.c">struct iocb {
  __u64 data;           /* user data */
  ...
  __u16 aio_lio_opcode; /* see IOCB_CMD_ above */
  ...
  __u32 aio_fildes;     /* file descriptor */
  __u64 aio_buf;        /* pointer to buffer */
  __u64 aio_nbytes;     /* buffer size */
...
}
</code></pre>
	<p>The completion notification retrieved from <code>io_getevents</code>:</p>
	<pre><code class="language-.c">struct io_event {
  __u64  data;  /* user data */
  __u64  obj;   /* pointer to request iocb */
  __s64  res;   /* result code for this event */
  __s64  res2;  /* secondary result */
};

</code></pre>
	<p>Let's try an example. Here's the simplest program reading /etc/passwd file with Linux AIO API:</p>
	<pre><code class="language-.c">fd = open("/etc/passwd", O_RDONLY);

aio_context_t ctx = 0;
r = io_setup(128, &amp;ctx);

char buf[4096];
struct iocb cb = {.aio_fildes = fd,
                  .aio_lio_opcode = IOCB_CMD_PREAD,
                  .aio_buf = (uint64_t)buf,
                  .aio_nbytes = sizeof(buf)};
struct iocb *list_of_iocb[1] = {&amp;cb};

r = io_submit(ctx, 1, list_of_iocb);

struct io_event events[1] = {{0}};
r = io_getevents(ctx, 1, 1, events, NULL);

bytes_read = events[0].res;
printf("read %lld bytes from /etc/passwd\n", bytes_read);
</code></pre>
	<p>Full source is, <a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2019-01-io-submit/aio_passwd.c" target="_blank">as usual, on github</a>. Here's an strace of this program:</p>
	<pre><code class="language-.txt">openat(AT_FDCWD, "/etc/passwd", O_RDONLY)
io_setup(128, [0x7f4fd60ea000])
io_submit(0x7f4fd60ea000, 1, [{aio_lio_opcode=IOCB_CMD_PREAD, aio_fildes=3, aio_buf=0x7ffc5ff703d0, aio_nbytes=4096, aio_offset=0}])
io_getevents(0x7f4fd60ea000, 1, 1, [{data=0, obj=0x7ffc5ff70390, res=2494, res2=0}], NULL)
</code></pre>
	<p>This all worked fine! But the disk read was not asynchronous: the <code>io_submit</code> syscall blocked and did all the work! The <code>io_getevents</code> call finished instantly. We could try to make the disk read async, but this requires O_DIRECT flag which skips the caches.</p>
	<p>Lets try to better illustrate the blocking nature of <code>io_submit</code> on normal files. Here's similar example, showing strace when reading large 1GiB block from <code>/dev/zero</code>:</p>
	<pre><code>io_submit(0x7fe1e800a000, 1, [{aio_lio_opcode=IOCB_CMD_PREAD, aio_fildes=3, aio_buf=0x7fe1a79f4000, aio_nbytes=1073741824, aio_offset=0}]) \
    = 1 &lt;0.738380&gt;
io_getevents(0x7fe1e800a000, 1, 1, [{data=0, obj=0x7fffb9588910, res=1073741824, res2=0}], NULL) \
    = 1 &lt;0.000015&gt;
</code></pre>
	<p>The kernel spent 738ms in <code>io_submit</code> and only 15us in <code>io_getevents</code>. The kernel behaves the same way with network sockets - all the work is done in <code>io_submit</code>.</p>
	<!--kg-card-end: markdown-->
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cloudflare.com/content/images/2019/01/Network_card-2.jpg" class="kg-image" alt="" loading="lazy">
		<figcaption><a href="https://commons.wikimedia.org/wiki/File:Network_card.jpg" target="_blank">Photo</a> by <a href="https://commons.wikimedia.org/wiki/User:Helix84" target="_blank">Helix84</a> CC/BY-SA/3.0</figcaption>
	</figure><!--kg-card-begin: markdown-->
	<h2 id="linuxaiowithsocketsbatching">Linux AIO with sockets - batching</h2>
	<p>The implementation of <code>io_submit</code> is rather conservative. Unless the passed descriptor is O_DIRECT file, it will just block and perform the requested action. In case of network sockets it means:</p>
	<ul>
		<li>For blocking sockets IOCB_CMD_PREAD will hang until a packet arrives.</li>
		<li>For non-blocking sockets IOCB_CMD_PREAD will -11 (EAGAIN) return code.</li>
	</ul>
	<p>These are exactly the same semantics as for vanilla <code>read()</code> syscall. It's fair to say that for network sockets <code>io_submit</code> is no smarter than good old read/write calls.</p>
	<p>It's important to note the <code>iocb</code> requests passed to kernel are evaluated in-order sequentially.</p>
	<p>While Linux AIO won't help with async operations, it can definitely be used for syscall batching!</p>
	<p>If you have a web server needing to send and receive data from hundreds of network sockets, using <code>io_submit</code> can be a great idea. This would avoid having to call <code>send</code> and <code>recv</code> hundreds of times. This will improve performance - jumping back and forth from userspace to kernel is not free, especially <a href="https://gist.github.com/antirez/9e716670f76133ec81cb24036f86ee95" target="_blank">since the meltdown and spectre mitigations</a>.</p>
	<table>
		<thead>
			<tr>
				<th></th>
				<th>One buffer</th>
				<th>Multiple buffers</th>
			</tr>
		</thead>
		<tbody>
			<tr>
				<td>One file descriptor</td>
				<td>read()</td>
				<td>readv()</td>
			</tr>
			<tr>
				<td>Many file descriptors</td>
				<td>io_submit + IOCB_CMD_PREAD</td>
				<td>io_submit + IOCB_CMD_PREADV</td>
			</tr>
		</tbody>
	</table>
	<p>To illustrate the batching aspect of <code>io_submit</code>, let's create a small program forwarding data from one TCP socket to another. In simplest form, without Linux AIO, the program would have a trivial flow like this:</p>
	<pre><code class="language-.py">while True:
  d = sd1.read(4096)
  sd2.write(d)
</code></pre>
	<p>We can express the same logic with Linux AIO. The code will look like this:</p>
	<pre><code class="language-.c">struct iocb cb[2] = {{.aio_fildes = sd2,
                      .aio_lio_opcode = IOCB_CMD_PWRITE,
                      .aio_buf = (uint64_t)&amp;buf[0],
                      .aio_nbytes = 0},
                     {.aio_fildes = sd1,
                     .aio_lio_opcode = IOCB_CMD_PREAD,
                     .aio_buf = (uint64_t)&amp;buf[0],
                     .aio_nbytes = BUF_SZ}};
struct iocb *list_of_iocb[2] = {&amp;cb[0], &amp;cb[1]};
while(1) {
  r = io_submit(ctx, 2, list_of_iocb);

  struct io_event events[2] = {};
  r = io_getevents(ctx, 2, 2, events, NULL);
  cb[0].aio_nbytes = events[1].res;
}
</code></pre>
	<p>This code submits two jobs to <code>io_submit</code>. First, request to write data to <code>sd2</code> then to read data from <code>sd1</code>. After the read is done, the code fixes up the write buffer size and loops again. The code does a cool trick - the first write is of size 0. We are doing so since we can fuse write+read in one io_submit (but not read+write). After a read is done we have to fix the write buffer size.</p>
	<p>Is this code faster than the simple read/write version? Not yet. Both versions have two syscalls: read+write and io_submit+io_getevents. Fortunately, we can improve it.</p>
	<!--kg-card-end: markdown--><!--kg-card-begin: markdown-->
	<h2 id="gettingridofio_getevents">Getting rid of io_getevents</h2>
	<p>When running <code>io_setup()</code>, the kernel allocates a couple of pages of memory for the process. This is how this memory block looks like in /proc/&lt;pid&gt;/maps:</p>
	<pre><code>marek:~$ cat /proc/`pidof -s aio_passwd`/maps
...
7f7db8f60000-7f7db8f63000 rw-s 00000000 00:12 2314562     /[aio] (deleted)
...
</code></pre>
	<p>The [aio] memory region (12KiB in my case) was allocated by the <code>io_setup</code>. This memory range is used a ring buffer storing the completion events. In most cases, there isn't any reason to call the real <code>io_getevents</code> syscall. The completion data can be easily retrieved from the ring buffer without the need of consulting the kernel. Here is a fixed version of the code:</p>
	<pre><code class="language-.c">int io_getevents(aio_context_t ctx, long min_nr, long max_nr,
                 struct io_event *events, struct timespec *timeout)
{
    int i = 0;

    struct aio_ring *ring = (struct aio_ring*)ctx;
    if (ring == NULL || ring-&gt;magic != AIO_RING_MAGIC) {
        goto do_syscall;
    }

    while (i &lt; max_nr) {
        unsigned head = ring-&gt;head;
        if (head == ring-&gt;tail) {
            /* There are no more completions */
            break;
        } else {
            /* There is another completion to reap */
            events[i] = ring-&gt;events[head];
            read_barrier();
            ring-&gt;head = (head + 1) % ring-&gt;nr;
            i++;
        }
    }

    if (i == 0 &amp;&amp; timeout != NULL &amp;&amp; timeout-&gt;tv_sec == 0 &amp;&amp; timeout-&gt;tv_nsec == 0) {
        /* Requested non blocking operation. */
        return 0;
    }

    if (i &amp;&amp; i &gt;= min_nr) {
        return i;
    }

do_syscall:
    return syscall(__NR_io_getevents, ctx, min_nr-i, max_nr-i, &amp;events[i], timeout);
}

</code></pre>
	<p><a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2019-01-io-submit/aio_poll.c#L63" target="_blank">Here's full code</a>. This ring buffer interface is poorly documented. I adapted this code from <a href="https://github.com/axboe/fio/blob/702906e9e3e03e9836421d5e5b5eaae3cd99d398/engines/libaio.c#L149-L172" target="_blank">the axboe/fio project</a>.</p>
	<p>With this code fixing the <code>io_getevents</code> function, our Linux AIO version of the TCP proxy needs only one syscall per loop, and indeed is a tiny bit faster than the read+write code.</p>
	<!--kg-card-end: markdown-->
	<figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cloudflare.com/content/images/2019/01/16026681353_90b26e0731_z.jpg" class="kg-image" alt="" loading="lazy">
		<figcaption><a href="https://www.flickr.com/photos/99279135@N05/16026681353/" target="_blank">Photo </a>by <a href="https://www.flickr.com/photos/99279135@N05/" target="_blank">Train Photos </a>CC/BY-SA/2.0</figcaption>
	</figure><!--kg-card-begin: markdown-->
	<h2 id="epollalternative">Epoll alternative</h2>
	<p>With the addition of IOCB_CMD_POLL in kernel 4.18, one could use <code>io_submit</code> also as select/poll/epoll equivalent. For example, here's some code waiting for data on a socket:</p>
	<pre><code class="language-.c">struct iocb cb = {.aio_fildes = sd,
                  .aio_lio_opcode = IOCB_CMD_POLL,
                  .aio_buf = POLLIN};
struct iocb *list_of_iocb[1] = {&amp;cb};

r = io_submit(ctx, 1, list_of_iocb);
r = io_getevents(ctx, 1, 1, events, NULL);
</code></pre>
	<p><a href="https://github.com/cloudflare/cloudflare-blog/blob/master/2019-01-io-submit/aio_poll.c" target="_blank">Full code</a>. Here's the strace view:</p>
	<pre><code>io_submit(0x7fe44bddd000, 1, [{aio_lio_opcode=IOCB_CMD_POLL, aio_fildes=3}]) \
    = 1 &lt;0.000015&gt;
io_getevents(0x7fe44bddd000, 1, 1, [{data=0, obj=0x7ffef65c11a8, res=1, res2=0}], NULL) \
    = 1 &lt;1.000377&gt;
</code></pre>
	<p>As you can see this time the "async" part worked fine, the <code>io_submit</code> finished instantly and the <code>io_getevents</code> successfully blocked for 1s while awaiting data. This is pretty powerful and can be used instead of the <code>epoll_wait()</code> syscall.</p>
	<p>Furthermore, normally dealing with <code>epoll</code> requires juggling <code>epoll_ctl</code> syscalls. Application developers go to great lengths to avoid calling this syscall too often. <a href="http://man7.org/linux/man-pages/man7/epoll.7.html" target="_blank">Just read the man page</a> on EPOLLONESHOT and EPOLLET flags. Using <code>io_submit</code> for polling works around this whole complexity, and doesn't require any spurious syscalls. Just push your sockets to the iocb request vector, call <code>io_submit</code> exactly once and wait for completions. The API can't be simpler than this.</p>
	<!--kg-card-end: markdown--><!--kg-card-begin: markdown-->
	<h2 id="summary">Summary</h2>
	<p>In this blog post we reviewed the Linux AIO API. While initially conceived to be a disk-only API, it seems to be working in the same way as normal read/write syscalls on network sockets. But as opposed to read/write <code>io_submit</code> allows syscall batching, potentially improving performance.</p>
	<p>Since kernel 4.18 <code>io_submit</code> and <code>io_getevents</code> can be used to wait for events like POLLIN and POLLOUT on network sockets. This is great, and could be used as a replacement for <code>epoll()</code> in the event loop.</p>
	<p>I can imagine a network server that could just be doing <code>io_submit</code> and <code>io_getevents</code> syscalls, as opposed to the usual mix of <code>read</code>, <code>write</code>, <code>epoll_ctl</code> and <code>epoll_wait</code>. With such design the syscall batching aspect of <code>io_submit</code> could really shine. Such a server would be meaningfully faster.</p>
	<p>Sadly, even with recent Linux AIO API improvements, the larger discussion remains. Famously, <a href="https://lwn.net/Articles/671657/" target="_blank">Linus hates it</a>:</p>
	<blockquote>
		<p>AIO is a horrible ad-hoc design, with the main excuse being "other, less gifted people, made that design, and we are implementing it for compatibility because database people - who seldom have any shred of taste - actually use it". But AIO was always really really ugly.</p>
	</blockquote>
	<p>Over the years there had been multiple attempts on creating a better batching and async interfaces, unfortunately, lacking coherent vision. For example, <a href="https://github.com/torvalds/linux/blob/6f0d349d922ba44e4348a17a78ea51b7135965b1/Documentation/networking/msg_zerocopy.rst" target="_blank">recent addition of <code>sendto(MSG_ZEROCOPY)</code></a> allows for truly async transmission operations, but no batching. <code>io_submit</code> allows batching but not async. It's even worse than that - Linux currently has three ways of delivering async notifications - signals, <code>io_getevents</code> and <code>MSG_ERRQUEUE</code>.</p>
	<p>Having said that I'm really excited to see the new developments which allow developing faster network servers. I'm jumping on the code to replace my rusty epoll event loops with io_submit!</p>
	<!--kg-card-end: markdown-->
</div>